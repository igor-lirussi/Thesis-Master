<!DOCTYPE html>
<html  lang="it"  dir="ltr">

    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Master Thesis - Igor Lirussi</title>
        <link rel="shortcut icon" type="image/png" href="favicon.png">
        <link rel="apple-touch-icon-precomposed" href="apple-touch-icon.png">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/uikit/2.26.4/css/uikit.gradient.css">

        <link rel="stylesheet" href="style.css">
        <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />
        <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
        <script src="./pandoc-uikit/uikit.js"></script>
        <script src="./pandoc-uikit/scripts.js"></script>
        <script src="./pandoc-uikit/jquery.sticky-kit.js"></script>

        <meta name="generator" content="pandoc-uikit" />
                <meta name="author" content="Lirussi Igor" />
                        <meta name="date" content="2023-11-20" />
                <title>Master Thesis - Igor Lirussi</title>
        <style type="text/css">code{white-space: pre;}</style>
                        <style type="text/css">
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            .sourceCode { overflow: visible; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #aaaaaa;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
            div.sourceCode
              {  background-color: #f8f8f8; }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #ef2929; } /* Alert */
            code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #204a87; } /* Attribute */
            code span.bn { color: #0000cf; } /* BaseN */
            code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #4e9a06; } /* Char */
            code span.cn { color: #8f5902; } /* Constant */
            code span.co { color: #8f5902; font-style: italic; } /* Comment */
            code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
            code span.dt { color: #204a87; } /* DataType */
            code span.dv { color: #0000cf; } /* DecVal */
            code span.er { color: #a40000; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #0000cf; } /* Float */
            code span.fu { color: #204a87; font-weight: bold; } /* Function */
            code span.im { } /* Import */
            code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
            code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
            code span.ot { color: #8f5902; } /* Other */
            code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
            code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
            code span.ss { color: #4e9a06; } /* SpecialString */
            code span.st { color: #4e9a06; } /* String */
            code span.va { color: #000000; } /* Variable */
            code span.vs { color: #4e9a06; } /* VerbatimString */
            code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
        </style>
                        <link rel="stylesheet" href="./pandoc-uikit/uikit.css" />
                                          <script
                                          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
                                          type="text/javascript"></script>
                               
    </head>

    <body>


        <div class="uk-container uk-container-center uk-margin-top uk-margin-large-bottom">

                        <div class="uk-grid" data-uk-grid-margin>
                <div class="uk-width-1-1">
                    <h1 class="uk-heading">Master Thesis - Igor Lirussi</h1>
                                        <h1 class="uk-heading" style="margin:0">2023-11-20</h1>
                                                            <p class="uk-text-large">Lirussi
Igor</p>
                                    </div>
            </div>
            
            <div class="uk-grid" data-uk-grid-margin >          
                <div class="uk-width-medium-1-4">
                    <div class="uk-overflow-container" data-uk-sticky="{top:25,media: 768}">
                        <div class="uk-panel uk-panel-box menu-begin" >

                                                        <ul>
                                                        <li><a
                                                        href="#fooabstract"
                                                        id="footoc-abstract">Abstract</a></li>
                                                        <li><a
                                                        href="#foosection"
                                                        id="footoc-section"></a></li>
                                                        <li><a
                                                        href="#fooacknowledgements"
                                                        id="footoc-acknowledgements">Acknowledgements</a></li>
                                                        <li><a
                                                        href="#foolist-of-symbols"
                                                        id="footoc-list-of-symbols">List
                                                        of Symbols</a></li>
                                                        <li><a
                                                        href="#foolist-of-acronymsabbreviations"
                                                        id="footoc-list-of-acronymsabbreviations">List
                                                        of
                                                        Acronyms/Abbreviations</a></li>
                                                        <li><a
                                                        href="#foochap:introduction"
                                                        id="footoc-chap:introduction"><span
                                                        class="toc-section-number">1</span>
                                                        Introduction</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foooverview"
                                                        id="footoc-overview"><span
                                                        class="toc-section-number">1.1</span>
                                                        Overview</a></li>
                                                        <li><a
                                                        href="#foochallenges"
                                                        id="footoc-challenges"><span
                                                        class="toc-section-number">1.2</span>
                                                        Challenges</a></li>
                                                        <li><a
                                                        href="#fooobjectives"
                                                        id="footoc-objectives"><span
                                                        class="toc-section-number">1.3</span>
                                                        Objectives</a></li>
                                                        <li><a
                                                        href="#foothesis-structure"
                                                        id="footoc-thesis-structure"><span
                                                        class="toc-section-number">1.4</span>
                                                        Thesis
                                                        Structure</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:background"
                                                        id="footoc-chap:background"><span
                                                        class="toc-section-number">2</span>
                                                        State of the Art</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foogaussian-processes"
                                                        id="footoc-gaussian-processes"><span
                                                        class="toc-section-number">2.1</span>
                                                        Gaussian
                                                        Processes</a></li>
                                                        <li><a href="#foocnp"
                                                        id="footoc-cnp"><span
                                                        class="toc-section-number">2.2</span>
                                                        CNP</a></li>
                                                        <li><a
                                                        href="#foocnmps"
                                                        id="footoc-cnmps"><span
                                                        class="toc-section-number">2.3</span>
                                                        CNMPs</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:platforms"
                                                        id="footoc-chap:platforms"><span
                                                        class="toc-section-number">3</span>
                                                        Platforms</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foobaxter-robot"
                                                        id="footoc-baxter-robot"><span
                                                        class="toc-section-number">3.1</span>
                                                        Baxter Robot</a></li>
                                                        <li><a
                                                        href="#foour10-robot"
                                                        id="footoc-ur10-robot"><span
                                                        class="toc-section-number">3.2</span>
                                                        UR10 Robot</a></li>
                                                        <li><a
                                                        href="#foof-robotiq-gripper"
                                                        id="footoc-f-robotiq-gripper"><span
                                                        class="toc-section-number">3.3</span>
                                                        3F Robotiq
                                                        Gripper</a></li>
                                                        <li><a
                                                        href="#fooft-300-s-force-torque-sensor"
                                                        id="footoc-ft-300-s-force-torque-sensor"><span
                                                        class="toc-section-number">3.4</span>
                                                        FT 300-S Force Torque
                                                        Sensor</a></li>
                                                        <li><a
                                                        href="#fooframeworks"
                                                        id="footoc-frameworks"><span
                                                        class="toc-section-number">3.5</span>
                                                        Frameworks</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:design"
                                                        id="footoc-chap:design"><span
                                                        class="toc-section-number">4</span>
                                                        Design</a>
                                                        <ul>
                                                        <li><a
                                                        href="#fooend-to-end-concatenation"
                                                        id="footoc-end-to-end-concatenation"><span
                                                        class="toc-section-number">4.1</span>
                                                        End-To-End
                                                        Concatenation</a></li>
                                                        <li><a
                                                        href="#foopartial-combination"
                                                        id="footoc-partial-combination"><span
                                                        class="toc-section-number">4.2</span>
                                                        Partial
                                                        Combination</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:implementation"
                                                        id="footoc-chap:implementation"><span
                                                        class="toc-section-number">5</span>
                                                        Implementation</a></li>
                                                        <li><a
                                                        href="#foochap:validation"
                                                        id="footoc-chap:validation"><span
                                                        class="toc-section-number">6</span>
                                                        Validation and
                                                        Testing</a></li>
                                                        <li><a
                                                        href="#foochap:conclusions"
                                                        id="footoc-chap:conclusions"><span
                                                        class="toc-section-number">7</span>
                                                        Conclusions</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foofuture-work"
                                                        id="footoc-future-work"><span
                                                        class="toc-section-number">7.1</span>
                                                        Future work</a></li>
                                                        </ul></li>
                                                        </ul>
                            
                        </div>
                    </div>
                </div>

                <div class="uk-width-medium-3-4">
<div class="titlepage">
<div class="center">
<p><strong>ALMA MATER STUDIORUM &#x2013; UNIVERSITY OF BOLOGNA<br />
CESENA CAMPUS</strong><br />
</p>
<p>School of Engineering and Architecture<br />
Second Cycle Degree/Two-year Master in<br />
Computer Science and Engineering</p>
<p><strong>Novel high-level skill generation<br />
in robotics by combining movement primitives<br />
learned by CNMP models</strong></p>
<p>Master&#x2019;s thesis in<br />
<span class="smallcaps">Intelligent Robotic Systems</span></p>
<div class="flushleft">
<p><em>Supervisor</em><br />
<strong>Prof.</strong> <strong>Andrea Roli</strong><br />
<em>Co-Supervisor</em><br />
<strong>Prof.</strong> <strong>Emre U&#x11F;ur</strong><br />
(Bo&#x11F;azi&#xE7;i University, Istanbul)</p>
</div>
<div class="flushright">
<p><em>Candidate</em><br />
<strong>Igor Lirussi</strong></p>
</div>
<p><br />
</p>
<p>Academic Year 2022-2023</p>
</div>
</div>
<section id="fooabstract" class="level1 unnumbered">
<h1 class="unnumbered">Abstract</h1>
<p>Max 2000 characters, strict. UniBo has that limit in the upload system!
Will write at the end.</p>
</section>
<section id="foosection" class="level1 unnumbered">
<h1 class="unnumbered"></h1>
<div class="flushright">
<p><em>Dedication here</em></p>
</div>
</section>
<section id="fooacknowledgements" class="level1 unnumbered">
<h1 class="unnumbered">Acknowledgements</h1>
<p>Acknowledgments here.</p>
</section>
<section id="foolist-of-symbols" class="level1 unnumbered">
<h1 class="unnumbered">List of Symbols</h1>
<div class="tabbing">
<p>&#x304;&#x304;</p>
<p><span class="math inline">\(a_{ij}\)</span></p>
<p>Description of <span class="math inline">\(a_{ij}\)</span></p>
<p><br />
<span class="math inline">\(\mathbf{A}\)</span></p>
<p>State transition matrix of a hidden Markov model</p>
<p><br />
<span class="math inline">\(t\)</span></p>
<p>Time</p>
<p><br />
</p>
<p><br />
<span class="math inline">\(\alpha\)</span></p>
<p>Blending parameter <em>or</em> scale</p>
<p><br />
<span class="math inline">\(\beta_t(i)\)</span></p>
<p>Backward variable</p>
<p><br />
<span class="math inline">\(\gamma\)</span></p>
<p>External parameter (Task parameter)</p>
<p><br />
<span class="math inline">\(\Theta\)</span></p>
<p>Parameter set</p>
<p><br />
<span class="math inline">\(\sigma\)</span></p>
<p>Standard Deviation</p>
<p><br />
<span class="math inline">\(\mu\)</span></p>
<p>Mean</p>
<p><br />
</p>
</div>
</section>
<section id="foolist-of-acronymsabbreviations" class="level1 unnumbered">
<h1 class="unnumbered">List of Acronyms/Abbreviations</h1>
<div class="tabbing">
<p>&#x304;&#x304;</p>
<p>2D</p>
<p>Two Dimensional</p>
<p><br />
3D</p>
<p>Three Dimensional</p>
<p><br />
IR</p>
<p>Infrared</p>
<p><br />
ROS</p>
<p>Robot Operating System</p>
<p><br />
RGB</p>
<p>Red Green Blue</p>
<p><br />
RGBD</p>
<p>Red Green Blue Depth</p>
<p><br />
GP</p>
<p>Gaussian Processes</p>
<p><br />
CNP</p>
<p>Conditional Neural Processes</p>
<p><br />
CNMP</p>
<p>Conditional Neural Movement Primitives</p>
<p><br />
YOLO</p>
<p>You Only Look Once model</p>
<p><br />
UR10</p>
<p>Universal Robot 10</p>
<p><br />
DOF</p>
<p>Degrees of freedom</p>
<p><br />
IMU</p>
<p>Inertial measurement unit</p>
<p><br />
RPC</p>
<p>Remote Procedure Call</p>
<p><br />
CPU</p>
<p>Central Processing Unit</p>
<p><br />
GPU</p>
<p>Graphic Processing Unit</p>
<p><br />
</p>
</div>
</section>
<section id="foochap:introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span>
Introduction</h1>
<section id="foooverview" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Overview</h2>
<p>Humans have a remarkable ability to achieve complex goals in a wide variety
of tasks. A person is usually exposed to different scenarios during the day,
starting from the home environment to the commute, work, mealtime, and so on.
The versatility of our species is a key factor, and the human cognitive
flexibility has been appointed as a major driver in evolution <span
class="citation" data-cites="deak2003development">(Deak 2003)</span>, <span
class="citation" data-cites="karmiloff1994beyond">(Karmiloff-Smith
1994)</span>. Some situations are more complicated than others; nevertheless,
regardless of their difference, humans excel in meeting the different demands
to solve the tasks desired.</p>
<p>All these scenarios present different small challenges to solve in order to
accomplish the desired high-level goal. Humans switch contexts in a really
flexible and natural way and constantly take care of the multitude of these
small problems that are faced to complete the desired objective. For example,
a general task can be divided into subtasks, which can then be further divided
into smaller ones <span class="citation"
data-cites="kroemer2021review">(Kroemer, Niekum, e Konidaris 2021)</span>. The
strategy of breaking down intricate objectives into smaller, manageable,
simpler activities is the most widely used heuristic to solve problems <span
class="citation" data-cites="egidi2006decomposition">(Egidi 2006)</span>.</p>
<p>Many of these sub-challenges require an interaction with one or more
objects. For example, the action of opening involves a door to pass through it
while moving or to access the fridge for cooking. The reaching action can
imply an object like a pen in the office to write or a glass of water to
drink. To push as an action often implies a button to enable a device in the
workplace, or to turn on a car to commute, or the stove to heat a meal.
Objects have undoubtedly strong importance in the small actions performed to
achieve a goal, and their affordance is still the object of research in humans
<span class="citation" data-cites="maranesi2014cortical">(Maranesi, Bonini, e
Fogassi 2014)</span>, <span class="citation"
data-cites="osiurak2017affordance">(Osiurak, Rossetti, e Badets 2017)</span>
and machines <span class="citation"
data-cites="horton2012affordances">(Horton, Chakraborty, e Amant
2012)</span>.</p>
<p>As seen, many different movements and sub-actions, often involving objects,
are executed in daily life. Furthermore, they are also adapted to accomplish
the current desired goals. The adaptation can involve a simple difference of
position with respect to the previous location, both of the object or the
executor, or can involve a completely different context to which the action
learned is transferred. These skills are learned and discovered at the
beginning, and then the knowledge of the action is abstracted and adapted to
different purposes.</p>
<p>Moreover, a person builds sequences of actions naturally to achieve the
objective and, as discussed, adapts them to the environment. The skills are
often combined together one after the other, based on the scenario but also
based on the result and position of the previous execution. Occasionally, it
can happen that part of an action is used and part of another action, mixing
previously learned movements if the situation requires it. This results in the
creation of new combinations and compositions of previously known
activities.</p>
<p>Lastly, dissecting complex challenges requires also decision-making under
uncertainty, which is essential for achieving high-level goals since the
sequence of activities is not always clear in advance. Often, the goal changes
mid-way in response to the environment, or the initial assessment is
sub-optimal or incorrect, forcing a change in planning and a new decision on
what subsequent action to take. So it&#x2019;s worth noting that online decision
under dynamic circumstances and change of skill executed allows a person to
navigate the complexities of daily scenarios with success.</p>
<p>The human mind&#x2019;s capacity for abstraction, planning, and execution is still
a remote objective for robotics <span class="citation"
data-cites="konidaris2019necessity">(Konidaris 2019)</span>. This level of
adaptation to the environment and building of compounded behaviors is still a
hard challenge to solve nowadays.</p>
<p>For this reason, robots currently are not pervasive in society like other
technologies. Humanoid robots have little if no presence and, despite the
potential different uses, are relegated to mainly interaction and exhibition
duties. The majority of robots work in a controlled environment, like
factories, where the surroundings are specifically designed for them. The
actions taken are repetitive, fixed, and in contact with a simple, defined set
of objects.</p>
<p>Furthermore, even if some robots are able to integrate into semi-structured
environments (for example, the robotic vacuum cleaners for homes or lawnmowers
for gardens), they are specialized to a single task in a single scenario.
Multi-purpose robots require a more human-compatible design and a higher
degree of intelligent behavior <span class="citation"
data-cites="dechant2021toward">(DeChant e Bauer 2021)</span>, but versatile
humanoid robots are still not pervasive in the current status of society.</p>
<p>In this study, we propose a computational model that is biologically
inspired. Our approach consists in the use of mathematics and artificial
intelligence to emulate human abstraction and adaptation capabilities in the
execution of a series of primitive actions. We want to prove how demonstrating
basic movements to a robot and composing them together with flexibility may
lead to achieving complex tasks of various natures. Specifically, movement
primitives are reused and combined differently for different goals, avoiding
explicit teaching of multiple objectives. The trajectories for the skills
learned are adapted to the environment and partially composed thanks to the
interpolation abilities of Conditional Neural Movement Primitives (CNMP)
networks <span class="citation" data-cites="Ugur-RSS-19">(Ugur 2019)</span>.
Lastly, the approach has been implemented and tested on an anthropomorphic
robot and on an industrial collaborative robot.</p>
</section>
<section id="foochallenges" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span>
Challenges</h2>
<p>Robotics dominates many fields, but as discussed, often the environment is
controlled, designed to help the robot in its task, and not human-friendly. If
the purpose is to integrate robots into the human environment, robots must
adapt to humans, not vice-versa. All environments in which humans are present
are not organized or predictable, and this means one issue is that robots have
to accommodate for these conditions. A challenge is definitely to introduce
the machine to an unstructured environment, and this implies many
sub-issues.</p>
<p>Having surroundings that might change forces the machines to have a great
amount of perception. The system has to be extremely aware of the objects and
people around it to operate in a safe and meaningful way. This translates into
equipping many sensors and using real-time data from all available sources.
Moreover, the machine cannot rely on these detection instruments mounted on
the external world since a humanoid robot is expected to be mobile. Having a
multi-purpose system that can act in different scenarios implies, indeed, a
self-contained arrangement of sensors.</p>
<p>The perception brings, in cascade, the necessity of storing this
information and creating an internal copy of the surroundings that works as a
base for planning and future predictions. Creating a digital twin for the
environment is not essential for all the actions since some of them can be
executed in real-time, but it is required to plan their effects and combine
results together. For example, if a sponge is needed to clean a table, it
would be faster to have the knowledge of its last position, but it can also be
researched on demand and used while observing the effects in real-time till
the table is clean. On the other hand, complex actions that combine multiple
primitives need a future prediction of their effects on the environment, so
its internal representation is required.</p>
<p>With changing surroundings, it is possible also that the expected position
of objects is no longer consistent with the representation. This forces the
system to find an alternative or explore the environment till the object is
found. Other kinds of exploration possible are the exploration of the action
space to infer new actions and results, or the exploration of objects&#x2019;
capabilities to learn new affordances and usages. <span class="citation"
data-cites="Ahmetoglu_2022">(Ahmetoglu et al. 2022)</span></p>
<p>Another factor worth taking into consideration is the planning subject.
Plans have to be structured in a meaningful way otherwise, an incorrect
sequence won&#x2019;t just produce an incorrect result but might bring the system
further away from the final goal. The combinations of actions generated
usually have importance in the order of execution, so the product of the
skills has to be considered carefully.</p>
<p>Furthermore, objects and tools are usually designed for humans, so their
capabilities might vary depending on the machine used and might influence the
actions in the planning phase. Giving meanings to the objects, both in terms
of affordances and representations, is still a tricky challenge in robotics
and partially involves the previously investigated challenges of planning and
exploration.</p>
<p>Also, obstacle avoidance, whenever there is an object in the trajectory of
movement, is a factor to take into consideration. The robot is required to be
aware of the surroundings and itself, not to collide, hurt, damage, or just
fail the designated goal. Humans adapt previously known actions whenever an
obstacle or an impediment is present.</p>
<p>Part of the adaptation challenge is also being able to transfer the skills
known to new locations and scenarios. For example, learning how to turn a key
for the door and use the action for the key of the car or the knob to turn on
the stove. This is an essential capability that is difficult to implement in a
machine.</p>
<p>Another more hidden challenge is how the actions are merged among them.
Usually, humans, when they pass from one action to another, apply a smooth
transition. This means that the movements don&#x2019;t have to fully start and end as
they are learned, or the result will be artificial and sub-optimal.</p>
<p>Furthermore, object handling, grasping, and manipulation present some
issues that are the object of research. How to pick the item desired, where,
with which grasp, and with which force intensity are issues that can undermine
the final result.</p>
<p>Lastly, another challenge that will be encountered is the recognition when
the action is completed. Being aware of the right final state is essential for
successfully matching the expectations for the goal requested.</p>
<p>These challenges discussed are crucial aspects to consider, but not all of
them will be addressed in this project, and some will also be simplified.
Nevertheless, it&#x2019;s worth noting the scope and limitations of this work and the
boundaries within which the research operates.</p>
</section>
<section id="fooobjectives" class="level2" data-number="1.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span>
Objectives</h2>
<p>The aim of this research is to investigate novel skill generation by
combining previously taught ones with the use of CNMPs. The research aims to
be applied to robotics scenarios involving trajectories for object
manipulation and high-level goal achievement. The generation of new
combinations of skills will be performed by connecting skill segments that the
robot learned by demonstration. The amount of demonstrations given should be
reasonable for the system to be applied in real life by a human. The
combination of actions will be investigated in both the concatenation of
trajectories end-to-end and the use of parts of them. The ultimate goal is to
create a system that allows a robot, given some demonstrations, to reuse the
skills acquired to complete different objectives whose trajectories were never
taught explicitly. Furthermore, the adaptation should be acceptable in
different configurations of the environment and, ideally in different
scenarios.</p>
</section>
<section id="foothesis-structure" class="level2" data-number="1.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Thesis
Structure</h2>
<p>Accordingly, the remainder of this thesis is structured as follows.</p>
<p>discusses the background of the topic, the current advancements in the
field, and the related research with a literature review.</p>
<p>In <a href="#foochap:platforms" data-reference-type="ref"
data-reference="chap:platforms">3</a> the instruments and frameworks used in
this research are listed and analyzed to be able to understand the initial
setup and replicate the results.</p>
<p>The <a href="#foochap:design" data-reference-type="ref"
data-reference="chap:design">4</a> explains the design and architecture of the
proposed method. In order to understand the logic, the conceptual passages and
mathematical background.</p>
<p>The <a href="#foochap:implementation" data-reference-type="ref"
data-reference="chap:implementation">5</a> analyzes the key points of the
implemented solution through the explanation of the most important passages in
the code developed.</p>
<p>The <a href="#foochap:validation" data-reference-type="ref"
data-reference="chap:validation">6</a> shows the final results and the testing
on real-life robotic platforms.</p>
<p>Finally, concludes this thesis by summarising its main contribution and
future work.</p>
</section>
</section>
<section id="foochap:background" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> State of the
Art</h1>
<p>Key challenge is creating robots that are capable of directly interacting
with the world around them and robot manipulation is central in this. <span
class="citation" data-cites="kroemer2021review">(Kroemer, Niekum, e Konidaris
2021)</span></p>
<p>The ability to solve high level goals in robot is increasing <span
class="citation" data-cites="gupta2019relay">(Gupta et al. 2019)</span>, <span
class="citation" data-cites="simeonov2021long">(Simeonov et al. 2021)</span>
thanks to ...</p>
<p>Some may follow natural language instructions <span class="citation"
data-cites="hu2019hierarchical">(Hu et al. 2019)</span> Gaussian processes are
used in ML <span class="citation" data-cites="seeger2004gaussian">(Seeger
2004)</span> For instance in <span class="citation"
data-cites="Ugur-RSS-19">(Ugur 2019)</span> the authors propose a novel ...
And in <span class="citation"
data-cites="DBLP:journals/corr/abs-1807-01613">(Garnelo et al. 2018)</span> we
can find Conditional Neural Processes.</p>
<section id="foogaussian-processes" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Gaussian
Processes</h2>
</section>
<section id="foocnp" class="level2" data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span> CNP</h2>
</section>
<section id="foocnmps" class="level2" data-number="2.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span>
CNMPs</h2>
<p>Description in detail of CNMP work Allows to learn skills in tens,
ratherthousands, of real-world interactions</p>
</section>
</section>
<section id="foochap:platforms" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span>
Platforms</h1>
<p>In this chapter, all the physical and digital platforms utilized will be
explained to give a proper understanding of the initial architecture and a
more comprehensive idea of the environment of the experiments. The first part
will state the devices and their setup, capabilities, and configuration used.
Subsequently, the frameworks and libraries employed will be listed and
described.</p>
<section id="foobaxter-robot" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Baxter
Robot</h2>
<p>The Baxter robot is an industrial robot built by Rethink Robotics in 2011
<span class="citation" data-cites="wiki:baxter">(Wikipedia contributors
2023)</span>. The platform [Fig. <a href="#foofig:baxter"
data-reference-type="ref" data-reference="fig:baxter">3.1</a>] has two robotic
arms with interchangeable grippers at the wrist (End Effectors). The robot is
180cm tall and, with its pedestal, weighs 140 kg. The arms have 7 degrees of
freedom (DOF), which implies they have seven joints each. This makes it
kinematic redundant, meaning that for some points reached in space, multiple
pose configurations of the arms are possible. These two factors combined allow
the robot to have an impressive capability in manipulation. The robot can be
equipped with suction caps or two-jaw parallel grippers; we chose the last
ones in our configuration. The gripper, in addition to its position, also
offers information regarding the force applied while grasping an object.</p>
<p>The robot was designed with attention to collaborative tasks with humans.
For this reason, it eases teaching by demonstration by having integrated two
touch sensors in the wrists that unlock the motors of the arms, allowing the
user to move them easily and record the trajectories executed. The robot helps
the movement with a feature called "Zero-g mode", in which the weight of the
joints is neutralized actively by the motors. This enables the teaching expert
to demonstrate movements in a similar environment without gravity, without
having to carry the instrumentation load in time constantly. Furthermore, the
robot has many input buttons and LEDs, they are present on the hands, arms,
and chest, and they allow the programming of custom behaviors. They are
especially useful in retrieving and giving instructions to the robot without
reaching a computer, like closing the gripper at the desired moment, getting
feedback, or starting the trajectory recordings.</p>
<p>Another feature worth mentioning is the increased safety of operating
around humans. Thanks to active and passive safety systems equipped in the
platform, it doesn&#x2019;t require a cage for protection. On the other hand, making
the robot less hazardous comes with the cost of precision. A motor driving a
spring that drives Baxter&#x2019;s arm instead of just a direct motor impacts the
precision of movements, sometimes in terms of centimeters. This doesn&#x2019;t make
the robot perfectly suitable for industrial applications, but especially
appropriate for research and for the adaptability in our project.</p>
<p>The head of the robot includes a ring of sonar sensors for people
detection, a wide-angle camera, and a movable display that acts like a face.
Another benefit of the robot lies at the end of both hands. Immediately next
to the attachment for the tools, an infrared (IR) sensor provides data on the
distance from a solid object (i.e., a table) and an inertial measurement unit
(IMU). Moreover, an embedded RGB camera is also present, allowing to see
closely the object approached or to change the point of view on it without
additional external cameras.</p>
<figure id="foofig:baxter">
<img src="Images/baxter.png" />
<figcaption>Baxter Robot platform</figcaption>
</figure>
</section>
<section id="foour10-robot" class="level2" data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> UR10
Robot</h2>
<p>UR10 is a single industrial robot arm that shines in reliability and
precision [Fig. <a href="#foofig:ur10" data-reference-type="ref"
data-reference="fig:ur10">3.2</a>]. It has been manufactured by Universal
Robots and combines long reach with a high payload <span class="citation"
data-cites="url:ur10">(Universal Robots 2023)</span>. It is intended for
medium-duty tasks, so it&#x2019;s compact in its overall dimensions compared to a
fully intended industrial robot. It can reach an impressive height of 2.3m on
its pedestal. In our experiments, it was mounted on a pedestal 0.85m tall to
increase the reachability at table level.</p>
<p>The arm has a reaching radius of 1,3m from the mounting point, which
implies a workspace of approximately 5,3 square meters at the base level. The
robot has 6 Degrees of Freedom, with six rotating joints. It is able to reach
any point in its reaching radius but has no kinematics redundancy, meaning
only one position is possible for any given point. The total payload that can
be carried is 10 kg.</p>
<figure id="foofig:ur10">
<img src="Images/ur10.png" />
<figcaption>UR10 Robot platform</figcaption>
</figure>
<p>Like the previous robot, this one is designed to work collaboratively with
humans. It features built-in safety features, such as force/torque sensors, to
detect and respond to external forces or unexpected events. A button to
release the motor breaks is present on the floating touch screen and allows
the robot&#x2019;s motion by hand. This robot also doesn&#x2019;t require a safety cage
around for protection, but the emergency stop button always has to be within
easy reach.</p>
<p>The robot design emphasizes modularity, making it easier for users to
customize and adapt it for different tasks or use various end effectors. We
coupled it with a three-finger gripper described in the next paragraph.</p>
</section>
<section id="foof-robotiq-gripper" class="level2" data-number="3.3">
<h2 data-number="3.3"><span class="header-section-number">3.3</span> 3F
Robotiq Gripper</h2>
<p>At the end of the UR10 Robotic Arm, a Robotiq 3-Finger Adaptive Robot
Gripper was mounted [Fig. <a href="#foofig:gripper3f"
data-reference-type="ref" data-reference="fig:gripper3f">3.3</a>]. The gripper
has a human-inspired design and has three fingers with three joints each <span
class="citation" data-cites="url:3fgripper">(Robotiq 2023a)</span>. The
physical platform was chosen for its precision and safety, and it pairs well
with the UR10 capabilities. The gripper offers different grip modes; the ones
available are: "basic", "wide", "scissor" and "pinch". Each is appropriate for
distinctive objects to grip; the basic one is the most versatile, but the wide
one has more stability for big or long objects, and the pinch one is the best
for small objects. The "scissor mode" closes together the two fingers on the
same side, for high-precision manipulation. We mainly used the "pinch"
setting.</p>
<figure id="foofig:gripper3f">
<img src="Images/gripper3f.png" />
<figcaption>3F Robotiq Gripper platform</figcaption>
</figure>
<p>The gripper has a mass of 2.3 kg in contrast with a grip payload of 10 kg.
The grip force applied can range from 30 to 70 N, depending on the grip mode
selected. The precision declared is up to 0.05 mm.</p>
<p>This platform was designed as well for collaborative robotic applications,
allowing it to work safely alongside human operators. It incorporates safety
features to detect and respond to external forces, stopping in case of high
forces applied. The torque and speed of gripping data are available and
exposed through a dedicated ROS topic. Speed and torque are also adjustable
for the intended use. It is possible to control and retrieve data for each
finger individually.</p>
</section>
<section id="fooft-300-s-force-torque-sensor" class="level2"
data-number="3.4">
<h2 data-number="3.4"><span class="header-section-number">3.4</span> FT 300-S
Force Torque Sensor</h2>
<p>Between the UR10 robot and the 3F Finger Gripper, an FT 300-S Force Torque
Sensor [Fig. <a href="#foofig:ft-sensor" data-reference-type="ref"
data-reference="fig:ft-sensor">3.4</a>] was mounted to increase precision and
repeatability. The sensor offers high-resolution real-time measurements
regarding the force and torque applied to the three space dimensions and
improves the capabilities of the robot, making it able to detect the payload
carried or the amount of pressure between the object carried and the static
environment (i.e., the table).</p>
<p>The device was built for compatibility with the Universal Robot series and
has an IP65 rating. It also enables precise object placement such as
alignment, indexing, and insertion <span class="citation"
data-cites="url:ftsensor">(Robotiq 2023b)</span>. The FT 300-S is commonly
used in tasks where force and torque sensing are critical, but we used it to
increase the reliability of the payload measurements.</p>
<figure id="foofig:ft-sensor">
<img src="Images/ft_sensor.png" />
<figcaption>FT 300-S Force Torque Sensor</figcaption>
</figure>
</section>
<section id="fooframeworks" class="level2" data-number="3.5">
<h2 data-number="3.5"><span class="header-section-number">3.5</span>
Frameworks</h2>
<section id="fooros" class="level5" data-number="3.5.0.0.1">
<h5 data-number="3.5.0.0.1"><span
class="header-section-number">3.5.0.0.1</span> ROS</h5>
<p>The Robot Operating System (ROS) <span class="citation"
data-cites="url:ros">(ROS 2023)</span> is a framework to ease the deployment
of robot applications. The system is a set of software libraries and tools
that combine the state-of-the-art drivers for the most common robot interfaces
and contain the most used algorithms for robotics. ROS acts like a middleware
framework, allowing ease of communication of software with the robotic
hardware. It is widely used in research, from mobile robots to
manipulators.</p>
<p>It&#x2019;s platform-independent and open source, so it is possible to create a
custom robotics device compatible with it, and it&#x2019;s possible to develop
personalized libraries. Its modular architecture allows the creation or use of
a series of executable pieces of code called nodes, which run on a single or
even on many computers. The nodes communicate among themselves with messages
in the form of data structures previously defined. The distributed system
allows to decuple the computation of heavy tasks, like vision, 3D
reconstruction, and navigation, from the robot&#x2019;s hardware.</p>
<p>Communication occurs through "topics" and "services". Nodes running on any
computer ping the "master node" designated to retrieve all the possible topics
and services exposed from other nodes in the network. A node can be a
"publisher" or "subscriber" to a topic, sending messages to it or receiving
messages from it. Communication with topics is not blocking and it is
many-to-many: multiple nodes can publish, and simultaneously, multiple nodes
can subscribe to a topic. Services work in a similar way, but the
communication blocks the computation till the data requested is retrieved.
Their mechanism is analogous to Remote Procedure Calls (RPCs).</p>
<p>ROS is available with two commonly used programming languages: Python and
C++. We used the Python version with the "rospy" package in the experiments
with both robots.</p>
</section>
<section id="foopytorch" class="level5" data-number="3.5.0.0.2">
<h5 data-number="3.5.0.0.2"><span
class="header-section-number">3.5.0.0.2</span> Pytorch</h5>
<p>Pytorch is a Deep Learning framework <span class="citation"
data-cites="paszke2019pytorch">(Paszke et al. 2019)</span> that focuses on
speed and usability with an imperative and Pythonic programming style. The
Python library <span class="citation" data-cites="url:pytorch">(Pytorch
2023)</span> offers a wide variety of models and building blocks for
constructing neural networks. By design, it eases the debugging for the user
with a rich ecosystem of dedicated tools. It works on the CPU and on hardware
accelerators like GPUs. For this reason, PyTorch provides a multi-dimensional
array called a tensor, which is similar to NumPy arrays. Conversions among
both of them will be present in the code implementation.</p>
</section>
<section id="foojupyter-notebook" class="level5" data-number="3.5.0.0.3">
<h5 data-number="3.5.0.0.3"><span
class="header-section-number">3.5.0.0.3</span> Jupyter Notebook</h5>
<p>Jupyter Notebook is an open-source interactive web application <span
class="citation" data-cites="url:jupyter">(Jupyter Notebook 2023)</span>. It
supports multiple programming languages and offers a cell-based environment
where code and description/graphical results can be blended. Jupyter Notebook
integrates seamlessly with popular Python libraries, such as NumPy, Pandas,
Matplotlib, and scikit-learn; some of them will be described later. It was
used occasionally in our experiments to provide a fast and interactive coding
experience with Python. The notebooks can be easily shared, and the process is
clearly visualized. It was specifically useful in plotting multiple graphs
during the training stages of neural networks or debugging operations on
multi-dimensional arrays.</p>
</section>
<section id="fooanaconda-and-python-libraries" class="level5"
data-number="3.5.0.0.4">
<h5 data-number="3.5.0.0.4"><span
class="header-section-number">3.5.0.0.4</span> Anaconda and Python
Libraries</h5>
<p>Anaconda is an open-source software that contains open-source tools and
packages for data science, machine learning, and scientific computing <span
class="citation" data-cites="url:anaconda">(Anaconda 2023)</span>. It has been
used to track the packages utilized in the robotic platforms and in the
models&#x2019; development and training. The Conda package manager was the most used
tool to easily install, update, and manage various software packages and
dependencies. Some of the most important libraries are listed below.</p>
<section id="foomatplotlib" class="level6" data-number="3.5.0.0.4.1">
<h6 data-number="3.5.0.0.4.1"><span
class="header-section-number">3.5.0.0.4.1</span> MatplotLib</h6>
<p>Matplotlib is a comprehensive 2D plotting library for Python that generates
high-quality charts, plots, and visualizations. It has been widely used to
double-check the quality of the training or plot the trajectories recorded
with the robots.</p>
</section>
<section id="foonumpy" class="level6" data-number="3.5.0.0.4.2">
<h6 data-number="3.5.0.0.4.2"><span
class="header-section-number">3.5.0.0.4.2</span> Numpy</h6>
<p>NumPy is a package for numerical computing in Python. It supports large,
multi-dimensional arrays and matrices and a collection of mathematical
functions to operate on these arrays. It has been used for array slicing,
normalization, and smoothing data.</p>
</section>
</section>
</section>
</section>
<section id="foochap:design" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Design</h1>
<p>Intro to the combination of trajectories</p>
<section id="fooend-to-end-concatenation" class="level2" data-number="4.1">
<h2 data-number="4.1"><span class="header-section-number">4.1</span>
End-To-End Concatenation</h2>
</section>
<section id="foopartial-combination" class="level2" data-number="4.2">
<h2 data-number="4.2"><span class="header-section-number">4.2</span> Partial
Combination</h2>
<figure id="foofig:classes">
<embed src="figures/classes.pdf" />
<figcaption>A class diagram created with PlantUML</figcaption>
</figure>
<p>You may want to reference images in your thesis. In this case, you are
encouraged to make them <em>floating</em>, and reference them by means of
labels. For instance, in , we describe a class diagram produced by means of <a
href="http://plantuml.com">PlantUML</a>.</p>
</section>
</section>
<section id="foochap:implementation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span>
Implementation</h1>
<p>Write implementation here.</p>
<div class="sourceCode" id="foolst:pieceofcode" data-float=""
data-language="Java" data-caption="A piece of code"
label="lst:pieceofcode"><pre class="sourceCode java"><code class="sourceCode java"><span id="foolst:pieceofcode-1"><a href="#foolst:pieceofcode-1" aria-hidden="true" tabindex="-1"></a><span class="kw">public</span> <span class="kw">class</span> HelloWorld <span class="op">{</span></span>
<span id="foolst:pieceofcode-2"><a href="#foolst:pieceofcode-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">public</span> <span class="dt">static</span> <span class="dt">void</span> <span class="fu">main</span><span class="op">(</span><span class="bu">String</span><span class="op">[]</span> args<span class="op">)</span> <span class="op">{</span></span>
<span id="foolst:pieceofcode-3"><a href="#foolst:pieceofcode-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Prints &quot;Hello, World&quot; to the terminal window.</span></span>
<span id="foolst:pieceofcode-4"><a href="#foolst:pieceofcode-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">System</span><span class="op">.</span><span class="fu">out</span><span class="op">.</span><span class="fu">println</span><span class="op">(</span><span class="st">&quot;Hello, World&quot;</span><span class="op">);</span></span>
<span id="foolst:pieceofcode-5"><a href="#foolst:pieceofcode-5" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="foolst:pieceofcode-6"><a href="#foolst:pieceofcode-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>You may need to reference listings in your thesis. In this case, you are
encouraged to make them <em>floating</em>, and reference them by means of
labels. For instance, in , we describe an hello world program in Java.</p>
</section>
<section id="foochap:validation" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Validation
and Testing</h1>
<p>Result validation and testing on real-life robots here</p>
</section>
<section id="foochap:conclusions" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span>
Conclusions</h1>
<p>Humans are capable of executing a wide variety of complex actions, based on
prior experience. In this thesis, we provided a possible approach to novel
high-level skill generation by combining movement primitives learned by CNMP
models.</p>
<section id="foofuture-work" class="level2" data-number="7.1">
<h2 data-number="7.1"><span class="header-section-number">7.1</span> Future
work</h2>
<p>Possible future works are ...</p>
<div id="foorefs" class="references csl-bib-body hanging-indent" role="list">
<div id="fooref-Ahmetoglu_2022" class="csl-entry" role="listitem">
Ahmetoglu, Alper, M. Yunus Seker, Justus Piater, Erhan Oztop, e Emre Ugur.
2022. <span>&#xAB;DeepSym: Deep Symbol Generation and Rule Learning for Planning
from Unsupervised Robot Interaction&#xBB;</span>. <em>Journal of Artificial
Intelligence Research</em> 75 (novembre): 709&#x2013;45. <a
href="https://doi.org/10.1613/jair.1.13754">https://doi.org/10.1613/jair.1.13754</a>.
</div>
<div id="fooref-url:anaconda" class="csl-entry" role="listitem">
Anaconda. 2023. <span>&#xAB;Anaconda&#xBB;</span>. <a href="https://www.anaconda.com"
class="uri">https://www.anaconda.com</a>.
</div>
<div id="fooref-deak2003development" class="csl-entry" role="listitem">
Deak, Gedeon O. 2003. <span>&#xAB;The development of cognitive flexibility and
language abilities&#xBB;</span>. <em>Advances in child development and
behavior</em> 31: 273&#x2013;328.
</div>
<div id="fooref-dechant2021toward" class="csl-entry" role="listitem">
DeChant, Chad, e Daniel Bauer. 2021. <span>&#xAB;Toward robots that learn to
summarize their actions in natural language: a set of tasks&#xBB;</span>. In
<em>5th Annual Conference on Robot Learning, Blue Sky Submission Track</em>.
<a
href="https://openreview.net/forum?id=n3AW_ISWCXf">https://openreview.net/forum?id=n3AW_ISWCXf</a>.
</div>
<div id="fooref-egidi2006decomposition" class="csl-entry" role="listitem">
Egidi, Massimo. 2006. <span>&#xAB;Decomposition patterns in problem
solving&#xBB;</span>. <em>Contributions to Economic Analysis</em> 280: 15&#x2013;46.
</div>
<div id="fooref-DBLP:journals/corr/abs-1807-01613" class="csl-entry"
role="listitem">
Garnelo, Marta, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton,
Murray Shanahan, Yee Whye Teh, Danilo J. Rezende, e S. M. Ali Eslami. 2018.
<span>&#xAB;Conditional Neural Processes&#xBB;</span>. <em>CoRR</em> abs/1807.01613. <a
href="http://arxiv.org/abs/1807.01613">http://arxiv.org/abs/1807.01613</a>.
</div>
<div id="fooref-gupta2019relay" class="csl-entry" role="listitem">
Gupta, Abhishek, Vikash Kumar, Corey Lynch, Sergey Levine, e Karol Hausman.
2019. <span>&#xAB;Relay Policy Learning: Solving Long-Horizon Tasks via Imitation
and Reinforcement Learning&#xBB;</span>. <a
href="https://arxiv.org/abs/1910.11956">https://arxiv.org/abs/1910.11956</a>.
</div>
<div id="fooref-horton2012affordances" class="csl-entry" role="listitem">
Horton, Thomas E, Arpan Chakraborty, e Robert St Amant. 2012.
<span>&#xAB;Affordances for robots: a brief survey&#xBB;</span>. <em>AVANT. Pismo
Awangardy Filozoficzno-Naukowej</em> 2: 70&#x2013;84.
</div>
<div id="fooref-hu2019hierarchical" class="csl-entry" role="listitem">
Hu, Hengyuan, Denis Yarats, Qucheng Gong, Yuandong Tian, e Mike Lewis. 2019.
<span>&#xAB;Hierarchical decision making by generating and following natural
language instructions&#xBB;</span>. <em>Advances in neural information processing
systems</em> 32.
</div>
<div id="fooref-url:jupyter" class="csl-entry" role="listitem">
Jupyter Notebook. 2023. <span>&#xAB;Jupyter Notebook&#xBB;</span>. <a
href="https://jupyter.org" class="uri">https://jupyter.org</a>.
</div>
<div id="fooref-karmiloff1994beyond" class="csl-entry" role="listitem">
Karmiloff-Smith, By A. 1994. <span>&#xAB;Beyond modularity: A developmental
perspective on cognitive science&#xBB;</span>. <em>European journal of disorders of
communication</em> 29 (1): 95&#x2013;105.
</div>
<div id="fooref-konidaris2019necessity" class="csl-entry" role="listitem">
Konidaris, George. 2019. <span>&#xAB;On the necessity of abstraction&#xBB;</span>.
<em>Current opinion in behavioral sciences</em> 29: 1&#x2013;7.
</div>
<div id="fooref-kroemer2021review" class="csl-entry" role="listitem">
Kroemer, Oliver, Scott Niekum, e George Konidaris. 2021. <span>&#xAB;A review of
robot learning for manipulation: Challenges, representations, and
algorithms&#xBB;</span>. <em>The Journal of Machine Learning Research</em> 22 (1):
1395&#x2013;1476.
</div>
<div id="fooref-maranesi2014cortical" class="csl-entry" role="listitem">
Maranesi, Monica, Luca Bonini, e Leonardo Fogassi. 2014. <span>&#xAB;Cortical
processing of object affordances for self and others&#x2019; action&#xBB;</span>.
<em>Frontiers in psychology</em> 5: 538.
</div>
<div id="fooref-osiurak2017affordance" class="csl-entry" role="listitem">
Osiurak, Fran&#xE7;ois, Yves Rossetti, e Arnaud Badets. 2017. <span>&#xAB;What is an
affordance? 40 years later&#xBB;</span>. <em>Neuroscience &amp; Biobehavioral
Reviews</em> 77: 403&#x2013;17.
</div>
<div id="fooref-paszke2019pytorch" class="csl-entry" role="listitem">
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, et al. 2019. <span>&#xAB;PyTorch: An Imperative Style,
High-Performance Deep Learning Library&#xBB;</span>. <a
href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</a>.
</div>
<div id="fooref-url:pytorch" class="csl-entry" role="listitem">
Pytorch. 2023. <span>&#xAB;Tensors and Dynamic neural networks in Python with
strong GPU acceleration&#xBB;</span>. <a href="https://pytorch.org"
class="uri">https://pytorch.org</a>.
</div>
<div id="fooref-url:3fgripper" class="csl-entry" role="listitem">
Robotiq. 2023a. <span>&#xAB;3-Finger Adaptive Robot Gripper&#xBB;</span>. <a
href="https://robotiq.com/products/3-finger-adaptive-robot-gripper"
class="uri">https://robotiq.com/products/3-finger-adaptive-robot-gripper</a>.
</div>
<div id="fooref-url:ftsensor" class="csl-entry" role="listitem">
&#x2014;&#x2014;&#x2014;. 2023b. <span>&#xAB;FT 300-S Force Torque Sensor&#xBB;</span>. <a
href="https://robotiq.com/products/ft-300-force-torque-sensor"
class="uri">https://robotiq.com/products/ft-300-force-torque-sensor</a>.
</div>
<div id="fooref-url:ros" class="csl-entry" role="listitem">
ROS. 2023. <span>&#xAB;Robot Operating System&#xBB;</span>. <a
href="https://www.ros.org" class="uri">https://www.ros.org</a>.
</div>
<div id="fooref-seeger2004gaussian" class="csl-entry" role="listitem">
Seeger, Matthias. 2004. <span>&#xAB;Gaussian processes for machine
learning&#xBB;</span>. <em>International journal of neural systems</em> 14 (02):
69&#x2013;106.
</div>
<div id="fooref-simeonov2021long" class="csl-entry" role="listitem">
Simeonov, Anthony, Yilun Du, Beomjoon Kim, Francois Hogan, Joshua Tenenbaum,
Pulkit Agrawal, e Alberto Rodriguez. 2021. <span>&#xAB;A long horizon planning
framework for manipulating rigid pointcloud objects&#xBB;</span>. In <em>Conference
on Robot Learning</em>, 1582&#x2013;1601. PMLR.
</div>
<div id="fooref-Ugur-RSS-19" class="csl-entry" role="listitem">
Ugur, Muhammet Yunus Seker AND Mert Imre AND Justus Piater AND Emre. 2019.
<span>&#xAB;Conditional Neural Movement Primitives&#xBB;</span>. In <em>Proceedings of
Robotics: Science and Systems</em>. FreiburgimBreisgau, Germany. <a
href="https://doi.org/10.15607/RSS.2019.XV.071">https://doi.org/10.15607/RSS.2019.XV.071</a>.
</div>
<div id="fooref-url:ur10" class="csl-entry" role="listitem">
Universal Robots. 2023. <span>&#xAB;UR10 (robot) &#x2014; medium duty industrial
collaborative robot&#xBB;</span>. <a
href="https://www.universal-robots.com/products/ur10-robot/"
class="uri">https://www.universal-robots.com/products/ur10-robot/</a>.
</div>
<div id="fooref-wiki:baxter" class="csl-entry" role="listitem">
Wikipedia contributors. 2023. <span>&#xAB;Baxter (robot) &#x2014;
<span>Wikipedia</span><span>,</span> The Free Encyclopedia&#xBB;</span>. <a
href="https://en.wikipedia.org/w/index.php?title=Baxter_(robot)&amp;oldid=1183931177"
class="uri">https://en.wikipedia.org/w/index.php?title=Baxter_(robot)&amp;oldid=1183931177</a>.
</div>
</div>
</section>
</section>                    
                </div>
            </div>

</script>
            <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>
        </div>
    </body>
</html>
