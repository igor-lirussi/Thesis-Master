<!DOCTYPE html>
<html  lang="it"  dir="ltr">

    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Master Thesis - Igor Lirussi</title>
        <link rel="shortcut icon" type="image/png" href="favicon.png">
        <link rel="apple-touch-icon-precomposed" href="apple-touch-icon.png">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/uikit/2.26.4/css/uikit.gradient.css">

        <link rel="stylesheet" href="style.css">
        <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />
        <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
        <script src="./pandoc-uikit/uikit.js"></script>
        <script src="./pandoc-uikit/scripts.js"></script>
        <script src="./pandoc-uikit/jquery.sticky-kit.js"></script>

        <meta name="generator" content="pandoc-uikit" />
                <meta name="author" content="Lirussi Igor" />
                        <meta name="date" content="2023-11-26" />
                <title>Master Thesis - Igor Lirussi</title>
        <style type="text/css">code{white-space: pre;}</style>
                        <style type="text/css">
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            .sourceCode { overflow: visible; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #aaaaaa;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
            div.sourceCode
              {  background-color: #f8f8f8; }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #ef2929; } /* Alert */
            code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #204a87; } /* Attribute */
            code span.bn { color: #0000cf; } /* BaseN */
            code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #4e9a06; } /* Char */
            code span.cn { color: #8f5902; } /* Constant */
            code span.co { color: #8f5902; font-style: italic; } /* Comment */
            code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
            code span.dt { color: #204a87; } /* DataType */
            code span.dv { color: #0000cf; } /* DecVal */
            code span.er { color: #a40000; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #0000cf; } /* Float */
            code span.fu { color: #204a87; font-weight: bold; } /* Function */
            code span.im { } /* Import */
            code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
            code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
            code span.ot { color: #8f5902; } /* Other */
            code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
            code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
            code span.ss { color: #4e9a06; } /* SpecialString */
            code span.st { color: #4e9a06; } /* String */
            code span.va { color: #000000; } /* Variable */
            code span.vs { color: #4e9a06; } /* VerbatimString */
            code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
        </style>
                        <link rel="stylesheet" href="./pandoc-uikit/uikit.css" />
                                          <script
                                          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
                                          type="text/javascript"></script>
                               
    </head>

    <body>


        <div class="uk-container uk-container-center uk-margin-top uk-margin-large-bottom">

                        <div class="uk-grid" data-uk-grid-margin>
                <div class="uk-width-1-1">
                    <h1 class="uk-heading">Master Thesis - Igor Lirussi</h1>
                                        <h1 class="uk-heading" style="margin:0">2023-11-26</h1>
                                                            <p class="uk-text-large">Lirussi
Igor</p>
                                    </div>
            </div>
            
            <div class="uk-grid" data-uk-grid-margin >          
                <div class="uk-width-medium-1-4">
                    <div class="uk-overflow-container" data-uk-sticky="{top:25,media: 768}">
                        <div class="uk-panel uk-panel-box menu-begin" >

                                                        <ul>
                                                        <li><a
                                                        href="#fooabstract"
                                                        id="footoc-abstract">Abstract</a></li>
                                                        <li><a
                                                        href="#foosection"
                                                        id="footoc-section"></a></li>
                                                        <li><a
                                                        href="#fooacknowledgements"
                                                        id="footoc-acknowledgements">Acknowledgements</a></li>
                                                        <li><a
                                                        href="#foolist-of-symbols"
                                                        id="footoc-list-of-symbols">List
                                                        of Symbols</a></li>
                                                        <li><a
                                                        href="#foolist-of-acronymsabbreviations"
                                                        id="footoc-list-of-acronymsabbreviations">List
                                                        of
                                                        Acronyms/Abbreviations</a></li>
                                                        <li><a
                                                        href="#foochap:introduction"
                                                        id="footoc-chap:introduction"><span
                                                        class="toc-section-number">1</span>
                                                        Introduction</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foooverview"
                                                        id="footoc-overview"><span
                                                        class="toc-section-number">1.1</span>
                                                        Overview</a></li>
                                                        <li><a
                                                        href="#foochallenges"
                                                        id="footoc-challenges"><span
                                                        class="toc-section-number">1.2</span>
                                                        Challenges</a></li>
                                                        <li><a
                                                        href="#fooobjectives"
                                                        id="footoc-objectives"><span
                                                        class="toc-section-number">1.3</span>
                                                        Objectives</a></li>
                                                        <li><a
                                                        href="#foothesis-structure"
                                                        id="footoc-thesis-structure"><span
                                                        class="toc-section-number">1.4</span>
                                                        Thesis
                                                        Structure</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:background"
                                                        id="footoc-chap:background"><span
                                                        class="toc-section-number">2</span>
                                                        State of the Art</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foogaussian-processes"
                                                        id="footoc-gaussian-processes"><span
                                                        class="toc-section-number">2.1</span>
                                                        Gaussian
                                                        Processes</a></li>
                                                        <li><a
                                                        href="#fooconditional-neural-processes"
                                                        id="footoc-conditional-neural-processes"><span
                                                        class="toc-section-number">2.2</span>
                                                        Conditional Neural
                                                        Processes</a></li>
                                                        <li><a
                                                        href="#fooconditional-neural-movement-primitives"
                                                        id="footoc-conditional-neural-movement-primitives"><span
                                                        class="toc-section-number">2.3</span>
                                                        Conditional Neural
                                                        Movement
                                                        Primitives</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:platforms"
                                                        id="footoc-chap:platforms"><span
                                                        class="toc-section-number">3</span>
                                                        Platforms</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foobaxter-robot"
                                                        id="footoc-baxter-robot"><span
                                                        class="toc-section-number">3.1</span>
                                                        Baxter Robot</a></li>
                                                        <li><a
                                                        href="#foour10-robot"
                                                        id="footoc-ur10-robot"><span
                                                        class="toc-section-number">3.2</span>
                                                        UR10 Robot</a></li>
                                                        <li><a
                                                        href="#foof-robotiq-gripper"
                                                        id="footoc-f-robotiq-gripper"><span
                                                        class="toc-section-number">3.3</span>
                                                        3F Robotiq
                                                        Gripper</a></li>
                                                        <li><a
                                                        href="#fooft-300-s-force-torque-sensor"
                                                        id="footoc-ft-300-s-force-torque-sensor"><span
                                                        class="toc-section-number">3.4</span>
                                                        FT 300-S Force Torque
                                                        Sensor</a></li>
                                                        <li><a
                                                        href="#fooframeworks"
                                                        id="footoc-frameworks"><span
                                                        class="toc-section-number">3.5</span>
                                                        Frameworks</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:design"
                                                        id="footoc-chap:design"><span
                                                        class="toc-section-number">4</span>
                                                        Design</a>
                                                        <ul>
                                                        <li><a
                                                        href="#fooend-to-end-concatenation"
                                                        id="footoc-end-to-end-concatenation"><span
                                                        class="toc-section-number">4.1</span>
                                                        End-To-End
                                                        Concatenation</a></li>
                                                        <li><a
                                                        href="#foopartial-combination"
                                                        id="footoc-partial-combination"><span
                                                        class="toc-section-number">4.2</span>
                                                        Partial
                                                        Combination</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:implementation"
                                                        id="footoc-chap:implementation"><span
                                                        class="toc-section-number">5</span>
                                                        Implementation</a></li>
                                                        <li><a
                                                        href="#foochap:validation"
                                                        id="footoc-chap:validation"><span
                                                        class="toc-section-number">6</span>
                                                        Validation and
                                                        Testing</a></li>
                                                        <li><a
                                                        href="#foochap:conclusions"
                                                        id="footoc-chap:conclusions"><span
                                                        class="toc-section-number">7</span>
                                                        Conclusions</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foofuture-work"
                                                        id="footoc-future-work"><span
                                                        class="toc-section-number">7.1</span>
                                                        Future work</a></li>
                                                        </ul></li>
                                                        </ul>
                            
                        </div>
                    </div>
                </div>

                <div class="uk-width-medium-3-4">
<div class="titlepage">
<div class="center">
<p><strong>ALMA MATER STUDIORUM &#x2013; UNIVERSITY OF BOLOGNA<br />
CESENA CAMPUS</strong><br />
</p>
<p>School of Engineering and Architecture<br />
Second Cycle Degree/Two-year Master in<br />
Computer Science and Engineering</p>
<p><strong>Novel robotic skill synthesis<br />
with Conditional Neural<br />
Movement Primitives</strong></p>
<p>Master&#x2019;s thesis in<br />
<span class="smallcaps">Intelligent Robotic Systems</span></p>
<div class="flushleft">
<p><em>Supervisor</em><br />
<strong>Prof.</strong> <strong>Andrea Roli</strong><br />
<em>Co-Supervisor</em><br />
<strong>Prof.</strong> <strong>Emre U&#x11F;ur</strong><br />
(Bo&#x11F;azi&#xE7;i University, Istanbul)</p>
</div>
<div class="flushright">
<p><em>Candidate</em><br />
<strong>Igor Lirussi</strong></p>
</div>
<p><br />
</p>
<p>Academic Year 2022-2023</p>
</div>
</div>
<section id="fooabstract" class="level1 unnumbered">
<h1 class="unnumbered">Abstract</h1>
<p>Max 2000 characters, strict. UniBo has that limit in the upload system!
Will write at the end.</p>
</section>
<section id="foosection" class="level1 unnumbered">
<h1 class="unnumbered"></h1>
<div class="flushright">
<p><em>Dedication here</em></p>
</div>
</section>
<section id="fooacknowledgements" class="level1 unnumbered">
<h1 class="unnumbered">Acknowledgements</h1>
<p>Acknowledgments here.</p>
</section>
<section id="foolist-of-symbols" class="level1 unnumbered">
<h1 class="unnumbered">List of Symbols</h1>
<div class="tabbing">
<p>&#x304;&#x304;</p>
<p><span class="math inline">\(a_{ij}\)</span></p>
<p>Description of <span class="math inline">\(a_{ij}\)</span></p>
<p><br />
<span class="math inline">\(t\)</span></p>
<p>time</p>
<p><br />
<span class="math inline">\(r\)</span></p>
<p>representation</p>
<p><br />
<span class="math inline">\(x\)</span></p>
<p>target point</p>
<p><br />
<span class="math inline">\(SM(t)\)</span></p>
<p>Sensorimotor data at time t</p>
<p><br />
<span class="math inline">\(E\)</span></p>
<p>Encoder network</p>
<p><br />
<span class="math inline">\(Q\)</span></p>
<p>Decoder network</p>
<p><br />
<span class="math inline">\(O\)</span></p>
<p>Set of observation points</p>
<p><br />
<span class="math inline">\(D\)</span></p>
<p>Set of demonstration trajectories</p>
<p><br />
<span class="math inline">\(T\)</span></p>
<p>Set of target points desired</p>
<p><br />
</p>
<p><br />
<span class="math inline">\(\alpha\)</span></p>
<p>Blending parameter <em>or</em> scale</p>
<p><br />
<span class="math inline">\(\beta_t(i)\)</span></p>
<p>Backward variable</p>
<p><br />
<span class="math inline">\(\gamma\)</span></p>
<p>External parameter (Task parameter)</p>
<p><br />
<span class="math inline">\(\theta\)</span></p>
<p>Parameter set of Decoder Network</p>
<p><br />
<span class="math inline">\(\phi\)</span></p>
<p>Parameter set of Encoder Network</p>
<p><br />
<span class="math inline">\(\sigma\)</span></p>
<p>Standard Deviation</p>
<p><br />
<span class="math inline">\(\mu\)</span></p>
<p>Mean</p>
<p><br />
<span class="math inline">\(\tau\)</span></p>
<p>Trajectory</p>
<p><br />
</p>
</div>
</section>
<section id="foolist-of-acronymsabbreviations" class="level1 unnumbered">
<h1 class="unnumbered">List of Acronyms/Abbreviations</h1>
<div class="tabbing">
<p>&#x304;&#x304;</p>
<p>1D</p>
<p>One Dimensional</p>
<p><br />
2D</p>
<p>Two Dimensional</p>
<p><br />
3D</p>
<p>Three Dimensional</p>
<p><br />
IR</p>
<p>Infrared</p>
<p><br />
ROS</p>
<p>Robot Operating System</p>
<p><br />
RGB</p>
<p>Red Green Blue</p>
<p><br />
RGBD</p>
<p>Red Green Blue Depth</p>
<p><br />
SL</p>
<p>Supervised Learning</p>
<p><br />
LfD</p>
<p>Learning from Demonstration</p>
<p><br />
DL</p>
<p>Deep Learnign</p>
<p><br />
GMM</p>
<p>Gaussian Mixture Model</p>
<p><br />
HMM</p>
<p>Hidden Markov Model</p>
<p><br />
GP</p>
<p>Gaussian Processes</p>
<p><br />
NP</p>
<p>Neural Processes</p>
<p><br />
CNP</p>
<p>Conditional Neural Processes</p>
<p><br />
DNP</p>
<p>Dynamic Movement Primitives</p>
<p><br />
Pro-MP</p>
<p>Probabilistic Movement Primitives</p>
<p><br />
CNMP</p>
<p>Conditional Neural Movement Primitives</p>
<p><br />
YOLO</p>
<p>You Only Look Once model</p>
<p><br />
UR10</p>
<p>Universal Robot 10</p>
<p><br />
DOF</p>
<p>Degrees of freedom</p>
<p><br />
IMU</p>
<p>Inertial measurement unit</p>
<p><br />
RPC</p>
<p>Remote Procedure Call</p>
<p><br />
CPU</p>
<p>Central Processing Unit</p>
<p><br />
GPU</p>
<p>Graphic Processing Unit</p>
<p><br />
</p>
</div>
</section>
<section id="foochap:introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span>
Introduction</h1>
<section id="foooverview" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Overview</h2>
<p>Humans have a remarkable ability to achieve complex goals in a wide variety
of tasks. A person is usually exposed to different scenarios during the day,
starting from the home environment to the commute, work, mealtime, and so on.
The versatility of our species is a key factor, and the human cognitive
flexibility has been appointed as a major driver in evolution <span
class="citation" data-cites="deak2003development">(Deak 2003)</span>, <span
class="citation" data-cites="karmiloff1994beyond">(Karmiloff-Smith
1994)</span>. Some situations are more complicated than others; nevertheless,
regardless of their difference, humans excel in meeting the different demands
to solve the tasks desired.</p>
<p>All these scenarios present different small challenges to solve in order to
accomplish the desired high-level goal. Humans switch contexts in a really
flexible and natural way and constantly take care of the multitude of these
small problems that are faced to complete the desired objective. For example,
a general task can be divided into subtasks, which can then be further divided
into smaller ones <span class="citation"
data-cites="kroemer2021review">(Kroemer, Niekum, e Konidaris 2021)</span>. The
strategy of breaking down intricate objectives into smaller, manageable,
simpler activities is the most widely used heuristic to solve problems <span
class="citation" data-cites="egidi2006decomposition">(Egidi 2006)</span>.</p>
<p>Many of these sub-challenges require an interaction with one or more
objects. For example, the action of opening involves a door to pass through it
while moving or to access the fridge for cooking. The reaching action can
imply an object like a pen in the office to write or a glass of water to
drink. To push as an action often implies a button to enable a device in the
workplace, or to turn on a car to commute, or the stove to heat a meal.
Objects have undoubtedly strong importance in the small actions performed to
achieve a goal, and their affordance is still the object of research in humans
<span class="citation" data-cites="maranesi2014cortical">(Maranesi, Bonini, e
Fogassi 2014)</span>, <span class="citation"
data-cites="osiurak2017affordance">(Osiurak, Rossetti, e Badets 2017)</span>
and machines <span class="citation"
data-cites="horton2012affordances">(Horton, Chakraborty, e Amant
2012)</span>.</p>
<p>As seen, many different movements and sub-actions, often involving objects,
are executed in daily life. Furthermore, they are also adapted to accomplish
the current desired goals. The adaptation can involve a simple difference of
position with respect to the previous location, both of the object or the
executor, or can involve a completely different context to which the action
learned is transferred. These skills are learned and discovered at the
beginning, and then the knowledge of the action is abstracted and adapted to
different purposes.</p>
<p>Moreover, a person builds sequences of actions naturally to achieve the
objective and, as discussed, adapts them to the environment. The skills are
often combined together one after the other, based on the scenario but also
based on the result and position of the previous execution. Occasionally, it
can happen that part of an action is used and part of another action, mixing
previously learned movements if the situation requires it. This results in the
creation of new combinations and compositions of previously known
activities.</p>
<p>Lastly, dissecting complex challenges requires also decision-making under
uncertainty, which is essential for achieving high-level goals since the
sequence of activities is not always clear in advance. Often, the goal changes
mid-way in response to the environment, or the initial assessment is
sub-optimal or incorrect, forcing a change in planning and a new decision on
what subsequent action to take. So it&#x2019;s worth noting that online decision
under dynamic circumstances and change of skill executed allows a person to
navigate the complexities of daily scenarios with success.</p>
<p>The human mind&#x2019;s capacity for abstraction, planning, and execution is still
a remote objective for robotics <span class="citation"
data-cites="konidaris2019necessity">(Konidaris 2019)</span>. This level of
adaptation to the environment and building of compounded behaviors is still a
hard challenge to solve nowadays.</p>
<p>For this reason, robots currently are not pervasive in society like other
technologies. Humanoid robots have little if no presence and, despite the
potential different uses, are relegated to mainly interaction and exhibition
duties. The majority of robots work in a controlled environment, like
factories, where the surroundings are specifically designed for them. The
actions taken are repetitive, fixed, and in contact with a simple, defined set
of objects.</p>
<p>Furthermore, even if some robots are able to integrate into semi-structured
environments (for example, the robotic vacuum cleaners for homes or lawnmowers
for gardens), they are specialized to a single task in a single scenario.
Multi-purpose robots require a more human-compatible design and a higher
degree of intelligent behavior <span class="citation"
data-cites="dechant2021toward">(DeChant e Bauer 2021)</span>, but versatile
humanoid robots are still not pervasive in the current status of society.</p>
<p>In this study, we propose a computational model that is biologically
inspired. Our approach consists in the use of mathematics and artificial
intelligence to emulate human abstraction and adaptation capabilities in the
execution of a series of primitive actions. We want to prove how demonstrating
basic movements to a robot and composing them together with flexibility may
lead to achieving complex tasks of various natures. Specifically, movement
primitives are reused and combined differently for different goals, avoiding
explicit teaching of multiple objectives. The trajectories for the skills
learned are adapted to the environment and partially composed thanks to the
interpolation abilities of Conditional Neural Movement Primitives (CNMP)
networks <span class="citation" data-cites="Ugur-RSS-19">(M. Y. S. A. M. I. A.
J. P. A. E. Ugur 2019)</span>. Lastly, the approach has been implemented and
tested on an anthropomorphic robot and on an industrial collaborative
robot.</p>
</section>
<section id="foochallenges" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span>
Challenges</h2>
<p>Robotics dominates many fields, but as discussed, often the environment is
controlled, designed to help the robot in its task, and not human-friendly. If
the purpose is to integrate robots into the human environment, robots must
adapt to humans, not vice-versa. All environments in which humans are present
are not organized or predictable, and this means one issue is that robots have
to accommodate for these conditions. A challenge is definitely to introduce
the machine to an unstructured environment, and this implies many
sub-issues.</p>
<p>Having surroundings that might change forces the machines to have a great
amount of perception. The system has to be extremely aware of the objects and
people around it to operate in a safe and meaningful way. This translates into
equipping many sensors and using real-time data from all available sources.
Moreover, the machine cannot rely on these detection instruments mounted on
the external world since a humanoid robot is expected to be mobile. Having a
multi-purpose system that can act in different scenarios implies, indeed, a
self-contained arrangement of sensors.</p>
<p>The perception brings, in cascade, the necessity of storing this
information and creating an internal copy of the surroundings that works as a
base for planning and future predictions. Creating a digital twin for the
environment is not essential for all the actions since some of them can be
executed in real-time, but it is required to plan their effects and combine
results together. For example, if a sponge is needed to clean a table, it
would be faster to have the knowledge of its last position, but it can also be
researched on demand and used while observing the effects in real-time till
the table is clean. On the other hand, complex actions that combine multiple
primitives need a future prediction of their effects on the environment, so
its internal representation is required.</p>
<p>With changing surroundings, it is possible also that the expected position
of objects is no longer consistent with the representation. This forces the
system to find an alternative or explore the environment till the object is
found. Other kinds of exploration possible are the exploration of the action
space to infer new actions and results, or the exploration of objects&#x2019;
capabilities to learn new affordances and usages. <span class="citation"
data-cites="Ahmetoglu_2022">(Ahmetoglu et al. 2022)</span></p>
<p>Another factor worth taking into consideration is the planning subject.
Plans have to be structured in a meaningful way otherwise, an incorrect
sequence won&#x2019;t just produce an incorrect result but might bring the system
further away from the final goal. The combinations of actions generated
usually have importance in the order of execution, so the product of the
skills has to be considered carefully.</p>
<p>Furthermore, objects and tools are usually designed for humans, so their
capabilities might vary depending on the machine used and might influence the
actions in the planning phase. Giving meanings to the objects, both in terms
of affordances and representations, is still a tricky challenge in robotics
<span class="citation" data-cites="7523298">(Jamone et al. 2018)</span> and
partially involves the previously investigated challenges of planning and
exploration.</p>
<p>Also, obstacle avoidance, whenever there is an object in the trajectory of
movement, is a factor to take into consideration. The robot is required to be
aware of the surroundings and itself, not to collide, hurt, damage, or just
fail the designated goal. Humans adapt previously known actions whenever an
obstacle or an impediment is present.</p>
<p>Part of the adaptation challenge is also being able to transfer the skills
known to new locations and scenarios. For example, learning how to turn a key
for the door and use the action for the key of the car or the knob to turn on
the stove. This is an essential capability that is difficult to implement in a
machine.</p>
<p>Another more hidden challenge is how the actions are merged among them.
Usually, humans, when they pass from one action to another, apply a smooth
transition. This means that the movements don&#x2019;t have to fully start and end as
they are learned, or the result will be artificial and sub-optimal.</p>
<p>Furthermore, object handling, grasping, and manipulation present some
issues that are the object of research. How to pick the item desired, where,
with which grasp, and with which force intensity are issues that can undermine
the final result.</p>
<p>Lastly, another challenge that will be encountered is the recognition when
the action is completed. Being aware of the right final state is essential for
successfully matching the expectations for the goal requested.</p>
<p>These challenges discussed are crucial aspects to consider, but not all of
them will be addressed in this project, and some will also be simplified.
Nevertheless, it&#x2019;s worth noting the scope and limitations of this work and the
boundaries within which the research operates.</p>
</section>
<section id="fooobjectives" class="level2" data-number="1.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span>
Objectives</h2>
<p>The aim of this research is to investigate novel skill generation by
combining previously taught ones with the use of CNMPs <span class="citation"
data-cites="Ugur-RSS-19">(M. Y. S. A. M. I. A. J. P. A. E. Ugur 2019)</span>.
The research aims to be applied to robotics scenarios involving trajectories
for object manipulation and high-level goal achievement. The generation of new
combinations of skills will be performed by connecting skill segments that the
robot learned by demonstration. The amount of demonstrations given should be
reasonable for the system to be applied in real life by a human. The
combination of actions will be investigated in both the concatenation of
trajectories end-to-end and the use of parts of them. The ultimate goal is to
create a system that allows a robot, given some demonstrations, to reuse the
skills acquired to complete different objectives whose trajectories were never
taught explicitly. Furthermore, the adaptation should be acceptable in
different configurations of the environment and, ideally in different
scenarios.</p>
</section>
<section id="foothesis-structure" class="level2" data-number="1.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Thesis
Structure</h2>
<p>Accordingly, the remainder of this thesis is structured as follows.</p>
<p>discusses the background of the topic, the current advancements in the
field, and the related research with a literature review.</p>
<p>In <a href="#foochap:platforms" data-reference-type="ref"
data-reference="chap:platforms">3</a> the instruments and frameworks used in
this research are listed and analyzed to be able to understand the initial
setup and replicate the results.</p>
<p>The <a href="#foochap:design" data-reference-type="ref"
data-reference="chap:design">4</a> explains the design and architecture of the
proposed method. In order to understand the logic, the conceptual passages and
mathematical background.</p>
<p>The <a href="#foochap:implementation" data-reference-type="ref"
data-reference="chap:implementation">5</a> analyzes the key points of the
implemented solution through the explanation of the most important passages in
the code developed.</p>
<p>The <a href="#foochap:validation" data-reference-type="ref"
data-reference="chap:validation">6</a> shows the final results and the testing
on real-life robotic platforms.</p>
<p>Finally, concludes this thesis by summarising its main contribution and
future work.</p>
</section>
</section>
<section id="foochap:background" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> State of the
Art</h1>
<p>In this chapter, we will proceed with the literature analysis and
state-of-the-art methods related to the topic. Furthermore, some basic notions
regarding the previous research relied upon will be explained.</p>
<p>Trajectory generation for robotics is a topic closely related to
manipulation and navigation. Every movement can be described as a trajectory,
defined as the composition of the path taken by an agent (or its joints) in
time <span class="citation" data-cites="biagiotti2008trajectory">(Biagiotti e
Melchiorri 2008)</span>. In this discussion, we will focus more on the
manipulation part, being the most clear example of how combining different
trajectories can produce novel robotic skills. Manipulation is one of the most
distinctive capabilities of robots, since their main objective is to perform
physical tasks in the real world. Any process that presents the necessity of a
human to engage with a physical object meaningfully can solely be automated by
robot manipulation. <span class="citation" data-cites="rosen2022role">(Rosen
et al. 2022)</span></p>
<p>Creating robots capable of directly interacting with the world around them
is still a key challenge in robotics, and manipulation is central to this.
<span class="citation" data-cites="kroemer2021review">(Kroemer, Niekum, e
Konidaris 2021)</span> Nevertheless, the ability to solve high-level goals in
robots is increasing <span class="citation" data-cites="gupta2019relay">(Gupta
et al. 2019)</span>, <span class="citation"
data-cites="simeonov2021long">(Simeonov et al. 2021)</span> thanks to the
recent advances in artificial intelligence. Some approaches may follow natural
language instructions to achieve complex sequences of actions <span
class="citation" data-cites="hu2019hierarchical">(Hu et al. 2019)</span>, but
according to the research objective, a certain degree of autonomy is desired.
This implies typically giving only the final goal and not the step-by-step
instructions.</p>
<p>A commonly used technique in robotics is Learning from Demonstration (LfD)
<span class="citation" data-cites="ARGALL2009469">(Argall et al. 2009)</span>,
and then <span class="citation"
data-cites="ravichandar2020recent">(Ravichandar et al. 2020)</span>. It allows
for solving a wide variety of robotics problems by imitating an external
agent. The demonstrator, often a human or another system, provides examples
(expert demonstrations) of how to perform a task, and the learning agent
generalizes from these demonstrations to acquire the ability to perform the
task later independently.</p>
<p>Famous learning from demonstration research includes statistical modeling
<span class="citation" data-cites="calinon2016tutorial">(Calinon 2016)</span>,
dynamic systems <span class="citation" data-cites="schaal2006dynamic">(Schaal
2006)</span>, and their union in <span class="citation"
data-cites="ugur2020compliant">(E. Ugur e Girgin 2020)</span>. In Dynamic
Movement Primitives (DMPs) <span class="citation"
data-cites="schaal2006dynamic">(Schaal 2006)</span>, a trajectory is
represented with a set of differential equations and learned with as little as
one shot. Thanks to the "point attractor" mechanism, it guarantees reaching a
point even under perturbations. DMPs have successfully been utilized in
difficult manipulation tasks such as in-hand manipulation and flipping boxes
using chopsticks <span class="citation"
data-cites="pastor2009learning">(Pastor et al. 2009)</span>. On the other
hand, DMPs require additional tuning to determine the number of basis
functions. Moreover, their approach is not designed to learn from multiple
trajectories and, therefore, cannot encode the important parts of multiple
demonstrations <span class="citation" data-cites="Ugur-RSS-19">(M. Y. S. A. M.
I. A. J. P. A. E. Ugur 2019)</span>.</p>
<p>In the Probabilistic Movement Primitives (ProMP) <span class="citation"
data-cites="paraschos2013probabilistic">(Paraschos et al. 2013)</span>,
instead, the distribution of basis functions is often represented using a
probabilistic framework, typically a Gaussian Mixture Model (GMM) or a similar
probabilistic model. Historically, Gaussian Mixture Models <span
class="citation" data-cites="nguyen2009model">(Nguyen-Tuong, Seeger, e Peters
2009)</span> have been prominent among various probabilistic approaches since
they provide adaptable solutions to the challenge of modeling trajectories. On
the other hand, GMMs involve estimating many parameters, especially when
dealing with high-dimensional data or a large number of components. This can
make training and inference computationally expensive, particularly when the
dataset is extensive, and if not done correctly, can lead to some failures,
Figure <a href="#foofig:prompcnmp" data-reference-type="ref"
data-reference="fig:prompcnmp">2.1</a>.</p>
<figure id="foofig:prompcnmp">
<img src="Images/ProMPvsCNMP.png" />
<figcaption>In ProMPs the distribution of basis functions is often represented
using a probabilistic framework, but can lead to some failures.</figcaption>
</figure>
<p>Another probabilistic model, like GMMs, often used in statistical modeling
techniques is Hidden Markov Models (HMMs) <span class="citation"
data-cites="lee2011incremental">(Lee e Ott 2011)</span>. HMMs were
successfully applied to learn multi-modal models from temperature, pressure,
and fingertip information for exploratory object classification tasks <span
class="citation" data-cites="chu2013using">(Chu et al. 2013)</span>.</p>
<p>A recent model developed in robotics called Conditional Neural Movement
Primitives (CNMP) <span class="citation" data-cites="Ugur-RSS-19">(M. Y. S. A.
M. I. A. J. P. A. E. Ugur 2019)</span> also learns from demonstrations, but
instead of using GMMs, they use neural networks to model the mapping from
conditions to trajectories directly. Neural networks allow the model to scale
better and to offer robust data approximation via gradient descent. Until
recently, neural networks in deep learning were trained to approximate a
single-output function. However, when data is a distribution, the single
function cannot approximate the underlying model. So the network can be
modeled as a probabilistic approximator, that can predict the distribution
parameters, mean, and variance. This makes CNMPs well-suited for tasks with
complex, high-dimensional state spaces. It allows one to learn skills in tens,
rather than thousands, of real-world interactions and interpolate among
them.</p>
<p>Based on the above-mentioned observations, the proposal is as follows. In
the following research, the ability of CNMPs to interpolate the trajectories
demonstrated is exploited to synthesize new complex skills. The model is based
on Gaussian Processes (GP) <span class="citation"
data-cites="seeger2004gaussian">(Seeger 2004)</span>, Neural Processes (NPs)
<span class="citation" data-cites="garnelo2018neural">(Garnelo, Schwarz, et
al. 2018)</span>, and Conditional Neural Processes (CNPs) <span
class="citation" data-cites="DBLP:journals/corr/abs-1807-01613">(Garnelo,
Rosenbaum, et al. 2018)</span>. For context, an explanation in detail of these
above-mentioned methods will follow.</p>
<section id="foogaussian-processes" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Gaussian
Processes</h2>
<p>Gaussian Processes <span class="citation"
data-cites="seeger2004gaussian">(Seeger 2004)</span> are probabilistic models
that define a distribution over functions, Figure <a href="#foofig:gp"
data-reference-type="ref" data-reference="fig:gp">2.2</a>. This means that
they leverage pre-existing knowledge about a set of functions and infer during
test-time specific functions that fit the data provided. Given a set of
observed points, there are infinite possible functions that pass through them.
Gaussian processes provide an elegant solution to this challenge by assigning
a probability to each of these potential functions. The mean of this
probability distribution then represents the most probable characterization of
the data given the observation points <span class="citation"
data-cites="Goertler2018VisualExplorationGaussian">(J. G&#xF6;rtler
2018)</span>.</p>
<p>This is called regression and is used, for example, in robotics or time
series forecasting. Gaussian processes are not limited to regression, and they
can also be extended to classification and clustering tasks <span
class="citation" data-cites="kapoor2010gaussian">(Kapoor et al. 2010)</span>
<span class="citation" data-cites="kim2007clustering">(Kim e Lee
2007)</span>.</p>
<figure id="foofig:gp">
<img src="Images/GP.png" />
<figcaption>Gaussian Processes are probabilistic models that define a
distribution over functions.</figcaption>
</figure>
<p>Many supervised learning problems can be seen as function approximations
since a dataset of observations <span class="math inline">\(\{x_i,
y_i\}^{n-1}_{i=0}\)</span> is basically a number <span
class="math inline">\(n\)</span> of evaluations <span
class="math inline">\(y_i = f(x_i)\)</span> of an unknown function <span
class="math inline">\(f\)</span>. A supervised learning algorithm returns an
approximated function <span class="math inline">\(g\)</span>. The goal is to
minimize the loss between the real function <span
class="math inline">\(f\)</span> and the predicted one <span
class="math inline">\(g\)</span>. The evaluation is carried out on unlabelled
data points <span class="math inline">\(x_j\)</span>.</p>
<p>On the other hand, the disadvantages of Gaussian Processes are prior
selection and training time for large datasets. Scaling issues with GPs have
been addressed in <span class="citation"
data-cites="snelson2006sparse">(Snelson e Ghahramani 2006)</span>. The limited
expressivity from functional restriction was addressed with DeepGPs in <span
class="citation" data-cites="damianou2013deep">(Damianou e Lawrence
2013)</span> <span class="citation"
data-cites="salimbeni2017doubly">(Salimbeni e Deisenroth 2017)</span>.
Overcoming these issues and attempting to combine Deep Learning (DL) with GPs
was proposed in <span class="citation" data-cites="wilson2016deep">(Wilson et
al. 2016)</span>, but the approach remains close to GPs since the network is
used to learn more expressive kernels to use with GPs.</p>
</section>
<section id="fooconditional-neural-processes" class="level2"
data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span>
Conditional Neural Processes</h2>
<p>In <span class="citation"
data-cites="DBLP:journals/corr/abs-1807-01613">(Garnelo, Rosenbaum, et al.
2018)</span>, the authors propose a novel research in which the inference
potential of Gaussian Processes and the performance of neural networks are
blended together. Neural networks are extensively employed as approximators of
functions and have demonstrated considerable efficacy but often require large
datasets for training. In CNPs, the prior knowledge is directly derived from
the data, allowing them to infer the underlying function distribution based on
observations. CNPs are built with three main blocks: Encoder, Aggregator, and
Decoder. Encoder <span class="math inline">\(E\)</span> and Decoder <span
class="math inline">\(Q\)</span> are typically Multi-layer perceptrons. The
model structure is shown in Figure <a href="#foofig:cnp"
data-reference-type="ref" data-reference="fig:cnp">2.3</a>.</p>
<figure id="foofig:cnp">
<img src="Images/CNP.png" />
<figcaption>The structure of a CNP network, with three main blocks: Encoder,
Aggregator, and Decoder.</figcaption>
</figure>
<p>The model scales with complexity <span
class="math inline">\(O(n+m)\)</span> for making <span
class="math inline">\(m\)</span> predictions from <span
class="math inline">\(n\)</span> observations, while GPs scale with <span
class="math inline">\(O(n+m)^3\)</span>. CNPs don&#x2019;t require the specification
of a kernel cause they learn it from the data provided in training. The
tradeoff is that the representations of the observations have fixed
dimensionality.</p>
<p>The work is based on the previous research of Neural Processes (NPs) <span
class="citation" data-cites="garnelo2018neural">(Garnelo, Schwarz, et al.
2018)</span>. NPs are suggested as a means to manage the substantial
computational demands of GPs while leveraging their flexibility and efficiency
with data. NPs help create different predictions by learning a shared hidden
representation. However, they have trouble with long sequences because they
automatically pick certain points. Building on NPs, Conditional Neural
Processes (CNPs) are strong models that make training more efficient by
allowing explicit conditioning.</p>
<figure id="foofig:cnp_data">
<img src="Images/CNPdata.png" />
<figcaption>CNP allows precise predictions for targets sampled from a
distribution conditioned with observations</figcaption>
</figure>
<p>The approach allows precise predictions for targets sampled from a
distribution conditioned with observations, Figure <a href="#foofig:cnp_data"
data-reference-type="ref" data-reference="fig:cnp_data">2.4</a>. Given a
varying number of observations <span class="math inline">\(O\)</span>, a
neural network <span class="math inline">\(E\)</span> is utilized as an
encoder to generate a fixed-size representation <span
class="math inline">\(r_i\)</span>.</p>
<p>Observations fed to the network don&#x2019;t have an order, following the
stochastic processes, because subsequently, they are aggregated with the
average operation to obtain a single representation <span
class="math inline">\(r\)</span>. Any commutative operation is valid and
usable. The resulting representation <span class="math inline">\(r\)</span>
contains the conditioning information and is fed to a decoder network <span
class="math inline">\(Q\)</span> along with the desired target <span
class="math inline">\(x_j\)</span> to query. The decoder network <span
class="math inline">\(Q\)</span> has parameters <span
class="math inline">\(\theta\)</span>. For all the targets <span
class="math inline">\(x_j \in T\)</span> the decoder outputs the mean and
standard deviation.</p>
<p>The formulation of the encoding of each observation is: <span
class="math display">\[r_i = E_\phi(x_i,y_i), \quad \forall(x_i,y_i) \in
O\]</span> and the following commutative operation between the encodings to
create a single one: <span class="math display">\[r = r_1 \oplus r_2 \oplus
... \oplus r_i,\]</span> The commutative operation expressed by <span
class="math inline">\(\oplus\)</span>, can be summation, average, product, and
so on.</p>
<p>The vector generated is concatenated with the target variables. The merged
representation is passed to the decoder to obtain the output as: <span
class="math display">\[\phi_j = Q_\theta(x_j, r), \quad \forall x_j \in
T\]</span> Where the output is: <span class="math display">\[\phi_j = (\mu_j,
\sigma_j^2)\]</span> which are the mean and the standard deviation of the
output variable.</p>
<p>In summary, the CNP model, with averaging operation, can be formulated as:
<span class="math display">\[\mu_j, \sigma_j^2 = Q_\theta \left( x_j \oplus
\frac{ \sum_{i}^{n} E\phi((x_i,y_i)) }{n}  \right)\]</span></p>
</section>
<section id="fooconditional-neural-movement-primitives" class="level2"
data-number="2.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span>
Conditional Neural Movement Primitives</h2>
<p>Finally, in <span class="citation" data-cites="Ugur-RSS-19">(M. Y. S. A. M.
I. A. J. P. A. E. Ugur 2019)</span>, Conditional Neural Movement Primitives
are proposed as a model. CNMPs, as the name suggests, are an extension of CNPs
and are particularly well suited for the robotics domain. The model
illustration can be seen in Figure <a href="#foofig:cnmp"
data-reference-type="ref" data-reference="fig:cnmp">2.5</a>.</p>
<figure id="foofig:cnmp">
<img src="Images/CNMP.png" />
<figcaption>CNMP model, it is conceived to work with temporal relations
between sensorimotor data and different task parameters</figcaption>
</figure>
<p>The "learning from demonstration" framework can learn non-linear
relationships between trajectories and reproduce them in joint or task space.
In this case, a trajectory is formally defined as a temporal function, <span
class="math inline">\(\tau = \tau (t)\)</span>, where the sensorimotor data in
time describes how a robot moves. So, each trajectory <span
class="math inline">\(\tau\)</span> is a list of ordered sensorimotor values:
<span class="math display">\[\tau = \{ SM(t_1), SM(t_2), ... , SM(t_T )
\}\]</span> where <span class="math inline">\(SM(t_i)\)</span> is the
sensorimotor data at an instant of time <span
class="math inline">\(t_i\)</span>. So, the challenge of trajectory generation
becomes figuring out a series of commands <span
class="math inline">\(SM(t_i)\)</span> that creates the movement desired <span
class="citation" data-cites="gasparetto2007new">(Gasparetto e Zanotto
2007)</span>. Finally, with a set of observations <span
class="math inline">\(O\)</span>, the model has to learn the function <span
class="math inline">\(\tau = f(t|O)\)</span>, using <span
class="math inline">\(N\)</span> expert demonstrations, <span
class="math inline">\(D = \{\tau_1, \tau_2, ... , \tau_N \}\)</span>.</p>
<p>CNMPs are conceived to work with temporal relations <span
class="math inline">\(t\)</span> and different task parameters <span
class="math inline">\(\gamma\)</span>. CNMPs maintain the permutation
invariance of CNPs over observations <span class="math inline">\(O\)</span>
and queries <span class="math inline">\(T\)</span>. Furthermore, to make the
model time-invariant, the sensorimotor trajectories are often scaled in the
interval [0,1]. The task parameter <span class="math inline">\(\gamma\)</span>
effectively adds one or more dimensionalities to the network&#x2019;s input, and it&#x2019;s
passed to both the encoder and the decoder. An observation becomes the
concatenation of <span class="math inline">\(SM(t_i)\)</span>, <span
class="math inline">\(t_i\)</span>, and <span
class="math inline">\(\gamma\)</span>. The dimensionality of <span
class="math inline">\(SM(t_i)\)</span> depends on factors like the Degrees of
Freedom of the robot joints and the number of variables corresponding to the
actuators. For the aggregation of the representations, the averaging operation
has been chosen. During training, a random trajectory <span
class="math inline">\(\tau_i\)</span> is selected from the <span
class="math inline">\(D\)</span> set of expert demonstrations. Next, a random
number <span class="math inline">\(n\)</span> of random observation points are
selected from the trajectory <span class="math inline">\(\tau_i\)</span>. The
encoder takes the <span class="math inline">\(n\)</span> observations and
produces <span class="math inline">\(n\)</span> representations. The final
representation is obtained by averaging the representations produced by the
encoder fed with all the observation points. The target data is predicted
using the representation and the query time <span
class="math inline">\(t\)</span> concatenated to the task parameter <span
class="math inline">\(\gamma\)</span></p>
<p>The encoder and the decoder are trained jointly with the error calculated
from the following loss function: <span class="math display">\[L(\theta,
\delta)= -logP(y_i|\mu_i, softmax(\sigma_j))\]</span> using both mean and
standard deviation produced by the network. As a note, the uncertainty of the
prediction provided by the variance is useful for the model&#x2019;s active
exploration to choose wisely where the next observations are needed. Moreover,
the capacity of CNMPs to deal with high-dimensionality input can also be used
to input images in the model. Image completion indeed can be seen as a
regression task. Leveraging the interpolation capabilities of CNMPs, our
approach will investigate novel synthesis by combining and concatenating
previously taught ones.</p>
</section>
</section>
<section id="foochap:platforms" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span>
Platforms</h1>
<p>In this chapter, all the physical and digital platforms utilized will be
explained to give a proper understanding of the initial architecture and a
more comprehensive idea of the environment of the experiments. The first part
will state the devices and their setup, capabilities, and configuration used.
Subsequently, the frameworks and libraries employed will be listed and
described.</p>
<section id="foobaxter-robot" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Baxter
Robot</h2>
<p>The Baxter robot is an industrial robot built by Rethink Robotics in 2011
<span class="citation" data-cites="wiki:baxter">(Wikipedia contributors
2023)</span>. The platform [Fig. <a href="#foofig:baxter"
data-reference-type="ref" data-reference="fig:baxter">3.1</a>] has two robotic
arms with interchangeable grippers at the wrist (End Effectors). The robot is
180cm tall and, with its pedestal, weighs 140 kg. The arms have 7 degrees of
freedom (DOF), which implies they have seven joints each. This makes it
kinematic redundant, meaning that for some points reached in space, multiple
pose configurations of the arms are possible. These two factors combined allow
the robot to have an impressive capability in manipulation. The robot can be
equipped with suction caps or two-jaw parallel grippers; we chose the last
ones in our configuration. The gripper, in addition to its position, also
offers information regarding the force applied while grasping an object.</p>
<p>The robot was designed with attention to collaborative tasks with humans.
For this reason, it eases teaching by demonstration by having integrated two
touch sensors in the wrists that unlock the motors of the arms, allowing the
user to move them easily and record the trajectories executed. The robot helps
the movement with a feature called "Zero-g mode", in which the weight of the
joints is neutralized actively by the motors. This enables the teaching expert
to demonstrate movements in a similar environment without gravity, without
having to carry the instrumentation load in time constantly. Furthermore, the
robot has many input buttons and LEDs, they are present on the hands, arms,
and chest, and they allow the programming of custom behaviors. They are
especially useful in retrieving and giving instructions to the robot without
reaching a computer, like closing the gripper at the desired moment, getting
feedback, or starting the trajectory recordings.</p>
<p>Another feature worth mentioning is the increased safety of operating
around humans. Thanks to active and passive safety systems equipped in the
platform, it doesn&#x2019;t require a cage for protection. On the other hand, making
the robot less hazardous comes with the cost of precision. A motor driving a
spring that drives Baxter&#x2019;s arm instead of just a direct motor impacts the
precision of movements, sometimes in terms of centimeters. This doesn&#x2019;t make
the robot perfectly suitable for industrial applications, but especially
appropriate for research and for the adaptability in our project.</p>
<p>The head of the robot includes a ring of sonar sensors for people
detection, a wide-angle camera, and a movable display that acts like a face.
Another benefit of the robot lies at the end of both hands. Immediately next
to the attachment for the tools, an infrared (IR) sensor provides data on the
distance from a solid object (i.e., a table) and an inertial measurement unit
(IMU). Moreover, an embedded RGB camera is also present, allowing to see
closely the object approached or to change the point of view on it without
additional external cameras.</p>
<figure id="foofig:baxter">
<img src="Images/baxter.png" />
<figcaption>Baxter Robot platform</figcaption>
</figure>
</section>
<section id="foour10-robot" class="level2" data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> UR10
Robot</h2>
<p>UR10 is a single industrial robot arm that shines in reliability and
precision [Fig. <a href="#foofig:ur10" data-reference-type="ref"
data-reference="fig:ur10">3.2</a>]. It has been manufactured by Universal
Robots and combines long reach with a high payload <span class="citation"
data-cites="url:ur10">(Universal Robots 2023)</span>. It is intended for
medium-duty tasks, so it&#x2019;s compact in its overall dimensions compared to a
fully intended industrial robot. It can reach an impressive height of 2.3m on
its pedestal. In our experiments, it was mounted on a pedestal 0.85m tall to
increase the reachability at table level.</p>
<p>The arm has a reaching radius of 1,3m from the mounting point, which
implies a workspace of approximately 5,3 square meters at the base level. The
robot has 6 Degrees of Freedom, with six rotating joints. It is able to reach
any point in its reaching radius but has no kinematics redundancy, meaning
only one position is possible for any given point. The total payload that can
be carried is 10 kg.</p>
<figure id="foofig:ur10">
<img src="Images/ur10.png" />
<figcaption>UR10 Robot platform</figcaption>
</figure>
<p>Like the previous robot, this one is designed to work collaboratively with
humans. It features built-in safety features, such as force/torque sensors, to
detect and respond to external forces or unexpected events. A button to
release the motor breaks is present on the floating touch screen and allows
the robot&#x2019;s motion by hand. This robot also doesn&#x2019;t require a safety cage
around for protection, but the emergency stop button always has to be within
easy reach.</p>
<p>The robot design emphasizes modularity, making it easier for users to
customize and adapt it for different tasks or use various end effectors. We
coupled it with a three-finger gripper described in the next paragraph.</p>
</section>
<section id="foof-robotiq-gripper" class="level2" data-number="3.3">
<h2 data-number="3.3"><span class="header-section-number">3.3</span> 3F
Robotiq Gripper</h2>
<p>At the end of the UR10 Robotic Arm, a Robotiq 3-Finger Adaptive Robot
Gripper was mounted [Fig. <a href="#foofig:gripper3f"
data-reference-type="ref" data-reference="fig:gripper3f">3.3</a>]. The gripper
has a human-inspired design and has three fingers with three joints each <span
class="citation" data-cites="url:3fgripper">(Robotiq 2023a)</span>. The
physical platform was chosen for its precision and safety, and it pairs well
with the UR10 capabilities. The gripper offers different grip modes; the ones
available are: "basic", "wide", "scissor" and "pinch". Each is appropriate for
distinctive objects to grip; the basic one is the most versatile, but the wide
one has more stability for big or long objects, and the pinch one is the best
for small objects. The "scissor mode" closes together the two fingers on the
same side, for high-precision manipulation. We mainly used the "pinch"
setting.</p>
<figure id="foofig:gripper3f">
<img src="Images/gripper3f.png" />
<figcaption>3F Robotiq Gripper platform</figcaption>
</figure>
<p>The gripper has a mass of 2.3 kg in contrast with a grip payload of 10 kg.
The grip force applied can range from 30 to 70 N, depending on the grip mode
selected. The precision declared is up to 0.05 mm.</p>
<p>This platform was designed as well for collaborative robotic applications,
allowing it to work safely alongside human operators. It incorporates safety
features to detect and respond to external forces, stopping in case of high
forces applied. The torque and speed of gripping data are available and
exposed through a dedicated ROS topic. Speed and torque are also adjustable
for the intended use. It is possible to control and retrieve data for each
finger individually.</p>
</section>
<section id="fooft-300-s-force-torque-sensor" class="level2"
data-number="3.4">
<h2 data-number="3.4"><span class="header-section-number">3.4</span> FT 300-S
Force Torque Sensor</h2>
<p>Between the UR10 robot and the 3F Finger Gripper, an FT 300-S Force Torque
Sensor [Fig. <a href="#foofig:ft-sensor" data-reference-type="ref"
data-reference="fig:ft-sensor">3.4</a>] was mounted to increase precision and
repeatability. The sensor offers high-resolution real-time measurements
regarding the force and torque applied to the three space dimensions and
improves the capabilities of the robot, making it able to detect the payload
carried or the amount of pressure between the object carried and the static
environment (i.e., the table).</p>
<p>The device was built for compatibility with the Universal Robot series and
has an IP65 rating. It also enables precise object placement such as
alignment, indexing, and insertion <span class="citation"
data-cites="url:ftsensor">(Robotiq 2023b)</span>. The FT 300-S is commonly
used in tasks where force and torque sensing are critical, but we used it to
increase the reliability of the payload measurements.</p>
<figure id="foofig:ft-sensor">
<img src="Images/ft_sensor.png" />
<figcaption>FT 300-S Force Torque Sensor</figcaption>
</figure>
</section>
<section id="fooframeworks" class="level2" data-number="3.5">
<h2 data-number="3.5"><span class="header-section-number">3.5</span>
Frameworks</h2>
<section id="fooros" class="level5" data-number="3.5.0.0.1">
<h5 data-number="3.5.0.0.1"><span
class="header-section-number">3.5.0.0.1</span> ROS</h5>
<p>The Robot Operating System (ROS) <span class="citation"
data-cites="url:ros">(ROS 2023)</span> is a framework to standardize the
deployment of robot applications. The system is a set of software libraries
and tools that combine the state-of-the-art drivers for the most common robot
interfaces and contain the most used algorithms for robotics.</p>
<p>Since robotics programming is a complex challenge, the idea behind ROS is
to use the "divide et impera" approach and split it into multiple
sub-problems. This division requires a distributed strategy, and distribution
implies communication among parts. One of ROS&#x2019;s main objectives is to
standardize communication. For this reason, ROS acts like a middleware
framework, allowing the ease of the dialog between software and the robotic
hardware. It is widely used in research and industry, from mobile robots to
manipulators. In our research, we used ROS version 1.</p>
<p>It&#x2019;s platform-independent and open source, so it is possible to create a
custom robotics device compatible with it, and it&#x2019;s possible to develop
personalized libraries. Its modular architecture allows the creation or use of
a series of executable pieces of code called nodes, which run on a single or
even on many computers. The nodes communicate among themselves with messages
in the form of data structures previously defined. The distributed system
allows to decuple the computation of heavy tasks, like vision, 3D
reconstruction, and navigation, from the robot&#x2019;s hardware.</p>
<p>Communication occurs through "topics" and "services". Nodes running on any
computer ping the "master node" designated to retrieve all the possible topics
and services exposed from other nodes in the network. A node can be a
"publisher" or "subscriber" to a topic, sending messages to it or receiving
messages from it. Communication with topics is not blocking and it is
many-to-many: multiple nodes can publish, and simultaneously, multiple nodes
can subscribe to a topic. Services work in a similar way, but the
communication blocks the computation till the data requested is retrieved.
Their mechanism is analogous to Remote Procedure Calls (RPCs).</p>
<p>ROS is available with two commonly used programming languages: Python and
C++. We used the Python version with the "rospy" package in the experiments
with both robots.</p>
</section>
<section id="foopytorch" class="level5" data-number="3.5.0.0.2">
<h5 data-number="3.5.0.0.2"><span
class="header-section-number">3.5.0.0.2</span> Pytorch</h5>
<p>Pytorch is a Deep Learning framework <span class="citation"
data-cites="paszke2019pytorch">(Paszke et al. 2019)</span> that focuses on
speed and usability with an imperative and Pythonic programming style. The
Python library <span class="citation" data-cites="url:pytorch">(Pytorch
2023)</span> offers a wide variety of models and building blocks for
constructing neural networks. By design, it eases the debugging for the user
with a rich ecosystem of dedicated tools. It works on the CPU and on hardware
accelerators like GPUs. For this reason, PyTorch provides a multi-dimensional
array called a tensor, which is similar to NumPy arrays. Conversions among
both of them will be present in the code implementation.</p>
</section>
<section id="foojupyter-notebook" class="level5" data-number="3.5.0.0.3">
<h5 data-number="3.5.0.0.3"><span
class="header-section-number">3.5.0.0.3</span> Jupyter Notebook</h5>
<p>Jupyter Notebook is an open-source interactive web application <span
class="citation" data-cites="url:jupyter">(Jupyter Notebook 2023)</span>. It
supports multiple programming languages and offers a cell-based environment
where code and description/graphical results can be blended. Jupyter Notebook
integrates seamlessly with popular Python libraries, such as NumPy, Pandas,
Matplotlib, and scikit-learn; some of them will be described later. It was
used occasionally in our experiments to provide a fast and interactive coding
experience with Python. The notebooks can be easily shared, and the process is
clearly visualized. It was specifically useful in plotting multiple graphs
during the training stages of neural networks or debugging operations on
multi-dimensional arrays.</p>
</section>
<section id="fooanaconda-and-python-libraries" class="level5"
data-number="3.5.0.0.4">
<h5 data-number="3.5.0.0.4"><span
class="header-section-number">3.5.0.0.4</span> Anaconda and Python
Libraries</h5>
<p>Anaconda is an open-source software that contains open-source tools and
packages for data science, machine learning, and scientific computing <span
class="citation" data-cites="url:anaconda">(Anaconda 2023)</span>. It has been
used to track the packages utilized in the robotic platforms and in the
models&#x2019; development and training. The Conda package manager was the most used
tool to easily install, update, and manage various software packages and
dependencies. Some of the most important libraries are listed below.</p>
<section id="foomatplotlib" class="level6" data-number="3.5.0.0.4.1">
<h6 data-number="3.5.0.0.4.1"><span
class="header-section-number">3.5.0.0.4.1</span> MatplotLib</h6>
<p>Matplotlib is a comprehensive 2D plotting library for Python that generates
high-quality charts, plots, and visualizations. It has been widely used to
double-check the quality of the training or plot the trajectories recorded
with the robots.</p>
</section>
<section id="foonumpy" class="level6" data-number="3.5.0.0.4.2">
<h6 data-number="3.5.0.0.4.2"><span
class="header-section-number">3.5.0.0.4.2</span> Numpy</h6>
<p>NumPy is a package for numerical computing in Python. It supports large,
multi-dimensional arrays and matrices and a collection of mathematical
functions to operate on these arrays. It has been used for array slicing,
normalization, and smoothing data.</p>
</section>
</section>
</section>
</section>
<section id="foochap:design" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Design</h1>
<p>Intro to the combination of trajectories</p>
<section id="fooend-to-end-concatenation" class="level2" data-number="4.1">
<h2 data-number="4.1"><span class="header-section-number">4.1</span>
End-To-End Concatenation</h2>
</section>
<section id="foopartial-combination" class="level2" data-number="4.2">
<h2 data-number="4.2"><span class="header-section-number">4.2</span> Partial
Combination</h2>
<figure id="foofig:classes">
<embed src="figures/classes.pdf" />
<figcaption>A class diagram created with PlantUML</figcaption>
</figure>
<p>You may want to reference images in your thesis. In this case, you are
encouraged to make them <em>floating</em>, and reference them by means of
labels. For instance, in , we describe a class diagram produced by means of <a
href="http://plantuml.com">PlantUML</a>.</p>
</section>
</section>
<section id="foochap:implementation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span>
Implementation</h1>
<p>Write implementation here.</p>
<div class="sourceCode" id="foolst:pieceofcode" data-float=""
data-language="Java" data-caption="A piece of code"
label="lst:pieceofcode"><pre class="sourceCode java"><code class="sourceCode java"><span id="foolst:pieceofcode-1"><a href="#foolst:pieceofcode-1" aria-hidden="true" tabindex="-1"></a><span class="kw">public</span> <span class="kw">class</span> HelloWorld <span class="op">{</span></span>
<span id="foolst:pieceofcode-2"><a href="#foolst:pieceofcode-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">public</span> <span class="dt">static</span> <span class="dt">void</span> <span class="fu">main</span><span class="op">(</span><span class="bu">String</span><span class="op">[]</span> args<span class="op">)</span> <span class="op">{</span></span>
<span id="foolst:pieceofcode-3"><a href="#foolst:pieceofcode-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Prints &quot;Hello, World&quot; to the terminal window.</span></span>
<span id="foolst:pieceofcode-4"><a href="#foolst:pieceofcode-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">System</span><span class="op">.</span><span class="fu">out</span><span class="op">.</span><span class="fu">println</span><span class="op">(</span><span class="st">&quot;Hello, World&quot;</span><span class="op">);</span></span>
<span id="foolst:pieceofcode-5"><a href="#foolst:pieceofcode-5" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="foolst:pieceofcode-6"><a href="#foolst:pieceofcode-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>You may need to reference listings in your thesis. In this case, you are
encouraged to make them <em>floating</em>, and reference them by means of
labels. For instance, in , we describe an hello world program in Java.</p>
</section>
<section id="foochap:validation" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Validation
and Testing</h1>
<p>Result validation and testing on real-life robots here</p>
</section>
<section id="foochap:conclusions" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span>
Conclusions</h1>
<p>Humans are capable of executing a wide variety of complex actions, based on
prior experience. In this thesis, we provided a possible approach to novel
high-level skill generation by combining movement primitives learned by CNMP
models.</p>
<section id="foofuture-work" class="level2" data-number="7.1">
<h2 data-number="7.1"><span class="header-section-number">7.1</span> Future
work</h2>
<p>Possible future works are ...</p>
<div id="foorefs" class="references csl-bib-body hanging-indent" role="list">
<div id="fooref-Ahmetoglu_2022" class="csl-entry" role="listitem">
Ahmetoglu, Alper, M. Yunus Seker, Justus Piater, Erhan Oztop, e Emre Ugur.
2022. <span>&#xAB;DeepSym: Deep Symbol Generation and Rule Learning for Planning
from Unsupervised Robot Interaction&#xBB;</span>. <em>Journal of Artificial
Intelligence Research</em> 75 (novembre): 709&#x2013;45. <a
href="https://doi.org/10.1613/jair.1.13754">https://doi.org/10.1613/jair.1.13754</a>.
</div>
<div id="fooref-url:anaconda" class="csl-entry" role="listitem">
Anaconda. 2023. <span>&#xAB;Anaconda&#xBB;</span>. <a href="https://www.anaconda.com"
class="uri">https://www.anaconda.com</a>.
</div>
<div id="fooref-ARGALL2009469" class="csl-entry" role="listitem">
Argall, Brenna D., Sonia Chernova, Manuela Veloso, e Brett Browning. 2009.
<span>&#xAB;A survey of robot learning from demonstration&#xBB;</span>. <em>Robotics and
Autonomous Systems</em> 57 (5): 469&#x2013;83. https://doi.org/<a
href="https://doi.org/10.1016/j.robot.2008.10.024">https://doi.org/10.1016/j.robot.2008.10.024</a>.
</div>
<div id="fooref-biagiotti2008trajectory" class="csl-entry" role="listitem">
Biagiotti, Luigi, e Claudio Melchiorri. 2008. <em>Trajectory planning for
automatic machines and robots</em>. Springer Science &amp; Business Media.
</div>
<div id="fooref-calinon2016tutorial" class="csl-entry" role="listitem">
Calinon, Sylvain. 2016. <span>&#xAB;A tutorial on task-parameterized movement
learning and retrieval&#xBB;</span>. <em>Intelligent service robotics</em> 9: 1&#x2013;29.
</div>
<div id="fooref-chu2013using" class="csl-entry" role="listitem">
Chu, Vivian, Ian McMahon, Lorenzo Riano, Craig G McDonald, Qin He, Jorge
Martinez Perez-Tejada, Michael Arrigo, et al. 2013. <span>&#xAB;Using robotic
exploratory procedures to learn the meaning of haptic adjectives&#xBB;</span>. In
<em>2013 IEEE International Conference on Robotics and Automation</em>,
3048&#x2013;55. IEEE.
</div>
<div id="fooref-damianou2013deep" class="csl-entry" role="listitem">
Damianou, Andreas, e Neil D Lawrence. 2013. <span>&#xAB;Deep gaussian
processes&#xBB;</span>. In <em>Artificial intelligence and statistics</em>, 207&#x2013;15.
PMLR.
</div>
<div id="fooref-deak2003development" class="csl-entry" role="listitem">
Deak, Gedeon O. 2003. <span>&#xAB;The development of cognitive flexibility and
language abilities&#xBB;</span>. <em>Advances in child development and
behavior</em> 31: 273&#x2013;328.
</div>
<div id="fooref-dechant2021toward" class="csl-entry" role="listitem">
DeChant, Chad, e Daniel Bauer. 2021. <span>&#xAB;Toward robots that learn to
summarize their actions in natural language: a set of tasks&#xBB;</span>. In
<em>5th Annual Conference on Robot Learning, Blue Sky Submission Track</em>.
<a
href="https://openreview.net/forum?id=n3AW_ISWCXf">https://openreview.net/forum?id=n3AW_ISWCXf</a>.
</div>
<div id="fooref-egidi2006decomposition" class="csl-entry" role="listitem">
Egidi, Massimo. 2006. <span>&#xAB;Decomposition patterns in problem
solving&#xBB;</span>. <em>Contributions to Economic Analysis</em> 280: 15&#x2013;46.
</div>
<div id="fooref-DBLP:journals/corr/abs-1807-01613" class="csl-entry"
role="listitem">
Garnelo, Marta, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton,
Murray Shanahan, Yee Whye Teh, Danilo J. Rezende, e S. M. Ali Eslami. 2018.
<span>&#xAB;Conditional Neural Processes&#xBB;</span>. <em>CoRR</em> abs/1807.01613. <a
href="http://arxiv.org/abs/1807.01613">http://arxiv.org/abs/1807.01613</a>.
</div>
<div id="fooref-garnelo2018neural" class="csl-entry" role="listitem">
Garnelo, Marta, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J
Rezende, SM Eslami, e Yee Whye Teh. 2018. <span>&#xAB;Neural processes&#xBB;</span>.
<em>arXiv preprint arXiv:1807.01622</em>.
</div>
<div id="fooref-gasparetto2007new" class="csl-entry" role="listitem">
Gasparetto, Alessandro, e V Zanotto. 2007. <span>&#xAB;A new method for smooth
trajectory planning of robot manipulators&#xBB;</span>. <em>Mechanism and machine
theory</em> 42 (4): 455&#x2013;71.
</div>
<div id="fooref-gupta2019relay" class="csl-entry" role="listitem">
Gupta, Abhishek, Vikash Kumar, Corey Lynch, Sergey Levine, e Karol Hausman.
2019. <span>&#xAB;Relay Policy Learning: Solving Long-Horizon Tasks via Imitation
and Reinforcement Learning&#xBB;</span>. <a
href="https://arxiv.org/abs/1910.11956">https://arxiv.org/abs/1910.11956</a>.
</div>
<div id="fooref-horton2012affordances" class="csl-entry" role="listitem">
Horton, Thomas E, Arpan Chakraborty, e Robert St Amant. 2012.
<span>&#xAB;Affordances for robots: a brief survey&#xBB;</span>. <em>AVANT. Pismo
Awangardy Filozoficzno-Naukowej</em> 2: 70&#x2013;84.
</div>
<div id="fooref-hu2019hierarchical" class="csl-entry" role="listitem">
Hu, Hengyuan, Denis Yarats, Qucheng Gong, Yuandong Tian, e Mike Lewis. 2019.
<span>&#xAB;Hierarchical decision making by generating and following natural
language instructions&#xBB;</span>. <em>Advances in neural information processing
systems</em> 32.
</div>
<div id="fooref-Goertler2018VisualExplorationGaussian" class="csl-entry"
role="listitem">
J. G&#xF6;rtler, O. Deussen, R. Kehlbeck. 2018. <span>&#xAB;A Visual Exploration of
Gaussian Processes&#xBB;</span>. In <em>Proceedings of the Workshop on
Visualization for AI Explainability (VISxAI)</em>. <a
href="https://www.jgoertler.com/visual-exploration-gaussian-processes/">https://www.jgoertler.com/visual-exploration-gaussian-processes/</a>.
</div>
<div id="fooref-7523298" class="csl-entry" role="listitem">
Jamone, Lorenzo, Emre Ugur, Angelo Cangelosi, Luciano Fadiga, Alexandre
Bernardino, Justus Piater, e Jos&#xE9; Santos-Victor. 2018. <span>&#xAB;Affordances in
Psychology, Neuroscience, and Robotics: A Survey&#xBB;</span>. <em>IEEE
Transactions on Cognitive and Developmental Systems</em> 10 (1): 4&#x2013;25. <a
href="https://doi.org/10.1109/TCDS.2016.2594134">https://doi.org/10.1109/TCDS.2016.2594134</a>.
</div>
<div id="fooref-url:jupyter" class="csl-entry" role="listitem">
Jupyter Notebook. 2023. <span>&#xAB;Jupyter Notebook&#xBB;</span>. <a
href="https://jupyter.org" class="uri">https://jupyter.org</a>.
</div>
<div id="fooref-kapoor2010gaussian" class="csl-entry" role="listitem">
Kapoor, Ashish, Kristen Grauman, Raquel Urtasun, e Trevor Darrell. 2010.
<span>&#xAB;Gaussian processes for object categorization&#xBB;</span>. <em>International
journal of computer vision</em> 88: 169&#x2013;88.
</div>
<div id="fooref-karmiloff1994beyond" class="csl-entry" role="listitem">
Karmiloff-Smith, By A. 1994. <span>&#xAB;Beyond modularity: A developmental
perspective on cognitive science&#xBB;</span>. <em>European journal of disorders of
communication</em> 29 (1): 95&#x2013;105.
</div>
<div id="fooref-kim2007clustering" class="csl-entry" role="listitem">
Kim, Hyun-Chul, e Jaewook Lee. 2007. <span>&#xAB;Clustering based on gaussian
processes&#xBB;</span>. <em>Neural computation</em> 19 (11): 3088&#x2013;3107.
</div>
<div id="fooref-konidaris2019necessity" class="csl-entry" role="listitem">
Konidaris, George. 2019. <span>&#xAB;On the necessity of abstraction&#xBB;</span>.
<em>Current opinion in behavioral sciences</em> 29: 1&#x2013;7.
</div>
<div id="fooref-kroemer2021review" class="csl-entry" role="listitem">
Kroemer, Oliver, Scott Niekum, e George Konidaris. 2021. <span>&#xAB;A review of
robot learning for manipulation: Challenges, representations, and
algorithms&#xBB;</span>. <em>The Journal of Machine Learning Research</em> 22 (1):
1395&#x2013;1476.
</div>
<div id="fooref-lee2011incremental" class="csl-entry" role="listitem">
Lee, Dongheui, e Christian Ott. 2011. <span>&#xAB;Incremental kinesthetic teaching
of motion primitives using the motion refinement tube&#xBB;</span>. <em>Autonomous
Robots</em> 31: 115&#x2013;31.
</div>
<div id="fooref-maranesi2014cortical" class="csl-entry" role="listitem">
Maranesi, Monica, Luca Bonini, e Leonardo Fogassi. 2014. <span>&#xAB;Cortical
processing of object affordances for self and others&#x2019; action&#xBB;</span>.
<em>Frontiers in psychology</em> 5: 538.
</div>
<div id="fooref-nguyen2009model" class="csl-entry" role="listitem">
Nguyen-Tuong, Duy, Matthias Seeger, e Jan Peters. 2009. <span>&#xAB;Model learning
with local gaussian process regression&#xBB;</span>. <em>Advanced Robotics</em> 23
(15): 2015&#x2013;34.
</div>
<div id="fooref-osiurak2017affordance" class="csl-entry" role="listitem">
Osiurak, Fran&#xE7;ois, Yves Rossetti, e Arnaud Badets. 2017. <span>&#xAB;What is an
affordance? 40 years later&#xBB;</span>. <em>Neuroscience &amp; Biobehavioral
Reviews</em> 77: 403&#x2013;17.
</div>
<div id="fooref-paraschos2013probabilistic" class="csl-entry" role="listitem">
Paraschos, Alexandros, Christian Daniel, Jan R Peters, e Gerhard Neumann.
2013. <span>&#xAB;Probabilistic movement primitives&#xBB;</span>. <em>Advances in neural
information processing systems</em> 26.
</div>
<div id="fooref-pastor2009learning" class="csl-entry" role="listitem">
Pastor, Peter, Heiko Hoffmann, Tamim Asfour, e Stefan Schaal. 2009.
<span>&#xAB;Learning and generalization of motor skills by learning from
demonstration&#xBB;</span>. In <em>2009 IEEE International Conference on Robotics
and Automation</em>, 763&#x2013;68. IEEE.
</div>
<div id="fooref-paszke2019pytorch" class="csl-entry" role="listitem">
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, et al. 2019. <span>&#xAB;PyTorch: An Imperative Style,
High-Performance Deep Learning Library&#xBB;</span>. <a
href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</a>.
</div>
<div id="fooref-url:pytorch" class="csl-entry" role="listitem">
Pytorch. 2023. <span>&#xAB;Tensors and Dynamic neural networks in Python with
strong GPU acceleration&#xBB;</span>. <a href="https://pytorch.org"
class="uri">https://pytorch.org</a>.
</div>
<div id="fooref-ravichandar2020recent" class="csl-entry" role="listitem">
Ravichandar, Harish, Athanasios S Polydoros, Sonia Chernova, e Aude Billard.
2020. <span>&#xAB;Recent advances in robot learning from demonstration&#xBB;</span>.
<em>Annual review of control, robotics, and autonomous systems</em> 3:
297&#x2013;330.
</div>
<div id="fooref-url:3fgripper" class="csl-entry" role="listitem">
Robotiq. 2023a. <span>&#xAB;3-Finger Adaptive Robot Gripper&#xBB;</span>. <a
href="https://robotiq.com/products/3-finger-adaptive-robot-gripper"
class="uri">https://robotiq.com/products/3-finger-adaptive-robot-gripper</a>.
</div>
<div id="fooref-url:ftsensor" class="csl-entry" role="listitem">
&#x2014;&#x2014;&#x2014;. 2023b. <span>&#xAB;FT 300-S Force Torque Sensor&#xBB;</span>. <a
href="https://robotiq.com/products/ft-300-force-torque-sensor"
class="uri">https://robotiq.com/products/ft-300-force-torque-sensor</a>.
</div>
<div id="fooref-url:ros" class="csl-entry" role="listitem">
ROS. 2023. <span>&#xAB;Robot Operating System&#xBB;</span>. <a
href="https://www.ros.org" class="uri">https://www.ros.org</a>.
</div>
<div id="fooref-rosen2022role" class="csl-entry" role="listitem">
Rosen, Eric, Ben M Abbatematteo, Skye Thompson, Tuluhan Akbulut, e George
Konidaris. 2022. <span>&#xAB;On the Role of Structure in Manipulation Skill
Learning&#xBB;</span>. In <em>CoRL 2022 Workshop on Learning, Perception, and
Abstraction for Long-Horizon Planning</em>.
</div>
<div id="fooref-salimbeni2017doubly" class="csl-entry" role="listitem">
Salimbeni, Hugh, e Marc Deisenroth. 2017. <span>&#xAB;Doubly stochastic variational
inference for deep Gaussian processes&#xBB;</span>. <em>Advances in neural
information processing systems</em> 30.
</div>
<div id="fooref-schaal2006dynamic" class="csl-entry" role="listitem">
Schaal, Stefan. 2006. <span>&#xAB;Dynamic movement primitives-a framework for motor
control in humans and humanoid robotics&#xBB;</span>. In <em>Adaptive motion of
animals and machines</em>, 261&#x2013;80. Springer.
</div>
<div id="fooref-seeger2004gaussian" class="csl-entry" role="listitem">
Seeger, Matthias. 2004. <span>&#xAB;Gaussian processes for machine
learning&#xBB;</span>. <em>International journal of neural systems</em> 14 (02):
69&#x2013;106.
</div>
<div id="fooref-simeonov2021long" class="csl-entry" role="listitem">
Simeonov, Anthony, Yilun Du, Beomjoon Kim, Francois Hogan, Joshua Tenenbaum,
Pulkit Agrawal, e Alberto Rodriguez. 2021. <span>&#xAB;A long horizon planning
framework for manipulating rigid pointcloud objects&#xBB;</span>. In <em>Conference
on Robot Learning</em>, 1582&#x2013;1601. PMLR.
</div>
<div id="fooref-snelson2006sparse" class="csl-entry" role="listitem">
Snelson, Edward, e Zoubin Ghahramani. 2006. <span>&#xAB;Sparse Gaussian processes
using pseudo-inputs&#xBB;</span>. <em>Advances in Neural Information Processing
Systems</em> 18: 1259&#x2013;66.
</div>
<div id="fooref-ugur2020compliant" class="csl-entry" role="listitem">
Ugur, Emre, e Hakan Girgin. 2020. <span>&#xAB;Compliant parametric dynamic movement
primitives&#xBB;</span>. <em>Robotica</em> 38 (3): 457&#x2013;74.
</div>
<div id="fooref-Ugur-RSS-19" class="csl-entry" role="listitem">
Ugur, Muhammet Yunus Seker AND Mert Imre AND Justus Piater AND Emre. 2019.
<span>&#xAB;Conditional Neural Movement Primitives&#xBB;</span>. In <em>Proceedings of
Robotics: Science and Systems</em>. FreiburgimBreisgau, Germany. <a
href="https://doi.org/10.15607/RSS.2019.XV.071">https://doi.org/10.15607/RSS.2019.XV.071</a>.
</div>
<div id="fooref-url:ur10" class="csl-entry" role="listitem">
Universal Robots. 2023. <span>&#xAB;UR10 (robot) &#x2014; medium duty industrial
collaborative robot&#xBB;</span>. <a
href="https://www.universal-robots.com/products/ur10-robot/"
class="uri">https://www.universal-robots.com/products/ur10-robot/</a>.
</div>
<div id="fooref-wiki:baxter" class="csl-entry" role="listitem">
Wikipedia contributors. 2023. <span>&#xAB;Baxter (robot) &#x2014;
<span>Wikipedia</span><span>,</span> The Free Encyclopedia&#xBB;</span>. <a
href="https://en.wikipedia.org/w/index.php?title=Baxter_(robot)&amp;oldid=1183931177"
class="uri">https://en.wikipedia.org/w/index.php?title=Baxter_(robot)&amp;oldid=1183931177</a>.
</div>
<div id="fooref-wilson2016deep" class="csl-entry" role="listitem">
Wilson, Andrew Gordon, Zhiting Hu, Ruslan Salakhutdinov, e Eric P Xing. 2016.
<span>&#xAB;Deep kernel learning&#xBB;</span>. In <em>Artificial intelligence and
statistics</em>, 370&#x2013;78. PMLR.
</div>
</div>
</section>
</section>                    
                </div>
            </div>

</script>
            <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>
        </div>
    </body>
</html>
