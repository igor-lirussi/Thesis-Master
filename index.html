<!DOCTYPE html>
<html  lang="it"  dir="ltr">

    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Master Thesis - Igor Lirussi</title>
        <link rel="shortcut icon" type="image/png" href="favicon.png">
        <link rel="apple-touch-icon-precomposed" href="apple-touch-icon.png">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/uikit/2.26.4/css/uikit.gradient.css">

        <link rel="stylesheet" href="style.css">
        <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />
        <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
        <script src="./pandoc-uikit/uikit.js"></script>
        <script src="./pandoc-uikit/scripts.js"></script>
        <script src="./pandoc-uikit/jquery.sticky-kit.js"></script>

        <meta name="generator" content="pandoc-uikit" />
                <meta name="author" content="Lirussi Igor" />
                        <meta name="date" content="2023-12-04" />
                <title>Master Thesis - Igor Lirussi</title>
        <style type="text/css">code{white-space: pre;}</style>
                <style type="text/css">q { quotes: "&#x201C;" "&#x201D;" "&#x2018;" "&#x2019;"; }</style>
                        <style type="text/css">
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            .sourceCode { overflow: visible; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #aaaaaa;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
            div.sourceCode
              {  background-color: #f8f8f8; }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #ef2929; } /* Alert */
            code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #204a87; } /* Attribute */
            code span.bn { color: #0000cf; } /* BaseN */
            code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #4e9a06; } /* Char */
            code span.cn { color: #8f5902; } /* Constant */
            code span.co { color: #8f5902; font-style: italic; } /* Comment */
            code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
            code span.dt { color: #204a87; } /* DataType */
            code span.dv { color: #0000cf; } /* DecVal */
            code span.er { color: #a40000; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #0000cf; } /* Float */
            code span.fu { color: #204a87; font-weight: bold; } /* Function */
            code span.im { } /* Import */
            code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
            code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
            code span.ot { color: #8f5902; } /* Other */
            code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
            code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
            code span.ss { color: #4e9a06; } /* SpecialString */
            code span.st { color: #4e9a06; } /* String */
            code span.va { color: #000000; } /* Variable */
            code span.vs { color: #4e9a06; } /* VerbatimString */
            code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
        </style>
                        <link rel="stylesheet" href="./pandoc-uikit/uikit.css" />
                                          <script
                                          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
                                          type="text/javascript"></script>
                               
    </head>

    <body>


        <div class="uk-container uk-container-center uk-margin-top uk-margin-large-bottom">

                        <div class="uk-grid" data-uk-grid-margin>
                <div class="uk-width-1-1">
                    <h1 class="uk-heading">Master Thesis - Igor Lirussi</h1>
                                        <h1 class="uk-heading" style="margin:0">2023-12-04</h1>
                                                            <p class="uk-text-large">Lirussi
Igor</p>
                                    </div>
            </div>
            
            <div class="uk-grid" data-uk-grid-margin >          
                <div class="uk-width-medium-1-4">
                    <div class="uk-overflow-container" data-uk-sticky="{top:25,media: 768}">
                        <div class="uk-panel uk-panel-box menu-begin" >

                                                        <ul>
                                                        <li><a
                                                        href="#fooabstract"
                                                        id="footoc-abstract">Abstract</a></li>
                                                        <li><a
                                                        href="#foosection"
                                                        id="footoc-section"></a></li>
                                                        <li><a
                                                        href="#fooacknowledgements"
                                                        id="footoc-acknowledgements">Acknowledgements</a></li>
                                                        <li><a
                                                        href="#foolist-of-symbols"
                                                        id="footoc-list-of-symbols">List
                                                        of Symbols</a></li>
                                                        <li><a
                                                        href="#foolist-of-acronymsabbreviations"
                                                        id="footoc-list-of-acronymsabbreviations">List
                                                        of
                                                        Acronyms/Abbreviations</a></li>
                                                        <li><a
                                                        href="#foochap:introduction"
                                                        id="footoc-chap:introduction"><span
                                                        class="toc-section-number">1</span>
                                                        Introduction</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foooverview"
                                                        id="footoc-overview"><span
                                                        class="toc-section-number">1.1</span>
                                                        Overview</a></li>
                                                        <li><a
                                                        href="#foochallenges"
                                                        id="footoc-challenges"><span
                                                        class="toc-section-number">1.2</span>
                                                        Challenges</a></li>
                                                        <li><a
                                                        href="#fooobjectives"
                                                        id="footoc-objectives"><span
                                                        class="toc-section-number">1.3</span>
                                                        Objectives</a></li>
                                                        <li><a
                                                        href="#foothesis-structure"
                                                        id="footoc-thesis-structure"><span
                                                        class="toc-section-number">1.4</span>
                                                        Thesis
                                                        Structure</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:background"
                                                        id="footoc-chap:background"><span
                                                        class="toc-section-number">2</span>
                                                        Related Work</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foogaussian-processes"
                                                        id="footoc-gaussian-processes"><span
                                                        class="toc-section-number">2.1</span>
                                                        Gaussian
                                                        Processes</a></li>
                                                        <li><a
                                                        href="#fooconditional-neural-processes"
                                                        id="footoc-conditional-neural-processes"><span
                                                        class="toc-section-number">2.2</span>
                                                        Conditional Neural
                                                        Processes</a></li>
                                                        <li><a
                                                        href="#fooconditional-neural-movement-primitives"
                                                        id="footoc-conditional-neural-movement-primitives"><span
                                                        class="toc-section-number">2.3</span>
                                                        Conditional Neural
                                                        Movement
                                                        Primitives</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:platforms"
                                                        id="footoc-chap:platforms"><span
                                                        class="toc-section-number">3</span>
                                                        Platforms</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foobaxter-robot"
                                                        id="footoc-baxter-robot"><span
                                                        class="toc-section-number">3.1</span>
                                                        Baxter Robot</a></li>
                                                        <li><a
                                                        href="#foour10-robot"
                                                        id="footoc-ur10-robot"><span
                                                        class="toc-section-number">3.2</span>
                                                        UR10 Robot</a></li>
                                                        <li><a
                                                        href="#foof-robotiq-gripper"
                                                        id="footoc-f-robotiq-gripper"><span
                                                        class="toc-section-number">3.3</span>
                                                        3F Robotiq
                                                        Gripper</a></li>
                                                        <li><a
                                                        href="#fooft-300-s-force-torque-sensor"
                                                        id="footoc-ft-300-s-force-torque-sensor"><span
                                                        class="toc-section-number">3.4</span>
                                                        FT 300-S Force Torque
                                                        Sensor</a></li>
                                                        <li><a
                                                        href="#fooframeworks"
                                                        id="footoc-frameworks"><span
                                                        class="toc-section-number">3.5</span>
                                                        Frameworks</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:design"
                                                        id="footoc-chap:design"><span
                                                        class="toc-section-number">4</span>
                                                        Design</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foopartial-skill-combination"
                                                        id="footoc-partial-skill-combination"><span
                                                        class="toc-section-number">4.1</span>
                                                        Partial Skill
                                                        Combination</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foocnmp-model-with-task-parameter-only-in-condition"
                                                        id="footoc-cnmp-model-with-task-parameter-only-in-condition"><span
                                                        class="toc-section-number">4.1.1</span>
                                                        CNMP model with
                                                        task-parameter only in
                                                        condition</a></li>
                                                        <li><a
                                                        href="#foocnmp-model-with-task-parameter-only-in-query"
                                                        id="footoc-cnmp-model-with-task-parameter-only-in-query"><span
                                                        class="toc-section-number">4.1.2</span>
                                                        CNMP model with
                                                        task-parameter only in
                                                        query</a></li>
                                                        <li><a
                                                        href="#foocomparison-of-the-previous-models"
                                                        id="footoc-comparison-of-the-previous-models"><span
                                                        class="toc-section-number">4.1.3</span>
                                                        Comparison of the
                                                        previous
                                                        models</a></li>
                                                        <li><a
                                                        href="#foocnmp-changing-task-in-time-with-one-conditioning-point"
                                                        id="footoc-cnmp-changing-task-in-time-with-one-conditioning-point"><span
                                                        class="toc-section-number">4.1.4</span>
                                                        CNMP changing task in
                                                        time with one
                                                        conditioning
                                                        point</a></li>
                                                        <li><a
                                                        href="#foocnmp-changing-task-in-time-with-multiple-conditioning-points"
                                                        id="footoc-cnmp-changing-task-in-time-with-multiple-conditioning-points"><span
                                                        class="toc-section-number">4.1.5</span>
                                                        CNMP changing task in
                                                        time with multiple
                                                        conditioning
                                                        points</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#fooend-to-end-skill-concatenation"
                                                        id="footoc-end-to-end-skill-concatenation"><span
                                                        class="toc-section-number">4.2</span>
                                                        End-To-End Skill
                                                        Concatenation</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foocnmp-for-skill-concatenation"
                                                        id="footoc-cnmp-for-skill-concatenation"><span
                                                        class="toc-section-number">4.2.1</span>
                                                        CNMP for skill
                                                        concatenation</a></li>
                                                        <li><a
                                                        href="#foocnmp-embedding-environment-representation"
                                                        id="footoc-cnmp-embedding-environment-representation"><span
                                                        class="toc-section-number">4.2.2</span>
                                                        CNMP embedding
                                                        environment
                                                        representation</a></li>
                                                        </ul></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:implementation"
                                                        id="footoc-chap:implementation"><span
                                                        class="toc-section-number">5</span>
                                                        Implementation</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foopartial-skill-combination-1"
                                                        id="footoc-partial-skill-combination-1"><span
                                                        class="toc-section-number">5.1</span>
                                                        Partial Skill
                                                        Combination</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foocnmp-model-with-task-parameter-only-in-condition-1"
                                                        id="footoc-cnmp-model-with-task-parameter-only-in-condition-1"><span
                                                        class="toc-section-number">5.1.1</span>
                                                        CNMP model with
                                                        task-parameter only in
                                                        condition</a></li>
                                                        <li><a
                                                        href="#foocnmp-model-with-task-parameter-only-in-query-1"
                                                        id="footoc-cnmp-model-with-task-parameter-only-in-query-1"><span
                                                        class="toc-section-number">5.1.2</span>
                                                        CNMP model with
                                                        task-parameter only in
                                                        query</a></li>
                                                        <li><a
                                                        href="#foocomparison-of-the-previous-models-1"
                                                        id="footoc-comparison-of-the-previous-models-1"><span
                                                        class="toc-section-number">5.1.3</span>
                                                        Comparison of the
                                                        previous
                                                        models</a></li>
                                                        <li><a
                                                        href="#foocnmp-changing-task-in-time-with-one-conditioning-point-1"
                                                        id="footoc-cnmp-changing-task-in-time-with-one-conditioning-point-1"><span
                                                        class="toc-section-number">5.1.4</span>
                                                        CNMP changing task in
                                                        time with one
                                                        conditioning
                                                        point</a></li>
                                                        <li><a
                                                        href="#foocnmp-changing-task-in-time-with-multiple-conditioning-points-1"
                                                        id="footoc-cnmp-changing-task-in-time-with-multiple-conditioning-points-1"><span
                                                        class="toc-section-number">5.1.5</span>
                                                        CNMP changing task in
                                                        time with multiple
                                                        conditioning
                                                        points</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#fooend-to-end-skill-concatenation-1"
                                                        id="footoc-end-to-end-skill-concatenation-1"><span
                                                        class="toc-section-number">5.2</span>
                                                        End-To-End Skill
                                                        Concatenation</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foobuilding-the-graph-of-concatenated-trajectories"
                                                        id="footoc-building-the-graph-of-concatenated-trajectories"><span
                                                        class="toc-section-number">5.2.1</span>
                                                        Building the graph of
                                                        concatenated
                                                        trajectories</a></li>
                                                        <li><a
                                                        href="#foopruning-the-graph-of-concatenated-trajectories"
                                                        id="footoc-pruning-the-graph-of-concatenated-trajectories"><span
                                                        class="toc-section-number">5.2.2</span>
                                                        Pruning the graph of
                                                        concatenated
                                                        trajectories</a></li>
                                                        </ul></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:validation"
                                                        id="footoc-chap:validation"><span
                                                        class="toc-section-number">6</span>
                                                        Validation and
                                                        Testing</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foopartial-skill-combination-2"
                                                        id="footoc-partial-skill-combination-2"><span
                                                        class="toc-section-number">6.1</span>
                                                        Partial Skill
                                                        Combination</a></li>
                                                        <li><a
                                                        href="#fooend-to-end-skill-concatenation-2"
                                                        id="footoc-end-to-end-skill-concatenation-2"><span
                                                        class="toc-section-number">6.2</span>
                                                        End-To-End Skill
                                                        Concatenation</a></li>
                                                        </ul></li>
                                                        <li><a
                                                        href="#foochap:conclusions"
                                                        id="footoc-chap:conclusions"><span
                                                        class="toc-section-number">7</span>
                                                        Conclusions</a>
                                                        <ul>
                                                        <li><a
                                                        href="#foofuture-work"
                                                        id="footoc-future-work"><span
                                                        class="toc-section-number">7.1</span>
                                                        Future work</a></li>
                                                        </ul></li>
                                                        </ul>
                            
                        </div>
                    </div>
                </div>

                <div class="uk-width-medium-3-4">
<div class="titlepage">
<div class="center">
<p><strong>ALMA MATER STUDIORUM &#x2013; UNIVERSITY OF BOLOGNA<br />
CESENA CAMPUS</strong><br />
</p>
<p>School of Engineering and Architecture<br />
Second Cycle Degree/Two-year Master in<br />
Computer Science and Engineering</p>
<p><strong>Novel robotic skill synthesis<br />
with Conditional Neural<br />
Movement Primitives</strong></p>
<p>Master&#x2019;s thesis in<br />
<span class="smallcaps">Intelligent Robotic Systems</span></p>
<div class="flushleft">
<p><em>Supervisor</em><br />
<strong>Prof.</strong> <strong>Andrea Roli</strong><br />
<em>Co-Supervisor</em><br />
<strong>Prof.</strong> <strong>Emre U&#x11F;ur</strong><br />
(Bo&#x11F;azi&#xE7;i University, Istanbul)</p>
</div>
<div class="flushright">
<p><em>Candidate</em><br />
<strong>Igor Lirussi</strong></p>
</div>
<p><br />
</p>
<p>Academic Year 2022-2023</p>
</div>
</div>
<section id="fooabstract" class="level1 unnumbered">
<h1 class="unnumbered">Abstract</h1>
<p>Max 2000 characters, strict. UniBo has that limit in the upload system!
Will write at the end.</p>
</section>
<section id="foosection" class="level1 unnumbered">
<h1 class="unnumbered"></h1>
<div class="flushright">
<p><em>Dedication here</em></p>
</div>
</section>
<section id="fooacknowledgements" class="level1 unnumbered">
<h1 class="unnumbered">Acknowledgements</h1>
<p>Acknowledgments here.</p>
</section>
<section id="foolist-of-symbols" class="level1 unnumbered">
<h1 class="unnumbered">List of Symbols</h1>
<div class="tabbing">
<p>&#x304;&#x304;</p>
<p><span class="math inline">\(a_{ij}\)</span></p>
<p>Description of <span class="math inline">\(a_{ij}\)</span></p>
<p><br />
<span class="math inline">\(t\)</span></p>
<p>Time</p>
<p><br />
<span class="math inline">\(r\)</span></p>
<p>Representation</p>
<p><br />
<span class="math inline">\(x\)</span></p>
<p>Target point (query point)</p>
<p><br />
<span class="math inline">\(SM(t)\)</span></p>
<p>Sensorimotor data at time t</p>
<p><br />
<span class="math inline">\(E\)</span></p>
<p>Encoder network</p>
<p><br />
<span class="math inline">\(Q\)</span></p>
<p>Decoder network</p>
<p><br />
<span class="math inline">\(O\)</span></p>
<p>Set of observation points (conditioning points)</p>
<p><br />
<span class="math inline">\(D\)</span></p>
<p>Set of demonstration trajectories (expert demonstrations)</p>
<p><br />
<span class="math inline">\(T\)</span></p>
<p>Set of target points <span class="math inline">\(x\)</span> desired</p>
<p><br />
</p>
<p><br />
<span class="math inline">\(\alpha\)</span></p>
<p>Blending parameter <em>or</em> scale</p>
<p><br />
<span class="math inline">\(\beta_t(i)\)</span></p>
<p>Backward variable</p>
<p><br />
<span class="math inline">\(\gamma\)</span></p>
<p>External parameter (Task parameter) (TP)</p>
<p><br />
<span class="math inline">\(\theta\)</span></p>
<p>Parameter set of Decoder Network</p>
<p><br />
<span class="math inline">\(\phi\)</span></p>
<p>Parameter set of Encoder Network</p>
<p><br />
<span class="math inline">\(\sigma\)</span></p>
<p>Standard Deviation</p>
<p><br />
<span class="math inline">\(\mu\)</span></p>
<p>Mean</p>
<p><br />
<span class="math inline">\(\tau\)</span></p>
<p>Trajectory</p>
<p><br />
</p>
</div>
</section>
<section id="foolist-of-acronymsabbreviations" class="level1 unnumbered">
<h1 class="unnumbered">List of Acronyms/Abbreviations</h1>
<div class="tabbing">
<p>&#x304;&#x304;</p>
<p>1D</p>
<p>One Dimensional</p>
<p><br />
2D</p>
<p>Two Dimensional</p>
<p><br />
3D</p>
<p>Three Dimensional</p>
<p><br />
IR</p>
<p>Infrared</p>
<p><br />
ROS</p>
<p>Robot Operating System</p>
<p><br />
RGB</p>
<p>Red Green Blue</p>
<p><br />
RGBD</p>
<p>Red Green Blue Depth</p>
<p><br />
SL</p>
<p>Supervised Learning</p>
<p><br />
LfD</p>
<p>Learning from Demonstration</p>
<p><br />
ML</p>
<p>Machine Learning</p>
<p><br />
DL</p>
<p>Deep Learnign</p>
<p><br />
GMM</p>
<p>Gaussian Mixture Model</p>
<p><br />
HMM</p>
<p>Hidden Markov Model</p>
<p><br />
MSE</p>
<p>Mean Squared Error</p>
<p><br />
ReLU</p>
<p>Rectified Linear Unit</p>
<p><br />
GP</p>
<p>Gaussian Processes</p>
<p><br />
NP</p>
<p>Neural Processes</p>
<p><br />
CNP</p>
<p>Conditional Neural Processes</p>
<p><br />
DNP</p>
<p>Dynamic Movement Primitives</p>
<p><br />
Pro-MP</p>
<p>Probabilistic Movement Primitives</p>
<p><br />
CNMP</p>
<p>Conditional Neural Movement Primitives</p>
<p><br />
TP</p>
<p>Task Parameter</p>
<p><br />
YOLO</p>
<p>You Only Look Once model</p>
<p><br />
UR10</p>
<p>Universal Robot 10</p>
<p><br />
DOF</p>
<p>Degrees of freedom</p>
<p><br />
IMU</p>
<p>Inertial measurement unit</p>
<p><br />
RPC</p>
<p>Remote Procedure Call</p>
<p><br />
CPU</p>
<p>Central Processing Unit</p>
<p><br />
GPU</p>
<p>Graphic Processing Unit</p>
<p><br />
</p>
</div>
</section>
<section id="foochap:introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span>
Introduction</h1>
<section id="foooverview" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Overview</h2>
<p>Humans have a remarkable ability to achieve complex goals in a wide variety
of tasks. A person is usually exposed to different scenarios during the day,
starting from the home environment to the commute, work, mealtime, and so on.
The versatility of our species is a key factor, and the human cognitive
flexibility has been appointed as a major driver in evolution <span
class="citation" data-cites="deak2003development">(Deak 2003)</span>, <span
class="citation" data-cites="karmiloff1994beyond">(Karmiloff-Smith
1994)</span>. Some situations are more complicated than others; nevertheless,
regardless of their difference, humans excel in meeting the different demands
to solve the tasks desired.</p>
<p>All these scenarios present different small challenges to solve in order to
accomplish the desired high-level goal. Humans switch contexts in a really
flexible and natural way and constantly take care of the multitude of these
small problems that are faced to complete the desired objective. For example,
a general task can be divided into subtasks, which can then be further divided
into smaller ones <span class="citation"
data-cites="kroemer2021review">(Kroemer, Niekum, e Konidaris 2021)</span>. The
strategy of breaking down intricate objectives into smaller, manageable,
simpler activities is the most widely used heuristic to solve problems <span
class="citation" data-cites="egidi2006decomposition">(Egidi 2006)</span>.</p>
<p>Many of these sub-challenges require an interaction with one or more
objects. For example, the action of opening involves a door to pass through it
while moving or to access the fridge for cooking. The reaching action can
imply an object like a pen in the office to write or a glass of water to
drink. To push as an action often implies a button to enable a device in the
workplace, or to turn on a car to commute, or the stove to heat a meal.
Objects have undoubtedly strong importance in the small actions performed to
achieve a goal, and their affordance is still the object of research in humans
<span class="citation" data-cites="maranesi2014cortical">(Maranesi, Bonini, e
Fogassi 2014)</span>, <span class="citation"
data-cites="osiurak2017affordance">(Osiurak, Rossetti, e Badets 2017)</span>
and machines <span class="citation"
data-cites="horton2012affordances">(Horton, Chakraborty, e Amant
2012)</span>.</p>
<p>As seen, many different movements and sub-actions, often involving objects,
are executed in daily life. Furthermore, they are also adapted to accomplish
the current desired goals. The adaptation can involve a simple difference of
position with respect to the previous location, both of the object or the
executor, or can involve a completely different context to which the action
learned is transferred. These skills are learned and discovered at the
beginning, and then the knowledge of the action is abstracted and adapted to
different purposes.</p>
<p>Moreover, a person builds sequences of actions naturally to achieve the
objective and, as discussed, adapts them to the environment. The skills are
often combined together one after the other, based on the scenario but also
based on the result and position of the previous execution. Occasionally, it
can happen that part of an action is used and part of another action, mixing
previously learned movements if the situation requires it. This results in the
creation of new combinations and compositions of previously known
activities.</p>
<p>Lastly, dissecting complex challenges requires also decision-making under
uncertainty, which is essential for achieving high-level goals since the
sequence of activities is not always clear in advance. Often, the goal changes
mid-way in response to the environment, or the initial assessment is
sub-optimal or incorrect, forcing a change in planning and a new decision on
what subsequent action to take. So it&#x2019;s worth noting that online decision
under dynamic circumstances and change of skill executed allows a person to
navigate the complexities of daily scenarios with success.</p>
<p>The human mind&#x2019;s capacity for abstraction, planning, and execution is still
a remote objective for robotics <span class="citation"
data-cites="konidaris2019necessity">(Konidaris 2019)</span>. This level of
adaptation to the environment and building of compounded behaviors is still a
hard challenge to solve nowadays.</p>
<p>For this reason, robots currently are not pervasive in society like other
technologies. Humanoid robots have little if no presence and, despite the
potential different uses, are relegated to mainly interaction and exhibition
duties. The majority of robots work in a controlled environment, like
factories, where the surroundings are specifically designed for them. The
actions taken are repetitive, fixed, and in contact with a simple, defined set
of objects.</p>
<p>Furthermore, even if some robots are able to integrate into semi-structured
environments (for example, the robotic vacuum cleaners for homes or lawnmowers
for gardens), they are specialized to a single task in a single scenario.
Multi-purpose robots require a more human-compatible design and a higher
degree of intelligent behavior <span class="citation"
data-cites="dechant2021toward">(DeChant e Bauer 2021)</span>, but versatile
humanoid robots are still not pervasive in the current status of society.</p>
<p>In this study, we propose a computational model that is biologically
inspired. Our approach consists in the use of mathematics and artificial
intelligence to emulate human abstraction and adaptation capabilities in the
execution of a series of primitive actions. We want to prove how demonstrating
basic movements to a robot and composing them together with flexibility may
lead to achieving complex tasks of various natures. Specifically, movement
primitives are reused and combined differently for different goals, avoiding
explicit teaching of multiple objectives. The trajectories for the skills
learned are adapted to the environment and partially composed thanks to the
interpolation abilities of Conditional Neural Movement Primitives (CNMP)
networks <span class="citation" data-cites="Ugur-RSS-19">(M. Y. S. A. M. I. A.
J. P. A. E. Ugur 2019)</span>. Lastly, the approach has been implemented and
tested on an anthropomorphic robot and on an industrial collaborative
robot.</p>
</section>
<section id="foochallenges" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span>
Challenges</h2>
<p>Robotics dominates many fields, but as discussed, often the environment is
controlled, designed to help the robot in its task, and not human-friendly. If
the purpose is to integrate robots into the human environment, robots must
adapt to humans, not vice-versa. All environments in which humans are present
are not organized or predictable, and this means one issue is that robots have
to accommodate for these conditions. A challenge is definitely to introduce
the machine to an unstructured environment, and this implies many
sub-issues.</p>
<p>Having surroundings that might change forces the machines to have a great
amount of perception. The system has to be extremely aware of the objects and
people around it to operate in a safe and meaningful way. This translates into
equipping many sensors and using real-time data from all available sources.
Moreover, the machine cannot rely on these detection instruments mounted on
the external world since a humanoid robot is expected to be mobile. Having a
multi-purpose system that can act in different scenarios implies, indeed, a
self-contained arrangement of sensors.</p>
<p>The perception brings, in cascade, the necessity of storing this
information and creating an internal copy of the surroundings that works as a
base for planning and future predictions. Creating a digital twin for the
environment is not essential for all the actions since some of them can be
executed in real-time, but it is required to plan their effects and combine
results together. For example, if a sponge is needed to clean a table, it
would be faster to have the knowledge of its last position, but it can also be
researched on demand and used while observing the effects in real-time till
the table is clean. On the other hand, complex actions that combine multiple
primitives need a future prediction of their effects on the environment, so
its internal representation is required.</p>
<p>With changing surroundings, it is possible also that the expected position
of objects is no longer consistent with the representation. This forces the
system to find an alternative or explore the environment till the object is
found. Other kinds of exploration possible are the exploration of the action
space to infer new actions and results, or the exploration of objects&#x2019;
capabilities to learn new affordances and usages. <span class="citation"
data-cites="Ahmetoglu_2022">(Ahmetoglu et al. 2022)</span></p>
<p>Another factor worth taking into consideration is the subject of planning.
Plans have to be structured in a meaningful way otherwise an incorrect
sequence won&#x2019;t just produce an incorrect result but might bring the system
further away from the final goal. The combinations of actions generated
usually have importance in the order of execution, so the product of the
skills has to be considered carefully.</p>
<p>Furthermore, objects and tools are usually designed for humans, so their
capabilities might vary depending on the machine used and might influence the
actions in the planning phase. Giving meanings to the objects, both in terms
of affordances and representations, is still a tricky challenge in robotics
<span class="citation" data-cites="7523298">(Jamone et al. 2018)</span> and
partially involves the previously investigated challenges of planning and
exploration.</p>
<p>Also, obstacle avoidance, whenever there is an object in the trajectory of
movement, is a factor to take into consideration. The robot is required to be
aware of the surroundings and itself, not to collide, hurt, damage them, or
just fail the designated goal. Humans adapt previously known actions whenever
an obstacle or an impediment is present.</p>
<p>Part of the adaptation challenge is also being able to transfer the skills
known to new locations and scenarios. For example, learning how to turn a key
for the door and use the action for the key of the car or the knob to turn on
the stove. This is an essential capability that is difficult to implement in a
machine.</p>
<p>Another more hidden challenge is how the actions are merged among them.
Usually, humans, when they pass from one action to another, apply a smooth
transition. This means that the movements don&#x2019;t have to fully start and end as
they are learned, or the result will be artificial and sub-optimal.</p>
<p>Furthermore, object handling, grasping, and manipulation present some
issues that are the object of research. How to pick the item desired, where,
with which grasp, and with which force intensity are issues that can undermine
the final result.</p>
<p>Lastly, another challenge that will be encountered is the recognition when
the action is completed. Being aware of the right final state is essential for
successfully matching the expectations for the goal requested.</p>
<p>These challenges discussed are crucial aspects to consider, but not all of
them will be addressed in this project, and some will also be simplified.
Nevertheless, it&#x2019;s worth noting the scope and limitations of this work and the
boundaries within which the research operates.</p>
</section>
<section id="fooobjectives" class="level2" data-number="1.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span>
Objectives</h2>
<p>The aim of this research is to investigate novel skill generation by
combining previously taught ones with the use of CNMPs <span class="citation"
data-cites="Ugur-RSS-19">(M. Y. S. A. M. I. A. J. P. A. E. Ugur 2019)</span>.
The research aims to be applied to robotics scenarios involving trajectories
for object manipulation and high-level goal achievement. The generation of new
combinations of skills will be performed by connecting skill segments that the
robot learned by demonstration. The amount of demonstrations given should be
reasonable for the system to be applied in real life by a human. The
combination of actions will be investigated in both the concatenation of
trajectories end-to-end and the use of parts of them. The ultimate goal is to
create a system that allows a robot, given some demonstrations, to reuse the
skills acquired to complete different objectives whose trajectories were never
taught explicitly. Furthermore, the adaptation should be acceptable in
different configurations of the environment and, ideally, in different
scenarios.</p>
</section>
<section id="foothesis-structure" class="level2" data-number="1.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Thesis
Structure</h2>
<p>Accordingly, the remainder of this thesis is structured as follows.</p>
<p>discusses the background of the topic, the current advancements in the
field, and the related research with a literature review.</p>
<p>In <a href="#foochap:platforms" data-reference-type="ref"
data-reference="chap:platforms">3</a> the instruments and frameworks used in
this research are listed and analyzed to be able to understand the initial
setup and replicate the results.</p>
<p>The <a href="#foochap:design" data-reference-type="ref"
data-reference="chap:design">4</a> explains the design and architecture of the
proposed method. In order to understand the logic, the conceptual passages and
mathematical background.</p>
<p>The <a href="#foochap:implementation" data-reference-type="ref"
data-reference="chap:implementation">5</a> analyzes the key points of the
implemented solution through the explanation of the most important passages in
the code developed.</p>
<p>The <a href="#foochap:validation" data-reference-type="ref"
data-reference="chap:validation">6</a> shows the final results and the testing
on real-life robotic platforms.</p>
<p>Finally, concludes this thesis by summarising its main contribution and
future work.</p>
</section>
</section>
<section id="foochap:background" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Related
Work</h1>
<p>In this chapter, we will proceed with the literature analysis and
state-of-the-art methods related to the topic. Furthermore, some basic notions
regarding the previous research relied upon will be explained.</p>
<p>Trajectory generation for robotics is a topic closely related to
manipulation and navigation. Every movement can be described as a trajectory,
defined as the composition of the path taken by an agent (or its joints) in
time <span class="citation" data-cites="biagiotti2008trajectory">(Biagiotti e
Melchiorri 2008)</span>. In this discussion, we will focus more on the
manipulation part, being the most clear example of how combining different
trajectories can produce novel robotic skills. Manipulation is one of the most
distinctive capabilities of robots, since their main objective is to perform
physical tasks in the real world. Any process that presents the necessity of a
human to engage with a physical object meaningfully can solely be automated by
robot manipulation. <span class="citation" data-cites="rosen2022role">(Rosen
et al. 2022)</span></p>
<p>Creating robots capable of directly interacting with the world around them
is still a key challenge in robotics, and manipulation is central to this.
<span class="citation" data-cites="kroemer2021review">(Kroemer, Niekum, e
Konidaris 2021)</span> Nevertheless, the ability to solve high-level goals in
robots is increasing <span class="citation" data-cites="gupta2019relay">(Gupta
et al. 2019)</span>, <span class="citation"
data-cites="simeonov2021long">(Simeonov et al. 2021)</span> thanks to the
recent advances in artificial intelligence. Some approaches may follow natural
language instructions to achieve complex sequences of actions <span
class="citation" data-cites="hu2019hierarchical">(Hu et al. 2019)</span>, but
according to the research objective, a certain degree of autonomy is desired.
This implies typically giving only the final goal and not the step-by-step
instructions.</p>
<p>Recently, a lot of research has invested in deep reinforcement learning to
map sensor inputs of a robot directly to motor torques <span class="citation"
data-cites="khazatsky2021can">(Khazatsky et al. 2021)</span>. These approaches
provide independence, due to not hard-coded behaviors, and versatility by
leveraging recent advancements in training deep networks. However, they
encounter difficulties because of the demand for a huge amount of samples and
underlying complexity. Reinforcement learning embraces the full complexity of
these problems by requiring both interactive, sequential prediction as in
"learning from demonstration" (LfD) and complex reward structures with only
<q>bandit</q> style feedback on the actions chosen <span class="citation"
data-cites="kober2013reinforcement">(Kober, Bagnell, e Peters 2013)</span>.
For this reason, recent research aims to minimize what must be learned and to
support sequential composition <span class="citation"
data-cites="rosen2022role">(Rosen et al. 2022)</span>. However, the collection
of trajectory samples in the real world requires too much time, so a framework
for robotic simulation is often used to simplify the setting.</p>
<p>A commonly used technique in robotics is Learning from Demonstration (LfD)
<span class="citation" data-cites="ARGALL2009469">(Argall et al. 2009)</span>
<span class="citation" data-cites="schaal1999imitation">(Schaal 1999)</span>,
and then <span class="citation"
data-cites="ravichandar2020recent">(Ravichandar et al. 2020)</span>. It allows
for solving a wide variety of robotics problems by imitating an external
agent. The demonstrator, often a human or another system, provides examples
(expert demonstrations) of how to perform a task, and the learning agent
generalizes from these demonstrations to acquire the ability to perform the
task later independently.</p>
<p>Famous learning from demonstration research includes statistical modeling
<span class="citation" data-cites="calinon2016tutorial">(Calinon 2016)</span>,
dynamic systems <span class="citation" data-cites="schaal2006dynamic">(Schaal
2006)</span>, and their union in <span class="citation"
data-cites="ugur2020compliant">(E. Ugur e Girgin 2020)</span>. In Dynamic
Movement Primitives (DMPs) <span class="citation"
data-cites="schaal2006dynamic">(Schaal 2006)</span> <span class="citation"
data-cites="ijspeert2002learning">(Ijspeert, Nakanishi, e Schaal 2002)</span>,
a trajectory is represented with a set of differential equations and learned
with as little as one shot. DMPs demonstrated a good capability in learning a
remarkable variety of dynamic behaviors <span class="citation"
data-cites="mulling2013learning">(M&#xFC;lling et al. 2013)</span>. Thanks to the
"point attractor" mechanism, it guarantees reaching a point even under
perturbations. DMPs have successfully been utilized in difficult manipulation
tasks such as in-hand manipulation and flipping boxes using chopsticks <span
class="citation" data-cites="pastor2009learning">(Pastor et al. 2009)</span>.
On the other hand, additional tuning is needed to determine the number of
basis functions. Moreover, the motor learning problems that are most
intriguing often involve a high number of dimensions <span class="citation"
data-cites="kober2008policy">(Kober e Peters 2008)</span>, and DMPs still
struggle to be integrated with high-dimensional, multi-modal data <span
class="citation" data-cites="saveriano2021dynamic">(Saveriano et al.
2021)</span>. Finally, their approach is not designed to learn from multiple
trajectories and, therefore, cannot encode the important parts of multiple
demonstrations <span class="citation" data-cites="Ugur-RSS-19">(M. Y. S. A. M.
I. A. J. P. A. E. Ugur 2019)</span>.</p>
<p>In the Probabilistic Movement Primitives (ProMP) <span class="citation"
data-cites="paraschos2013probabilistic">(Paraschos et al. 2013)</span>,
instead, the distribution of basis functions is often represented using a
probabilistic framework, typically a Gaussian Mixture Model (GMM) or a similar
probabilistic model. Probabilistic models allow to capture the variability
across demonstrations and different Degrees of Freedom and, furthermore,
enable variable conditioning in which predictions can be refined through new
observations.</p>
<figure id="foofig:prompcnmp">
<img src="Images/ProMPvsCNMP.png" />
<figcaption>In ProMPs the distribution of basis functions is often represented
using a probabilistic framework, but can lead to some failures.</figcaption>
</figure>
<p>Historically, Gaussian Mixture Models <span class="citation"
data-cites="nguyen2009model">(Nguyen-Tuong, Seeger, e Peters 2009)</span> have
been prominent among various probabilistic approaches since they provide
adaptable solutions to the challenge of modeling trajectories. On the other
hand, GMMs involve estimating many parameters, especially when dealing with
high-dimensional data or a large number of components. This can make training
and inference computationally expensive, particularly when the dataset is
extensive, and if not done correctly, can lead to some failures, Figure <a
href="#foofig:prompcnmp" data-reference-type="ref"
data-reference="fig:prompcnmp">2.1</a>.</p>
<p>Another probabilistic model, like GMMs, often used in statistical modeling
techniques is Hidden Markov Models (HMMs) <span class="citation"
data-cites="lee2011incremental">(Lee e Ott 2011)</span>. HMMs were
successfully applied to learn multi-modal models from temperature, pressure,
and fingertip information for exploratory object classification tasks <span
class="citation" data-cites="chu2013using">(Chu et al. 2013)</span>.</p>
<p>A recent model developed in robotics called Conditional Neural Movement
Primitives (CNMP) <span class="citation" data-cites="Ugur-RSS-19">(M. Y. S. A.
M. I. A. J. P. A. E. Ugur 2019)</span> also learns from demonstrations, but
instead of using GMMs, they use neural networks to model the mapping from
conditions to trajectories directly. Neural networks allow the model to scale
better and to offer robust data approximation via gradient descent. Until
recently, neural networks in deep learning were trained to approximate a
single-output function. However, when data is a distribution, the single
function cannot approximate the underlying model. So the network can be
modeled as a probabilistic approximator, that can predict the distribution
parameters, mean, and variance. This makes CNMPs well-suited for tasks with
complex, high-dimensional state spaces. It allows one to learn skills in tens,
rather than thousands, of real-world interactions and interpolate among
them.</p>
<p>Based on the above-mentioned observations, the proposal is as follows. In
the following research, the ability of CNMPs to interpolate the trajectories
demonstrated is exploited to synthesize new complex skills. The model is based
on Gaussian Processes (GP) <span class="citation"
data-cites="seeger2004gaussian">(Seeger 2004)</span>, Neural Processes (NPs)
<span class="citation" data-cites="garnelo2018neural">(Garnelo, Schwarz, et
al. 2018)</span>, and Conditional Neural Processes (CNPs) <span
class="citation" data-cites="DBLP:journals/corr/abs-1807-01613">(Garnelo,
Rosenbaum, et al. 2018)</span>. For context, an explanation in detail of these
above-mentioned methods will follow.</p>
<section id="foogaussian-processes" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Gaussian
Processes</h2>
<p>Gaussian Processes <span class="citation"
data-cites="seeger2004gaussian">(Seeger 2004)</span> are probabilistic models
that define a distribution over functions, Figure <a href="#foofig:gp"
data-reference-type="ref" data-reference="fig:gp">2.2</a>. This means that
they leverage pre-existing knowledge about a set of functions and infer during
test-time specific functions that fit the data provided. Given a set of
observed points, there are infinite possible functions that pass through
them.</p>
<p>Gaussian processes provide an elegant solution to this challenge by
assigning a probability to each of these potential functions. The mean of this
probability distribution then represents the most probable characterization of
the data given the observation points <span class="citation"
data-cites="Goertler2018VisualExplorationGaussian">(J. G&#xF6;rtler
2018)</span>.</p>
<p>This is called regression and is used, for example, in robotics or time
series forecasting. Gaussian processes are not limited to regression, and they
can also be extended to classification and clustering tasks <span
class="citation" data-cites="kapoor2010gaussian">(Kapoor et al. 2010)</span>
<span class="citation" data-cites="kim2007clustering">(Kim e Lee
2007)</span>.</p>
<figure id="foofig:gp">
<img src="Images/GP.png" />
<figcaption>Gaussian Processes are probabilistic models that define a
distribution over functions.</figcaption>
</figure>
<p>Many supervised learning problems can be seen as function approximations
since a dataset of observations <span class="math inline">\(\{x_i,
y_i\}^{n-1}_{i=0}\)</span> is basically a number <span
class="math inline">\(n\)</span> of evaluations <span
class="math inline">\(y_i = f(x_i)\)</span> of an unknown function <span
class="math inline">\(f\)</span>. A supervised learning algorithm returns an
approximated function <span class="math inline">\(g\)</span>. The goal is to
minimize the loss between the real function <span
class="math inline">\(f\)</span> and the predicted one <span
class="math inline">\(g\)</span>. The evaluation is carried out on unlabelled
data points <span class="math inline">\(x_j\)</span>.</p>
<p>On the other hand, the disadvantages of Gaussian Processes are prior
selection and training time for large datasets. Scaling issues with GPs have
been addressed in <span class="citation"
data-cites="snelson2006sparse">(Snelson e Ghahramani 2006)</span>. The limited
expressivity from functional restriction was addressed with DeepGPs in <span
class="citation" data-cites="damianou2013deep">(Damianou e Lawrence
2013)</span> <span class="citation"
data-cites="salimbeni2017doubly">(Salimbeni e Deisenroth 2017)</span>.</p>
<p>Overcoming these issues and attempting to combine Deep Learning (DL) with
GPs was proposed in <span class="citation" data-cites="wilson2016deep">(Wilson
et al. 2016)</span>, but the approach remains close to GPs since the network
is used to learn more expressive kernels to use with GPs.</p>
</section>
<section id="fooconditional-neural-processes" class="level2"
data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span>
Conditional Neural Processes</h2>
<p>In <span class="citation"
data-cites="DBLP:journals/corr/abs-1807-01613">(Garnelo, Rosenbaum, et al.
2018)</span>, the authors propose a novel research in which the inference
potential of Gaussian Processes and the performance of neural networks are
blended together. Neural networks are extensively employed as approximators of
functions and have demonstrated considerable efficacy but often require large
datasets for training. In CNPs, the prior knowledge is directly derived from
the data, allowing them to infer the underlying function distribution based on
observations. CNPs are built with three main blocks: Encoder, Aggregator, and
Decoder. Encoder <span class="math inline">\(E\)</span> and Decoder <span
class="math inline">\(Q\)</span> are typically Multi-layer perceptrons. The
model structure is shown in Figure <a href="#foofig:cnp"
data-reference-type="ref" data-reference="fig:cnp">2.3</a>.</p>
<figure id="foofig:cnp">
<img src="Images/CNP.png" />
<figcaption>The structure of a CNP network, with three main blocks: Encoder,
Aggregator, and Decoder.</figcaption>
</figure>
<p>The model scales with complexity <span
class="math inline">\(O(n+m)\)</span> for making <span
class="math inline">\(m\)</span> predictions from <span
class="math inline">\(n\)</span> observations, while GPs scale with <span
class="math inline">\(O(n+m)^3\)</span>. CNPs don&#x2019;t require the specification
of a kernel cause they learn it from the data provided in training. The
tradeoff is that the representations of the observations have fixed
dimensionality.</p>
<p>The work is based on the previous research of Neural Processes (NPs) <span
class="citation" data-cites="garnelo2018neural">(Garnelo, Schwarz, et al.
2018)</span>. NPs are suggested as a means to manage the substantial
computational demands of GPs while leveraging their flexibility and efficiency
with data. NPs help create different predictions by learning a shared hidden
representation. However, they have trouble with long sequences because they
automatically pick certain points. Building on NPs, Conditional Neural
Processes (CNPs) are strong models that make training more efficient by
allowing explicit conditioning.</p>
<figure id="foofig:cnp_data">
<img src="Images/CNPdata.png" />
<figcaption>CNP allows precise predictions for targets sampled from a
distribution conditioned with observations</figcaption>
</figure>
<p>The approach allows precise predictions for targets sampled from a
distribution conditioned with observations, Figure <a href="#foofig:cnp_data"
data-reference-type="ref" data-reference="fig:cnp_data">2.4</a>. Given a
varying number of observations <span class="math inline">\(O\)</span>, a
neural network <span class="math inline">\(E\)</span> is utilized as an
encoder to generate a fixed-size representation <span
class="math inline">\(r_i\)</span>.</p>
<p>Observations fed to the network don&#x2019;t have an order, following the
stochastic processes, because subsequently, they are aggregated with the
average operation to obtain a single representation <span
class="math inline">\(r\)</span>. Any commutative operation is valid and
usable. The resulting representation <span class="math inline">\(r\)</span>
contains the conditioning information and is fed to a decoder network <span
class="math inline">\(Q\)</span> along with the desired target <span
class="math inline">\(x_j\)</span> to query. The decoder network <span
class="math inline">\(Q\)</span> has parameters <span
class="math inline">\(\theta\)</span>. For all the targets <span
class="math inline">\(x_j \in T\)</span> the decoder outputs the mean and
standard deviation.</p>
<p>The formulation of the encoding of each observation is: <span
class="math display">\[r_i = E_\phi(x_i,y_i), \quad \forall(x_i,y_i) \in
O\]</span> and the following commutative operation between the encodings to
create a single one: <span
class="math display">\[\label{eq:commutative-operation}
    r = r_1 \oplus r_2 \oplus ... \oplus r_i,\]</span> The commutative
operation expressed by <span class="math inline">\(\oplus\)</span>, can be
summation, average, product, and so on.</p>
<p>The vector generated is concatenated with the target variables. The merged
representation is passed to the decoder to obtain the output as: <span
class="math display">\[\phi_j = Q_\theta(x_j, r), \quad \forall x_j \in
T\]</span> Where the output is: <span class="math display">\[\phi_j = (\mu_j,
\sigma_j^2)\]</span> which are the mean and the standard deviation of the
output variable.</p>
<p>In summary, the CNP model, with averaging operation, can be formulated as:
<span class="math display">\[\mu_j, \sigma_j^2 = Q_\theta \left( x_j \oplus
\frac{ \sum_{i}^{n} E\phi((x_i,y_i)) }{n}  \right)\]</span></p>
</section>
<section id="fooconditional-neural-movement-primitives" class="level2"
data-number="2.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span>
Conditional Neural Movement Primitives</h2>
<p>Finally, in <span class="citation" data-cites="Ugur-RSS-19">(M. Y. S. A. M.
I. A. J. P. A. E. Ugur 2019)</span>, Conditional Neural Movement Primitives
are proposed as a model. CNMPs, as the name suggests, are an extension of CNPs
and are particularly well suited for the robotics domain. The model
illustration can be seen in Figure <a href="#foofig:cnmp"
data-reference-type="ref" data-reference="fig:cnmp">2.5</a>.</p>
<figure id="foofig:cnmp">
<img src="Images/CNMP.png" />
<figcaption>CNMP model, it is conceived to work with temporal relations
between sensorimotor data and different task parameters</figcaption>
</figure>
<p>The "learning from demonstration" framework can learn non-linear
relationships between trajectories and reproduce them in joint or task space.
In this case, a trajectory is formally defined as a temporal function, <span
class="math inline">\(\tau = \tau (t)\)</span>, where the sensorimotor data in
time describes how a robot moves. So, each trajectory <span
class="math inline">\(\tau\)</span> is a list of ordered sensorimotor values:
<span class="math display">\[\tau = \{ SM(t_1), SM(t_2), ... , SM(t_T )
\}\]</span> where <span class="math inline">\(SM(t_i)\)</span> is the
sensorimotor data at an instant of time <span
class="math inline">\(t_i\)</span>. So, the challenge of trajectory generation
becomes figuring out a series of commands <span
class="math inline">\(SM(t_i)\)</span> that creates the movement desired <span
class="citation" data-cites="gasparetto2007new">(Gasparetto e Zanotto
2007)</span>. Finally, with a set of observations <span
class="math inline">\(O\)</span>, the model has to learn the function <span
class="math inline">\(\tau = f(t|O)\)</span>, using <span
class="math inline">\(N\)</span> expert demonstrations, <span
class="math inline">\(D = \{\tau_1, \tau_2, ... , \tau_N \}\)</span>.</p>
<p>CNMPs are conceived to work with temporal relations <span
class="math inline">\(t\)</span> and different task parameters <span
class="math inline">\(\gamma\)</span>. CNMPs maintain the permutation
invariance of CNPs over observations <span class="math inline">\(O\)</span>
and queries <span class="math inline">\(T\)</span>. Furthermore, to make the
model time-invariant, the sensorimotor trajectories are often scaled in the
interval [0,1]. The task parameter <span class="math inline">\(\gamma\)</span>
effectively adds one or more dimensionalities to the network&#x2019;s input, and it&#x2019;s
passed to both the encoder and the decoder. An observation becomes the
concatenation of <span class="math inline">\(SM(t_i)\)</span>, <span
class="math inline">\(t_i\)</span>, and <span
class="math inline">\(\gamma\)</span>. The dimensionality of <span
class="math inline">\(SM(t_i)\)</span> depends on factors like the Degrees of
Freedom of the robot joints and the number of variables corresponding to the
actuators. For the aggregation of the representations, the averaging operation
has been chosen. During training, a random trajectory <span
class="math inline">\(\tau_i\)</span> is selected from the <span
class="math inline">\(D\)</span> set of expert demonstrations. Next, a random
number <span class="math inline">\(n\)</span> of random observation points are
selected from the trajectory <span class="math inline">\(\tau_i\)</span>. The
encoder takes the <span class="math inline">\(n\)</span> observations and
produces <span class="math inline">\(n\)</span> representations. The final
representation is obtained by averaging the representations produced by the
encoder fed with all the observation points. The target data is predicted
using the representation and the query time <span
class="math inline">\(t\)</span> concatenated to the task parameter <span
class="math inline">\(\gamma\)</span></p>
<p>The encoder and the decoder are trained jointly with the error calculated
from the following loss function: <span class="math display">\[L(\theta,
\phi)= -logP(y_i|\mu_i, softmax(\sigma_j))\]</span> using both mean and
standard deviation produced by the network. As a note, the uncertainty of the
prediction provided by the variance is useful for the model&#x2019;s active
exploration to choose wisely where the next observations are needed. Moreover,
the capacity of CNMPs to deal with high-dimensionality input can also be used
to input images in the model. Image completion indeed can be seen as a
regression task. Leveraging the interpolation capabilities of CNMPs, our
approach will investigate novel synthesis by combining and concatenating
previously taught ones.</p>
</section>
</section>
<section id="foochap:platforms" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span>
Platforms</h1>
<p>In this chapter, all the physical and digital platforms utilized will be
explained to give a proper understanding of the initial architecture and a
more comprehensive idea of the environment of the experiments. The first part
will state the devices and their setup, capabilities, and configuration used.
Subsequently, the frameworks and libraries employed will be listed and
described.</p>
<section id="foobaxter-robot" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Baxter
Robot</h2>
<p>The Baxter robot is an industrial robot built by Rethink Robotics in 2011
<span class="citation" data-cites="wiki:baxter">(Wikipedia contributors
2023)</span>. The platform [Fig. <a href="#foofig:baxter"
data-reference-type="ref" data-reference="fig:baxter">3.1</a>] has two robotic
arms with interchangeable grippers at the wrist (End Effectors). The robot is
180cm tall and, with its pedestal, weighs 140 kg. The arms have 7 degrees of
freedom (DOF), which implies they have seven joints each. This makes it
kinematic redundant, meaning that for some points reached in space, multiple
pose configurations of the arms are possible. These two factors combined allow
the robot to have an impressive capability in manipulation. The robot can be
equipped with suction caps or two-jaw parallel grippers; we chose the last
ones in our configuration. The gripper, in addition to its position, also
offers information regarding the force applied while grasping an object.</p>
<p>The robot was designed with attention to collaborative tasks with humans.
For this reason, it eases teaching by demonstration by having integrated two
touch sensors in the wrists that unlock the motors of the arms, allowing the
user to move them easily and record the trajectories executed. The robot helps
the movement with a feature called "Zero-g mode", in which the weight of the
joints is neutralized actively by the motors. This enables the teaching expert
to demonstrate movements in a similar environment without gravity, without
having to carry the instrumentation load in time constantly. Furthermore, the
robot has many input buttons and LEDs, they are present on the hands, arms,
and chest, and they allow the programming of custom behaviors. They are
especially useful in retrieving and giving instructions to the robot without
reaching a computer, like closing the gripper at the desired moment, getting
feedback, or starting the trajectory recordings.</p>
<p>Another feature worth mentioning is the increased safety of operating
around humans. Thanks to active and passive safety systems equipped in the
platform, it doesn&#x2019;t require a cage for protection. On the other hand, making
the robot less hazardous comes with the cost of precision. A motor driving a
spring that drives Baxter&#x2019;s arm instead of just a direct motor impacts the
precision of movements, sometimes in terms of centimeters. This doesn&#x2019;t make
the robot perfectly suitable for industrial applications, but especially
appropriate for research and for the adaptability in our project.</p>
<p>The head of the robot includes a ring of sonar sensors for people
detection, a wide-angle camera, and a movable display that acts like a face.
Another benefit of the robot lies at the end of both hands. Immediately next
to the attachment for the tools, an infrared (IR) sensor provides data on the
distance from a solid object (i.e., a table) and an inertial measurement unit
(IMU). Moreover, an embedded RGB camera is also present, allowing to see
closely the object approached or to change the point of view on it without
additional external cameras.</p>
<figure id="foofig:baxter">
<img src="Images/baxter.png" />
<figcaption>Baxter Robot platform</figcaption>
</figure>
</section>
<section id="foour10-robot" class="level2" data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> UR10
Robot</h2>
<p>UR10 is a single industrial robot arm that shines in reliability and
precision [Fig. <a href="#foofig:ur10" data-reference-type="ref"
data-reference="fig:ur10">3.2</a>]. It has been manufactured by Universal
Robots and combines long reach with a high payload <span class="citation"
data-cites="url:ur10">(Universal Robots 2023)</span>. It is intended for
medium-duty tasks, so it&#x2019;s compact in its overall dimensions compared to a
fully intended industrial robot. It can reach an impressive height of 2.3m on
its pedestal. In our experiments, it was mounted on a pedestal 0.85m tall to
increase the reachability at table level.</p>
<p>The arm has a reaching radius of 1,3m from the mounting point, which
implies a workspace of approximately 5,3 square meters at the base level. The
robot has 6 Degrees of Freedom, with six rotating joints. It is able to reach
any point in its reaching radius but has no kinematics redundancy, meaning
only one position is possible for any given point. The total payload that can
be carried is 10 kg.</p>
<figure id="foofig:ur10">
<img src="Images/ur10.png" />
<figcaption>UR10 Robot platform</figcaption>
</figure>
<p>Like the previous robot, this one is designed to work collaboratively with
humans. It features built-in safety features, such as force/torque sensors, to
detect and respond to external forces or unexpected events. A button to
release the motor breaks is present on the floating touch screen and allows
the robot&#x2019;s motion by hand. This robot also doesn&#x2019;t require a safety cage
around for protection, but the emergency stop button always has to be within
easy reach.</p>
<p>The robot design emphasizes modularity, making it easier for users to
customize and adapt it for different tasks or use various end effectors. We
coupled it with a three-finger gripper described in the next paragraph.</p>
</section>
<section id="foof-robotiq-gripper" class="level2" data-number="3.3">
<h2 data-number="3.3"><span class="header-section-number">3.3</span> 3F
Robotiq Gripper</h2>
<p>At the end of the UR10 Robotic Arm, a Robotiq 3-Finger Adaptive Robot
Gripper was mounted [Fig. <a href="#foofig:gripper3f"
data-reference-type="ref" data-reference="fig:gripper3f">3.3</a>]. The gripper
has a human-inspired design and has three fingers with three joints each <span
class="citation" data-cites="url:3fgripper">(Robotiq 2023a)</span>. The
physical platform was chosen for its precision and safety, and it pairs well
with the UR10 capabilities. The gripper offers different grip modes; the ones
available are: "basic", "wide", "scissor" and "pinch". Each is appropriate for
distinctive objects to grip; the basic one is the most versatile, but the wide
one has more stability for big or long objects, and the pinch one is the best
for small objects. The "scissor mode" closes together the two fingers on the
same side, for high-precision manipulation. We mainly used the "pinch"
setting.</p>
<figure id="foofig:gripper3f">
<img src="Images/gripper3f.png" />
<figcaption>3F Robotiq Gripper platform</figcaption>
</figure>
<p>The gripper has a mass of 2.3 kg in contrast with a grip payload of 10 kg.
The grip force applied can range from 30 to 70 N, depending on the grip mode
selected. The precision declared is up to 0.05 mm.</p>
<p>This platform was designed as well for collaborative robotic applications,
allowing it to work safely alongside human operators. It incorporates safety
features to detect and respond to external forces, stopping in case of high
forces applied. The torque and speed of gripping data are available and
exposed through a dedicated ROS topic. Speed and torque are also adjustable
for the intended use. It is possible to control and retrieve data for each
finger individually.</p>
</section>
<section id="fooft-300-s-force-torque-sensor" class="level2"
data-number="3.4">
<h2 data-number="3.4"><span class="header-section-number">3.4</span> FT 300-S
Force Torque Sensor</h2>
<p>Between the UR10 robot and the 3F Finger Gripper, an FT 300-S Force Torque
Sensor [Fig. <a href="#foofig:ft-sensor" data-reference-type="ref"
data-reference="fig:ft-sensor">3.4</a>] was mounted to increase precision and
repeatability.</p>
<p>The sensor offers high-resolution real-time measurements regarding the
force and torque applied to the three space dimensions and improves the
capabilities of the robot. This device makes the UR10 able to detect the
payload carried or the amount of pressure between the object or gripper and
the static environment (i.e., the table).</p>
<p>The device was built for compatibility with the Universal Robot series and
has an IP65 rating. It also enables precise object placement such as
alignment, indexing, and insertion <span class="citation"
data-cites="url:ftsensor">(Robotiq 2023b)</span>.</p>
<p>It exposes the six readings of the forces and torques in the three axes
through a port opened in the computer connected to it. In out setup it has
been connected straight to the UR10 computer, allowing any user to retrieve
the necessary data.</p>
<p>The FT 300-S is commonly used in tasks where force and torque sensing are
critical, but we used it to increase the reliability of the payload
measurements and the safety of the operations.</p>
<figure id="foofig:ft-sensor">
<img src="Images/ft_sensor.png" />
<figcaption>FT 300-S Force Torque Sensor</figcaption>
</figure>
</section>
<section id="fooframeworks" class="level2" data-number="3.5">
<h2 data-number="3.5"><span class="header-section-number">3.5</span>
Frameworks</h2>
<section id="fooros" class="level5" data-number="3.5.0.0.1">
<h5 data-number="3.5.0.0.1"><span
class="header-section-number">3.5.0.0.1</span> ROS</h5>
<p>The Robot Operating System (ROS) <span class="citation"
data-cites="url:ros">(ROS 2023)</span> is a framework to standardize the
deployment of robot applications. The system is a set of software libraries
and tools that combine the state-of-the-art drivers for the most common robot
interfaces and contain the most used algorithms for robotics.</p>
<p>Since robotics programming is a complex challenge, the idea behind ROS is
to use the "divide et impera" approach and split it into multiple
sub-problems. This division requires a distributed strategy, and distribution
implies communication among parts. One of ROS&#x2019;s main objectives is to
standardize communication. For this reason, ROS acts like a middleware
framework, allowing the ease of the dialog between software and the robotic
hardware. It is widely used in research and industry, from mobile robots to
manipulators. In our research, we used ROS version 1.</p>
<p>It&#x2019;s platform-independent and open source, so it is possible to create a
custom robotics device compatible with it, and it&#x2019;s possible to develop
personalized libraries. Its modular architecture allows the creation or use of
a series of executable pieces of code called nodes, which run on a single or
even on many computers. The nodes communicate among themselves with messages
in the form of data structures previously defined. The distributed system
allows to decuple the computation of heavy tasks, like vision, 3D
reconstruction, and navigation, from the robot&#x2019;s hardware.</p>
<p>Communication occurs through "topics" and "services". Nodes running on any
computer ping the "master node" designated to retrieve all the possible topics
and services exposed from other nodes in the network. A node can be a
"publisher" or "subscriber" to a topic, sending messages to it or receiving
messages from it. Communication with topics is not blocking and it is
many-to-many: multiple nodes can publish, and simultaneously, multiple nodes
can subscribe to a topic. Services work in a similar way, but the
communication blocks the computation till the data requested is retrieved.
Their mechanism is analogous to Remote Procedure Calls (RPCs).</p>
<p>ROS is available with two commonly used programming languages: Python and
C++. We used the Python version with the "rospy" package in the experiments
with both robots.</p>
</section>
<section id="foopytorch" class="level5" data-number="3.5.0.0.2">
<h5 data-number="3.5.0.0.2"><span
class="header-section-number">3.5.0.0.2</span> Pytorch</h5>
<p>Pytorch is a Deep Learning framework <span class="citation"
data-cites="paszke2019pytorch">(Paszke et al. 2019)</span> that focuses on
speed and usability with an imperative and Pythonic programming style. The
Python library <span class="citation" data-cites="url:pytorch">(Pytorch
2023)</span> offers a wide variety of models and building blocks for
constructing neural networks. By design, it eases the debugging for the user
with a rich ecosystem of dedicated tools. It works on the CPU and on hardware
accelerators like GPUs. For this reason, PyTorch provides a multi-dimensional
array called a tensor, which is similar to NumPy arrays. Conversions among
both of them will be present in the code implementation.</p>
</section>
<section id="foojupyter-notebook" class="level5" data-number="3.5.0.0.3">
<h5 data-number="3.5.0.0.3"><span
class="header-section-number">3.5.0.0.3</span> Jupyter Notebook</h5>
<p>Jupyter Notebook is an open-source interactive web application <span
class="citation" data-cites="url:jupyter">(Jupyter Notebook 2023)</span>. It
supports multiple programming languages and offers a cell-based environment
where code and description/graphical results can be blended. Jupyter Notebook
integrates seamlessly with popular Python libraries, such as NumPy, Pandas,
Matplotlib, and scikit-learn; some of them will be described later. It was
used occasionally in our experiments to provide a fast and interactive coding
experience with Python. The notebooks can be easily shared, and the process is
clearly visualized. It was specifically useful in plotting multiple graphs
during the training stages of neural networks or debugging operations on
multi-dimensional arrays.</p>
</section>
<section id="fooanaconda-and-python-libraries" class="level5"
data-number="3.5.0.0.4">
<h5 data-number="3.5.0.0.4"><span
class="header-section-number">3.5.0.0.4</span> Anaconda and Python
Libraries</h5>
<p>Anaconda is an open-source software that contains open-source tools and
packages for data science, machine learning, and scientific computing <span
class="citation" data-cites="url:anaconda">(Anaconda 2023)</span>. It has been
used to track the packages utilized in the robotic platforms and in the
models&#x2019; development and training. The Conda package manager was the most used
tool to easily install, update, and manage various software packages and
dependencies. Some of the most important libraries are listed below.</p>
<section id="foomatplotlib" class="level6" data-number="3.5.0.0.4.1">
<h6 data-number="3.5.0.0.4.1"><span
class="header-section-number">3.5.0.0.4.1</span> MatplotLib</h6>
<p>Matplotlib is a comprehensive 2D plotting library for Python that generates
high-quality charts, plots, and visualizations. It has been widely used to
double-check the quality of the training or plot the trajectories recorded
with the robots.</p>
</section>
<section id="foonumpy" class="level6" data-number="3.5.0.0.4.2">
<h6 data-number="3.5.0.0.4.2"><span
class="header-section-number">3.5.0.0.4.2</span> Numpy</h6>
<p>NumPy is a package for numerical computing in Python. It supports large,
multi-dimensional arrays and matrices and a collection of mathematical
functions to operate on these arrays. It has been used for array slicing,
normalization, and smoothing data.</p>
</section>
</section>
</section>
</section>
<section id="foochap:design" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Design</h1>
<p>In the previous chapters, it has been presented the necessary knowledge
about the background research (<a href="#foochap:background"
data-reference-type="ref" data-reference="chap:background">2</a>) and the
platforms utilized in this work (<a href="#foochap:platforms"
data-reference-type="ref" data-reference="chap:platforms">3</a>). This chapter
discusses the conducted research from the design perspective, giving the main
points and emphasizing the key ideas without going into the implementation
details as the next chapter (<a href="#foochap:implementation"
data-reference-type="ref" data-reference="chap:implementation">5</a>).</p>
<p>As analyzed in the Intro (<a href="#foochap:introduction"
data-reference-type="ref" data-reference="chap:introduction">1</a>), humans
have remarkable cognitive flexibility and use it to achieve complex goals
across various daily scenarios. These goals are often broken down into smaller
sub-tasks (skills), and this is one objective of this research. This
combination of trajectories also requires an appropriate shift among them, in
both cases, the combination being end-to-end or partial.</p>
<p>Being biologically inspired, this research aims to emulate this human
flexibility in skill execution and task composition with the previously
explained CNMP networks. Movement primitives that define skills are combined
differently for different goals and are adapted to the environment and context
thanks to the interpolation abilities of CNMP networks.</p>
<p>The following discussion will proceed with analyzing before the partial
skill composition with CNMPs, modeling how it is possible to go from one
movement primitive to the other. The motivation of this order is that this
part enables the partial combination and is used as well in the transition
moment of end-to-end skill concatenation.</p>
<p>Next, it will be analyzed how it is possible to concatenate them one after
the other to achieve meaningful execution in order to reach the final goal
position or even goal-state.</p>
<section id="foopartial-skill-combination" class="level2" data-number="4.1">
<h2 data-number="4.1"><span class="header-section-number">4.1</span> Partial
Skill Combination</h2>
<p>In order to synthesize a new skill by combining parts of others, it is
required to teach the model at least two different types of movement
primitives. Subsequently, the robot has to be able to pass from one action
(skill) to the other.</p>
<p>The moment of this transition has to be arbitrarily decided according to
the necessities and not specifically crafted at teaching time.</p>
<p>The challenge of this apparently straightforward problem resides in the
moment of the transition, because it has to be executed in a meaningful,
natural, and safe way. These simple three requirements will lead the following
examination through different approaches till the one designed.</p>
<p>For the purpose of a better understanding of the design and its steps, from
now on, a simplistic example (<a href="#foofig:trajX"
data-reference-type="ref" data-reference="fig:trajX">4.1</a>) will be used to
further clarify the explanations given. Nevertheless, the simplification
doesn&#x2019;t exclude the possibility of more elaborate tasks that require complex
movements or sensorimotor data.</p>
<p>The two actions discussed will be a simple movement upwards and a similar
movement downwards. They can represent, for example, a trajectory in real
life, a manipulator movement, or a joint trajectory. This allows a discussion
that starts from a single 1D dimension and keeps the understanding manageable
at the subsequent introduction of new dimensions.</p>
<figure id="foofig:trajX">
<img src="figures/trajX.png" />
<figcaption> Trajectories for two different skills demonstrated, in different
colors. The generated ones in grey color, from the black conditioning points.
</figcaption>
</figure>
<p>Some demonstration trajectories are given for both skills, in the <a
href="#foofig:trajX" data-reference-type="ref"
data-reference="fig:trajX">4.1</a> are visible as the three colorful thinner
lines ascending and three colorful thinner lines descending. The two triplets
differ to some extent, so the network has enough knowledge to create as well
new trajectories never seen. This will prove later that the method designed
also works with newly generated movement primitives.</p>
<p>The thicker points in the <a href="#foofig:trajX" data-reference-type="ref"
data-reference="fig:trajX">4.1</a> are the conditions, these force the model
to generate a trajectory that passes through that state. The movement
primitive created is denoted as the grey line passing through the condition
point, along with the uncertainty of the prediction in every timestep as its
width.</p>
<p>The shift can not be abrupt, so it&#x2019;s not sufficient to directly stitch
together two parts of the trajectories collected. The simplest solution of
executing one trajectory till a certain desired timestep and executing the
second one after that moment will create an abrupt jump in the execution for
the majority of timesteps where the trajectories don&#x2019;t perfectly intersect (<a
href="#foofig:trajX-failing-approaches" data-reference-type="ref"
data-reference="fig:trajX-failing-approaches">4.2</a>). A jump in the movement
primitive will lead to an unnatural fast change of pace and position of the
robot during the execution, which not clearly referable to human behavior.
Moreover, moving from one position to a completely different one in the next
moment is an unsafe behavior. It might lead to damage to the robot itself and
its surroundings, harm to people, or activate the safety stop measures of the
robot due to the high speed and torque applied.</p>
<figure id="foofig:trajX-failing-approaches">
<img src="figures/trajX-failing-approaches.png" />
<figcaption>Examples of failing approaches with stitching in CNMPs. On the
left, simple combination of parts. On the right, stitching the right part
using a conditioning point at the end of the left part.</figcaption>
</figure>
<p>Being able through the CNMP model to generate trajectories from previous
demonstrations allows a certain flexibility, so a second approach might
suggest generating a second trajectory starting from the end points of the
first one. This solution only delays the problem subsequently because the
condition point(s) on the second trajectory will be dependent on the first
one, requiring further calculation, and won&#x2019;t be solely used for the effective
purpose of making a trajectory reach the desired state. In <a
href="#foofig:trajX-failing-approaches" data-reference-type="ref"
data-reference="fig:trajX-failing-approaches">4.2</a>, right plot, the first
part of the trajectory is generated with the left-most condition (<span
class="math inline">\(time=0.3\)</span>). Subsequently, the second part is
generated using a conditioning point at the end of the left part plus the
desired one. These two conditioning points will likely not be on the same
trajectory generated: let&#x2019;s recall the robot arms generally have 6 or 7 DoF,
<a href="#foochap:platforms" data-reference-type="ref"
data-reference="chap:platforms">3</a>, and even the cartesian space has three
dimensions plus the 4D quaternion for the orientation, so the probability of
trajectories intersecting in spaces of at least six dimensions is minimal.
This will lead the CNMP model to average the two trajectories obtained by the
two conditions on different points. In <a
href="#foofig:trajX-failing-approaches" data-reference-type="ref"
data-reference="fig:trajX-failing-approaches">4.2</a>, right plot, we see the
second trajectory part passing in the middle of the two right-most
observations that would generate independently two different movement
primitives. To pass from one to another of these last two trajectories, a
shift will still be required, and this raises again the same problem. So, even
using conditioning points, there is a tradeoff between jumping and not meeting
the conditions for the second trajectory.</p>
<p>Since the stitching of parts is not a feasible option, the approach
selected implies giving the same network the two skills and obtaining a
coherent output.</p>
<figure id="foofig:trajX+tp">
<img src="figures/trajX+tp.png" />
<figcaption> CNMP modeling uncertainty for a condition. In the parametrized
CNMP, uncertainty is solved, and skills are encoded with task parameters 1 for
ascending and 2 for descending. </figcaption>
</figure>
<p>The CNMP model is capable of storing different movement primitives of
different skills without requiring multiple networks. The selection of the
right trajectory is due to the conditioning points (observations) previously
explained. The observations are indeed useful for both finding the
interpolated trajectory from the demonstrations passing in a new state and
also for identifying the correct skill. In <a href="#foofig:trajX"
data-reference-type="ref" data-reference="fig:trajX">4.1</a>, the conditioning
points identified uniquely the trajectory to generate. However, as it happens
in <a href="#foofig:trajX+tp" data-reference-type="ref"
data-reference="fig:trajX+tp">4.3</a>, the conditions might be uncertain, so
the standard CNMP model averages the outputs, creating a misleading
trajectory.</p>
<p>To eliminate uncertainty, a task parameter <span
class="math inline">\(\gamma\)</span> (TP) can be included in both the input
and the query of the network, as in <a href="#foofig:cnmp"
data-reference-type="ref" data-reference="fig:cnmp">2.5</a>. The task
parameter is a full-fledged new dimension of the network, like input time
<span class="math inline">\(t\)</span> and the output value. In the case of
the task parameters, the dimension added is an input dimension, which excludes
uncertainty. Now, the network, given a condition, can uniquely identify the
movement primitive.</p>
<figure id="foofig:graph-tp-transition">
<img src="figures/graph-tp-dimension.jpg" />
<figcaption> Plot of the interpolation abilities of CNMPs also in the
task-parameter dimension. </figcaption>
</figure>
<p>How adding a dimension to the input is interpreted by the networks and
what&#x2019;s in between the two parameters has been researched to understand better
how to shift among them. In order to do this, a 3D plot is required. This is
achieved by leveraging the previously considered weakness of the model of
having uncertain conditions. These points are shared by parameters among and
maintain the same other dimensions while the external parameter <span
class="math inline">\(\gamma\)</span> varies. This doesn&#x2019;t introduce any bias
while the condition changes.</p>
<p>The visualization of a continuous change between different task parameters
is present in <a href="#foofig:graph-tp-transition" data-reference-type="ref"
data-reference="fig:graph-tp-transition">4.4</a>. As it&#x2019;s possible to observe,
the network interpolates nicely among the functions also in the
tasks-parameter space. The movement of the conditioning point in time is
depicted in the 3D plot with a red line ending in its final position.</p>
<p>It is worth noting that to obtain the graph in <a
href="#foofig:graph-tp-transition" data-reference-type="ref"
data-reference="fig:graph-tp-transition">4.4</a>, both task parameters in the
observations and in the query had to vary together. As visible in <a
href="#foofig:cnmp" data-reference-type="ref"
data-reference="fig:cnmp">2.5</a>, the original network design of CNMPs
implies the presence of this parameter (as <span
class="math inline">\(\gamma\)</span>) for the conditioning points and for the
query <span class="math inline">\(t\)</span>.</p>
<p>Successively, the difference in variating only one at a time has been
researched. In the first place, for all the time steps, only the task
parameter of the condition was changed, maintaining the task parameter of the
query constant. Next, the opposite was performed to understand how they
influence the network result differently.</p>
<figure id="foofig:tp-influence">
<img src="figures/comparisonCNMPtp-influence.png" />
<figcaption> Influence of task parameter from the conditioning points and from
the queries. </figcaption>
</figure>
<p>The plots comparison is visible in <a href="#foofig:tp-influence"
data-reference-type="ref" data-reference="fig:tp-influence">4.5</a>. On the
top row, for task parameter 2, the first graph shows how the result changes
according to the variation of the task parameter in the condition. Meanwhile,
the second graph shows how, for every time queried, keeping the parameter
fixed to 2 in the conditions and changing it in the query doesn&#x2019;t produce a
significant variation in the output. The second row repeats the procedure with
the other task parameter to crosscheck the results. It is clearly emerging how
the parameter in the observation seems to have a stronger influence than the
one in the query, which doesn&#x2019;t seem to contribute significantly.</p>
<p>The following subsections will investigate the structure of the network and
the design of two different architectures. This work proposes before a network
with task parameters only in condition and, next, the model with task
parameters only in the query. These two architectures are respectively built
and examined below.</p>
<section id="foocnmp-model-with-task-parameter-only-in-condition"
class="level3" data-number="4.1.1">
<h3 data-number="4.1.1"><span class="header-section-number">4.1.1</span> CNMP
model with task-parameter only in condition</h3>
<p>This part investigates the architecture of the CNMP model altered to be
able to infer the results having the task parameters (TPs) only in the input
of the conditions. This design allows querying afterward the network only with
the time <span class="math inline">\(t\)</span>. This architectural choice is
motivated by the previous analysis in which the task parameter in the query
seemed to have little if no importance for the results. This also feels
naturally more sensible for constant tasks as the request is only for the
value at a time step, and the task parameter would remain constant anyway.</p>
<figure id="foofig:CNMP_short_no_tp_query">
<img src="figures/CNMP_short_no_tp_query.png" />
<figcaption> Architecture of the CNMP model proposed without the original task
parameter in the queries. </figcaption>
</figure>
<p>In <a href="#foofig:CNMP_short_no_tp_query" data-reference-type="ref"
data-reference="fig:CNMP_short_no_tp_query">4.6</a>, the design changes can be
compared to the original model. The neural network now feeds the decoder only
with the time value to query and the constant representation of the
conditions. As a result, the decoder has an input dimension less than the
original model.</p>
<div class="longtable">
<p><span>|c|l|c|c|</span></p>
<p><br />
<strong>MSE Error</strong> &amp; <strong>TP</strong> &amp;
<strong>CNMP</strong> &amp; <strong>CNMP TP condition</strong><br />
&amp; 1 &amp; 2.200240773631327e-06 &amp; 0.0005239668753240399<br />
&amp; 2 &amp; 2.852831969051669e-08 &amp; 3.317327413240996e-05<br />
&amp; 1 &amp; 2.8834989373212024e-06 &amp; 0.0004749418782444545<br />
&amp; 2 &amp; 2.598636844065303e-07 &amp; 3.6395583963202646e-06<br />
</p>
</div>
<p>The new model has been trained on the same dataset and verified with the
same validation set.</p>
<p>The quantitative comparison results in <a
href="#footab:CNMPvsCNMPnoTPquery" data-reference-type="ref"
data-reference="tab:CNMPvsCNMPnoTPquery">[tab:CNMPvsCNMPnoTPquery]</a> show
the average mean square errors (MSE) for every task on demonstrated
trajectories and interpolated trajectories. The performance of the modified
architecture exhibits an increase in the errors that, while present, is not
significantly detrimental, suggesting a promising level of robustness in its
overall functionality.</p>
<p>The qualitative results of the interpolation can be seen in <a
href="#foofig:comparisonCNMPvsCNMPonlyTPcondition" data-reference-type="ref"
data-reference="fig:comparisonCNMPvsCNMPonlyTPcondition">4.7</a>, and although
some interpolation differences are visible, they still clearly maintain a
sufficient degree of correctness.</p>
<p>The new network architecture maintains the same interpolation capabilities,
but allows the user to query only the time, without worrying about the task
parameter, that will be inferred constant form the observations.</p>
<figure id="foofig:comparisonCNMPvsCNMPonlyTPcondition">
<img src="figures/comparisonCNMPvsCNMPonlyTPcondition.png" />
<figcaption> Interpolation comparison of CNMP model vs CNMP model with TP only
in the condition. </figcaption>
</figure>
</section>
<section id="foocnmp-model-with-task-parameter-only-in-query" class="level3"
data-number="4.1.2">
<h3 data-number="4.1.2"><span class="header-section-number">4.1.2</span> CNMP
model with task-parameter only in query</h3>
<p>This subsection analyses the opposite alternative to the previous
investigation. The architecture of the CNMP model is altered to generate the
results using the task parameters (TPs) only in the input of the query.</p>
<p>This design allows having conditioning points that are parameter-less and
querying the network subsequently with the time <span
class="math inline">\(t\)</span> and the task parameter <span
class="math inline">\(\gamma\)</span>.</p>
<p>This architectural choice seems to be more appropriate for possible changes
at run-time of skill by the model. However, it&#x2019;s clearly more challenging
since the information is provided later in the pipeline.</p>
<p>Moreover, conditioning points are responsible for the final position at
every time <span class="math inline">\(t\)</span>, and feeding the network
only subsequently with the task forces it to infer the <span
class="math inline">\(\gamma\)</span> of the conditions.</p>
<figure id="foofig:CNMP_short_no_tp_observations">
<img src="figures/CNMP_short_no_tp_observations.png" />
<figcaption> Architecture of the CNMP model proposed without the original task
parameter in the observations. </figcaption>
</figure>
<p>In <a href="#foofig:CNMP_short_no_tp_observations"
data-reference-type="ref"
data-reference="fig:CNMP_short_no_tp_observations">4.8</a>, the design changes
can be compared to the original model. The neural network now feeds the
encoder with observations that don&#x2019;t have any parameter, delaying the <span
class="math inline">\(\gamma\)</span> inference to the decoder. As a result,
the encoder has an input dimension less than the original model.</p>
<p>The second new model has been trained on the same dataset and verified with
the same validation set as the previously discussed ones.</p>
<div class="longtable">
<p><span>|c|l|c|l|</span></p>
<p><br />
<strong>MSE Error</strong> &amp; <strong>TP</strong> &amp;
<strong>CNMP</strong> &amp;<br />
&amp; 1 &amp; 2.200240773631327e-06 &amp; 0.00015134105003480186<br />
&amp; 2 &amp; 2.852831969051669e-08 &amp; 1.5758800492775196e-05<br />
&amp; 1 &amp; 2.8834989373212024e-06 &amp; 0.0008166800702431865<br />
&amp; 2 &amp; 2.598636844065303e-07 &amp; 0.0002007523980230904<br />
</p>
</div>
<p>The quantitative comparison results in <a
href="#footab:CNMPvsCNMPnoTPcondition" data-reference-type="ref"
data-reference="tab:CNMPvsCNMPnoTPcondition">[tab:CNMPvsCNMPnoTPcondition]</a>
show the average mean square errors (MSE) for every task on demonstrated
trajectories and interpolated trajectories. The modified architecture again
demonstrates a rise in errors compared to the original model, but these
errors, though now noticeable, do not compromise the usability of the
model.</p>
<figure id="foofig:comparisonCNMPvsCNMPonlyTPquery">
<img src="figures/comparisonCNMPvsCNMPonlyTPquery.png" />
<figcaption> Interpolation comparison of CNMP model vs CNMP model with TP only
in the query. </figcaption>
</figure>
<p>Not the same can be said for the qualitative results of the interpolation.
The comparison that can be seen in <a
href="#foofig:comparisonCNMPvsCNMPonlyTPquery" data-reference-type="ref"
data-reference="fig:comparisonCNMPvsCNMPonlyTPquery">4.9</a> shows a
remarkable drop in the values in the interpolated area between the two
functions. This will clearly impact the correctness of a transition among
them. The inaccuracy is probably due to the fact that the parameter
information goes through an inferior network depth compared to the full model.
Nevertheless, these results leave room for further research on how to
condition with one state parameterless and let the network, queried with
different tasks, pass through that state.</p>
</section>
<section id="foocomparison-of-the-previous-models" class="level3"
data-number="4.1.3">
<h3 data-number="4.1.3"><span class="header-section-number">4.1.3</span>
Comparison of the previous models</h3>
<p>A comparison of the three previously discussed models is presented below to
better evaluate the most capable and cross-validate the results. A different
dataset was built with different parametric functions: linear, parabolic, and
sinusoidal. This also implies the presence of three different tasks to feed to
a single CNMP network. For each skill, different trajectories are present in
the dataset to enable new trajectory generation for conditioning points of
unseen values.</p>
<p>Moreover, the transition among all tasks is now represented with three
values, so the first task is encoded as <span
class="math inline">\([1][0][0]\)</span>, the second as <span
class="math inline">\([0][1][0]\)</span>, the third as <span
class="math inline">\([0][0][1]\)</span>. This avoids passing through the
middle one as in the case of a single parameter <span
class="math inline">\([1..2..3]\)</span> encoded network. The transition is
performed by decreasing one value and increasing the other simultaneously.</p>
<p>In <a href="#foofig:comparisonCNMP3trajectories" data-reference-type="ref"
data-reference="fig:comparisonCNMP3trajectories">4.10</a>, it is possible to
observe all the possible combinations for interpolating the different tasks on
which the network has been trained. Even with more than two tasks and three
dimensions added to the input the CNMP model performs sufficiently well in all
three skills.</p>
<figure id="foofig:comparisonCNMP3trajectories">
<img src="figures/comparisonCNMP3trajectories.png" />
<figcaption> Interpolation comparison of the 3 CNMP models analyzed for a
different dataset and multiple tasks. </figcaption>
</figure>
</section>
<section id="foocnmp-changing-task-in-time-with-one-conditioning-point"
class="level3" data-number="4.1.4">
<h3 data-number="4.1.4"><span class="header-section-number">4.1.4</span> CNMP
changing task in time with one conditioning point</h3>
<p>Analyzed the interpolation capabilities of the CNMP networks also on the
task dimension, a possible method to shift the execution of a skill in time is
proposed below.</p>
<p>In order to build the interpolation visualizations, the network was always
queried for every time-step with a constant <span
class="math inline">\(\gamma\)</span>. However, since the queries are
independent, it&#x2019;s possible to query the time and parameter singularly as
required.</p>
<p>Moreover, from the previously constructed plots, it is evident how it is
possible to change tasks in time via querying the network with TPs linearly
changing with time.</p>
<figure id="foofig:tpshift">
<img src="figures/tpshift.png" />
<figcaption> Transition in time of task through task parameter shifting
</figcaption>
</figure>
<p>In <a href="#foofig:tpshift" data-reference-type="ref"
data-reference="fig:tpshift">4.11</a>, the path of the queries of time and
<span class="math inline">\(\gamma\)</span> is depicted in yellow. As time
goes on, left to right, the action moves from descending (back of the graph)
to ascending (front of the graph). This achieves a smooth change between task
parameters using the interpolation space provided by the model.</p>
<p>In this case, math leverages the neural network&#x2019;s hidden capabilities to
wisely input the required parameters to generate the desired output.</p>
<p>It&#x2019;s worth noting that this method works with any CNMP model variation
previously discussed, as long as the interpolation space is built correctly.
Further research could include better training to improve the results in the
interpolation dimensions.</p>
<p>The mixing of the two different skills is indeed performed through time,
and in this case, the change is linear. Different functions analyzed produced
different results, among them the most famous ones like sigmoid, logarithmic,
and others. The model is capable of different transition speeds and periods
depending on the selected function. The linear function is selected given that
it produces the smoothest transitions since inclination is minimal in all the
timesteps.</p>
<p>On the opposite, a sharper function, like a step function in <a
href="#foofig:tpshift-diff-functions" data-reference-type="ref"
data-reference="fig:tpshift-diff-functions">4.12</a>, produces a shift that is
immediate and full-fledged a stitch among two parts.</p>
<figure id="foofig:tpshift-diff-functions">
<img src="figures/tpshift-diff-functions.png" />
<figcaption> Different functions for changing task parameter in the CNMP
network </figcaption>
</figure>
<p>In <a href="#foofig:tpshift" data-reference-type="ref"
data-reference="fig:tpshift">4.11</a>, it is possible to see that the final
trajectory result is not passing through the conditioning point. This occurs
since, at that moment <span class="math inline">\(t=0.5\)</span>, the
transition between two different task parameters is halfway (<span
class="math inline">\(\gamma=1.5\)</span>) and not fully on one <span
class="math inline">\(\gamma\)</span>. The interpolated space varies in
quality depending on the network and its training, and this means the error
would be higher compared to the values on <span
class="math inline">\(\gamma\)</span> on which the model was trained.</p>
<p>Surprisingly, it&#x2019;s not actually a problem because the conditioning point is
nothing else than a point that symbolizes the transition between two
conditioning points with different <span class="math inline">\(\gamma\)</span>
located on different positions. The conditioning point is just a point that
allows the transition without introducing any bias because it&#x2019;s part of both
trajectories. In <a href="#foofig:tp-condition-point-meaning"
data-reference-type="ref"
data-reference="fig:tp-condition-point-meaning">4.13</a>, it is possible to
see the red conditioning point emulating two different conditions with
different TP, respectively descending for the left one and ascending for the
right one.</p>
<p>This means that the conditioning point for transition is not meant to be
anywhere, but it has to be at the intersection of the two desired functions
with different TPs. This will grant the resulting function to pass at the
initial time through the first conditioning point, since its TP is not mixed,
and at the final time through the second condition for the same reason.</p>
<p>In the example <a href="#foofig:tp-condition-point-meaning"
data-reference-type="ref"
data-reference="fig:tp-condition-point-meaning">4.13</a>, at time <span
class="math inline">\(t=0\)</span>, the function passes through the first
point because the task parameter is fully descending. This is guaranteed
because the red observation chosen is on the line produced by that initial
point. In the same way, at time <span class="math inline">\(t=1\)</span>, it&#x2019;s
guaranteed to pass through the rightmost point because the observation is also
on the function that is produced by that condition. For this reason, the
observation point is not meant to be independent but derived from the two
points to emulate.</p>
<figure id="foofig:tp-condition-point-meaning">
<img src="figures/tp-condition-point-meaning.png" />
<figcaption> A conditioning point that varies task parameter emulates two
different conditioning points of different task parameters. </figcaption>
</figure>
<p>The best strategy to find the conditioning point for the transition among
two conditions of different <span class="math inline">\(\gamma\)</span> seems
to place it on the intersection of the two trajectories generated by these
conditions. However, it is not always granted that the trajectories will
intersect. As discussed above, in multidimensional spaces, the probability of
this event decreases significantly.</p>
<p>When two functions don&#x2019;t intersect, the best possible solution to not bias
the interpolation is, while changing the <span
class="math inline">\(\gamma\)</span>, to shift as well the position of the
condition in time. In the series of queries to the network, instead of giving
the same context point and changing its task parameter, the position also
changes. The optimal change of position is from a point on the first function
generated by the first condition to the closest point on the function
generated by the second condition. This guarantees the series of points will
stay on the interpolation of the two functions and generate the desired
interpolation surface area.</p>
<p>Extending the concept, the algorithm defined looks for the closest points
among the two functions, and it transitions among them. The closest couple of
points is looked only in the time span in which the network was trained, since
it is a computation of cost <span class="math inline">\(t^2\)</span>. If there
is an intersection, the change on the conditioning point will be only on the
task parameter. If there is no intersection, the change of the conditioning
point will also be a change in position from the closest point of the first
function to the closest of the second one.</p>
</section>
<section id="foocnmp-changing-task-in-time-with-multiple-conditioning-points"
class="level3" data-number="4.1.5">
<h3 data-number="4.1.5"><span class="header-section-number">4.1.5</span> CNMP
changing task in time with multiple conditioning points</h3>
<p>The system developed enables a single transition using a single
conditioning point. The shift occurs completely from an initial time that is
<span class="math inline">\(t=0\)</span> to the final time of the network
training.</p>
<p>It&#x2019;s a remarkable success because the bare normal CNMP model is completely
incapable of changing the task coherently and continuously among its
predictions. If fed with two conditions of different TP, the network outputs a
completely unusable trajectory. This trajectory is a rough average of all the
different ones generated by the conditioning points. This is due to the fact
that the model, by definition, doesn&#x2019;t have an attention mechanism and doesn&#x2019;t
lose the conditioning power in time.</p>
<p>However, this research extends the method further to multiple conditions
with different task parameters. This enables the full control and
customization of the predicted output.</p>
<p>Furthermore, it achieves the shift among the observations in a desired time
span not restricted to the full training time length.</p>
<p>Once the interpolated surface is built, the time constraints are resolved
with a fast transition from the first condition timestep to the second
condition timestep. The resulting output sequence will still reside on the
interpolated surface but, having less time, will be faster.</p>
<figure id="foofig:tp-multiple-shift">
<img src="figures/tp-multiple-shift.png" />
<figcaption> Procedure to individuate multiple conditions for multiple shifts
in the task parameters. </figcaption>
</figure>
<p>The multiple transitions are achieved by coupling them one by one in
temporal order, so the first one determines the task parameter till its time.
Then, between the first and the second one, the task is shifted. At the second
observation, the task parameter will be fully on its task, but subsequently,
will start to be merged with the third one, and so on. This guarantees that at
the observation&#x2019;s times, the output function will pass through them, but in
between, the task change will occur.</p>
<p>In <a href="#foofig:tp-multiple-shift" data-reference-type="ref"
data-reference="fig:tp-multiple-shift">4.14</a>, it is possible to view the
initial observations of different parameters, two green conditioning points
for the descending task and one blue in the middle for the ascending task. The
first couple in temporal order is selected. At this point, the two functions
each condition will independently create are generated. Those are visible in
the <a href="#foofig:tp-multiple-shift" data-reference-type="ref"
data-reference="fig:tp-multiple-shift">4.14</a> in the second plot.</p>
<p>The closest points between these functions are selected, the absolute
distance among the points is plotted as the green line at the bottom of the
plot. In this case, since there is an intersection, the couple selected has
the same starting and ending position, which only differs for the <span
class="math inline">\(\gamma\)</span> parameter. During the shift time, the
conditioning point of the network will transition from the first identified to
the other one, changing TP. If the two closest points were in different
locations, it would also change location, along with task parameter. The
network is queried at the appropriate time with the condition designated.</p>
<figure id="foofig:tp-multiple-shift-condition">
<img src="figures/tp-multiple-shift-condition.png" />
<figcaption> Graph and full path in time of the condition point used to
transition multiple tasks parameters </figcaption>
</figure>
<p>The process repeats for the second couple and so on to find the
conditioning points to feed the network to obtain the desired result. The
final full path of the conditioning point is visible in <a
href="#foofig:tp-multiple-shift-condition" data-reference-type="ref"
data-reference="fig:tp-multiple-shift-condition">4.15</a> where it changes
position in time according to the place where it will not bias the current
transition. Meanwhile, the changing of the TP will occur during the time span
designated.</p>
<p>It&#x2019;s worth noting that the conditioning point will also change its time,
independently from the time queried. For this reason, in the 3D graph in <a
href="#foofig:tp-multiple-shift-condition" data-reference-type="ref"
data-reference="fig:tp-multiple-shift-condition">4.15</a> the fourth dimension
is introduced as color. The x axes of the graph corresponds now to the time of
the condition, while the time of the queries corresponds to the change in
color.</p>
<p>Finally, the results shown in <a href="#foofig:tp-multiple-shift-results"
data-reference-type="ref"
data-reference="fig:tp-multiple-shift-results">4.16</a> demonstrate how this
method can achieve trajectory predictions that are coherent across multiple
shifts of tasks. The normal CNMP model with the same conditioning points is
reported aside for comparison.</p>
<figure id="foofig:tp-multiple-shift-results">
<img src="figures/tp-multiple-shift-results.png" />
<figcaption> Final results of multiple tasks transition and comparison with
traditional CNMP network. </figcaption>
</figure>
</section>
</section>
<section id="fooend-to-end-skill-concatenation" class="level2"
data-number="4.2">
<h2 data-number="4.2"><span class="header-section-number">4.2</span>
End-To-End Skill Concatenation</h2>
<p>The proposed method in the previous chapter would be really limited if not
coupled with the ability to compose the skills in succession. Being able to
combine parts is indeed useful, but it will eventually end in the time span of
one action, even if we use parts of other ones. To really enhance the
potentiality of the network and exponentially enlarge the capacities of the
robot, a way to join the skills end-to-end is required.</p>
<p>In this part, a method is proposed with which it is possible to concatenate
skills one after the other to achieve a goal or reach the final target
position or desired state.</p>
<figure id="foofig:diff-traj-dataset">
<img src="figures/diff-traj-dataset.png" />
<figcaption> Artificial dataset of different skills to concatenate. On the
left, there are the demonstrations. On the right, there is the validation.
</figcaption>
</figure>
<p>In order to synthesize a series of actions, the demonstrations have to be
initially collected. Subsequently, the method presented will combine them
end-to-end to reach a desired objective.</p>
<p>An immediate challenge presented consists of the concatenation of the
movement primitives without incurring in jumps when passing from one to the
other. Abrupt jumps in the final movement execution, as previously analyzed,
can be dangerous and look unnatural.</p>
<p>The requirements, also for this research subpart, are indeed that the
motions have to be executed with meaning, they have to seem natural, and they
have to be safe.</p>
<p>Another challenge to face is the different lengths of the actions in time
and space. Some actions might be faster than others because they are more
easily executable, for example, picking an object compared to pouring water.
Furthermore, the same actions might last less if performed in different points
of space, for example, placing an object in a container next to it or further
away.</p>
<p>Another possible challenge is choosing the right action sequence to
perform. Having all the possibilities from a starting point means that a
meaningful choice has to be made. Every action has an effect on the
environment, and some of them have a specific order that can not be
permutated. Deciding the right order is a complication to solve,</p>
<p>For simplicity, the example that will follow will refer to a simple dataset
<a href="#foofig:diff-traj-dataset" data-reference-type="ref"
data-reference="fig:diff-traj-dataset">4.17</a> that eases the comprehension
of the concepts. Nevertheless, this method can still be extended to multiple
dimensions and more complex scenarios.</p>
<p>In the dataset artificially generated, two trajectories are provided for
each skill. They can resemble any task or movement in time, for example, push
or shake an object or move to a position. This dataset enables a clear
visualization later on. More realistic but more difficult-to-understand data
can be found in the <a href="#foochap:validation" data-reference-type="ref"
data-reference="chap:validation">6</a>.</p>
<p>The different skills, in this case, are given to the same network with the
task-parameter option seen in the previous chapter, but it&#x2019;s not necessary.
The task parameters are, this time, three, one for each task, and they are
boolean, so it&#x2019;s clearly visible which task is queried to the network. The
task parameters are visible in the legend of the <a
href="#foofig:diff-traj-dataset" data-reference-type="ref"
data-reference="fig:diff-traj-dataset">4.17</a>.</p>
<p>As mentioned in the previous chapter, for every skill, not one but multiple
different trajectories are collected. This is not mandatory, as the network
can learn with as little as one demonstration. However, having multiple expert
demonstrations allows the network to generate new unseen ones from
interpolation. This extra capability is fundamental since it allows the
extension of the effectiveness of the actions from a single point to a whole
area or volume. Furthermore, having a network that generates new unseen
trajectories will allow the method proposed to be validated later for these
cases.</p>
<p>The interpolation abilities of the network are validated on the
trajectories visible in the right plot. These trajectories enable the code to
verify if the movement primitives generated by the network have a minimum
error or not.</p>
<figure id="foofig:diff-traj-jump">
<img src="figures/diff-traj-jump.png" />
<figcaption> An example of failing concatenation of skills due to abrupt
jumps. </figcaption>
</figure>
<p>The joining of multiple trajectories can&#x2019;t be a simple stitch of the
previously recorded ones since it will also create abrupt jumps, <a
href="#foofig:diff-traj-jump" data-reference-type="ref"
data-reference="fig:diff-traj-jump">4.18</a>. We want to avoid this event for
the safety and human-like movements as required but also for a deeper further
reason.</p>
<p>Since the network can deal easily with multidimensionality, some dimensions
might be added in the future to represent the state of the world. Jumps in the
final trajectory must be avoided because the network would allow unmotivated
internal world representation changes without being the author of them.
However, this is an option that we will explore at the end of this
chapter.</p>
<section id="foocnmp-for-skill-concatenation" class="level3"
data-number="4.2.1">
<h3 data-number="4.2.1"><span class="header-section-number">4.2.1</span> CNMP
for skill concatenation</h3>
<figure id="foofig:diff-traj-interpolation">
<img src="figures/diff-traj-interpolation.png" />
<figcaption> An example of using interpolation with CNMP model to generate new
consecutive smooth trajectories. </figcaption>
</figure>
<p>To overcome part of the concatenation abrupt changes problem, the ability
of CNMP of interpolation is used again. Once the final point of an action is
reached, the model is queried to generate the next skills with a conditioning
starting point matching it. The model, conditioned on that initial position,
will generate the actions required that start from that state, avoiding big
discrepancies in the final trajectory generated.</p>
<p>In <a href="#foofig:diff-traj-interpolation" data-reference-type="ref"
data-reference="fig:diff-traj-interpolation">4.19</a>, it is possible to see
that the first trajectory is the result of the interpolation of the two
sinusoidal demonstrations of the dataset depicted in grey. Furthermore, the
end of the first trajectory is used as a condition to generate the second one.
The second is the interpolation of the two linear functions, also depicted in
the background.</p>
<p>It&#x2019;s worth noting that in the example, the output is monodimensional, and
the interpolation is simple, but in multidimensional inputs, the CNMPs can
interpolate and generate output primitives that are able to interpolate in the
whole 3D or joint space. This means that the trajectories of every joint, or
3D axis, won&#x2019;t have abrupt jumps.</p>
<p>Using a real-life example, if the final position of an action is reached
and no other action demonstrated starts from there, the model will use the
demonstrated trajectories that start from other points and combine them to
generate a skill that starts from that position.</p>
<figure id="foofig:diff-traj-concat">
<img src="figures/diff-traj-concat.png" />
<figcaption> An example of a recursive concatenation of movement primitives
using interpolation abilities of CNMPs. </figcaption>
</figure>
<p>At this point, given a starting point, the method proposed recursively
builds the graph of all the possible actions, <a
href="#foofig:diff-traj-concat" data-reference-type="ref"
data-reference="fig:diff-traj-concat">4.20</a>. This means that all the
actions are evaluated from the starting position, then for all the positions
reached, all the actions are generated, and so on.</p>
<p>The cost of this action is exponential with time; the cost is <span
class="math inline">\(O(n_a^{d})\)</span>, where <span
class="math inline">\(n_a\)</span> is the number of actions available and
<span class="math inline">\(n_t\)</span> is the depth number of subsequent
actions. The cost is high but can be easily reduced since the network can be
queried only for the prediction of the final step, and the queries take
milliseconds.</p>
<p>Furthermore, mechanisms for pruning the tree if a jump is detected can
significantly reduce the number of possibilities. Moreover, the building of
the tree can stop once a viable sequence is found to reach the final goal.
Lastly, more informed research is possible with heuristic algorithms that can
reduce the steps to find a desired goal. However, the scope of this research
remains to demonstrate the validity of the method, so the optimizations are
left for future work.</p>
<p>Even with the interpolation ability, the graph still contains some jumps,
and we want to avoid them for the reasons explained previously. This is due to
the fact that repeating some actions over and over again will bring the state
out of the demonstration range provided to the network.</p>
<p>The CNMP model shines in the interpolation but lacks the ability to
extrapolate from demonstrations. So, if a conditioning point is positioned out
of the area between expert demonstrations, the network will simply output the
demonstration that is the closest to that point without going further.</p>
<figure id="foofig:diff-traj-filtered">
<img src="figures/diff-traj-filtered.png" />
<figcaption> An example of filtering the skills generated based on
extrapolation limitations. </figcaption>
</figure>
<p>This problem actually helps to find the actions that are simply not
feasible. For the sake of safety and meaningfulness, it is not actually
reasonable to extrapolate the actions demonstrated to the whole space around
the agent. For example, concatenating too many push actions might, in real
life, exceed the reachability area of the robot or its arm length.</p>
<p>For this step the assumption taken is that the expert gives demonstrations
that are at the limits of the capacity of the robot. This allows the method to
derive what is feasible and what is not.</p>
<p>Consequently, to remove the actions that are not feasible from the option
set, it is sufficient to leverage the limitations of the model and just look
for big data variations in the sequences generated. After finding them, it&#x2019;s
possible to prune the tree from these unfeasible action concatenations.</p>
<p>In <a href="#foofig:diff-traj-filtered" data-reference-type="ref"
data-reference="fig:diff-traj-filtered">4.21</a> are presented the selected
trajectories generated by the network, with interpolation to avoid jumps, but
without extrapolation to avoid the impractical ones. As it is possible to see
in the picture, among the four trajectories chosen, it&#x2019;s also present in red
the one previously shown in <a href="#foofig:diff-traj-interpolation"
data-reference-type="ref"
data-reference="fig:diff-traj-interpolation">4.19</a> and created using
interpolation.</p>
<p>All the actions presented in the example have the same temporal length.
This is not mandatory but a choice for the sake of simplicity. An extension of
this research could include skills that have different durations. Overall,
since the majority of movement primitives are simple actions and last a few
seconds, this complication was not addressed.</p>
</section>
<section id="foocnmp-embedding-environment-representation" class="level3"
data-number="4.2.2">
<h3 data-number="4.2.2"><span class="header-section-number">4.2.2</span> CNMP
embedding environment representation</h3>
<p>Trajectories generated in this way are executable and this means that the
method works for spatial concatenation. On the other hand, the final goal in
this way can be only a position in space or a joint state. Although it is
really useful to make the robot or the agent reach new places using the
previous knowledge, this is limited to the device or, at maximum, the object
it can carry.</p>
<p>Upon observation, it&#x2019;s clear that many movement primitives involve an
external object or the interaction with the environment around them. Another
step in complexity is to develop the method for goals that are not only as a
point in time or robot position but also related to the external world.</p>
<p>The external world introduces a whole new degree of complication because it
requires having a degree of knowledge about it. Without this internal
representation of the environment, the robot would execute actions that are
possible but not meaningful. For example, picking an object that is not
present or placing a lid before the pot.</p>
<p>Since the CNMP model can deal with multidimensional data, the solution
proposed is to embed the state changes of the environment in the network.
Given a representation of the world, the network can learn how the actions
performed change it and the final outcome. The representation can be as simple
as a single <span class="math inline">\(x,y\)</span> position of an object
manipulated or the whole image of the environment.</p>
<p>Furthermore, the network&#x2019;s ability to interpolate will make it adaptable to
new states, as long as they can be interpolated from demonstrations, and it
will be able to predict the outcome of the actions on them.</p>
<div id="footab:cnmp_env_represenation">
<table>
<caption>Example of CNMP ability to embed environment
representations</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">left arm</th>
<th style="text-align: left;">right arm</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">move arm to center</td>
<td style="text-align: left;">[<strong>0</strong>,1,2] <span
class="math inline">\(\rightarrow\)</span> [<strong>1</strong>,1,2]</td>
<td style="text-align: left;">[0,1,<strong>2</strong>] <span
class="math inline">\(\rightarrow\)</span> [0,1,<strong>1</strong>]</td>
</tr>
<tr class="even">
<td style="text-align: left;">move arm to its side</td>
<td style="text-align: left;">[<strong>1</strong>,1,2] <span
class="math inline">\(\rightarrow\)</span> [<strong>0</strong>,1,2]</td>
<td style="text-align: left;">[0,1,<strong>1</strong>] <span
class="math inline">\(\rightarrow\)</span> [0,1,<strong>2</strong>]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">pick and place from side to center</td>
<td style="text-align: left;">[<strong>0</strong>,<strong>0</strong>,2] <span
class="math inline">\(\rightarrow\)</span>
[<strong>1</strong>,<strong>1</strong>,2]</td>
<td style="text-align: left;">[0,<strong>2</strong>,<strong>2</strong>] <span
class="math inline">\(\rightarrow\)</span>
[0,<strong>1</strong>,<strong>1</strong>]</td>
</tr>
<tr class="even">
<td style="text-align: left;">pick and place from center to side</td>
<td style="text-align: left;">[<strong>1</strong>,<strong>1</strong>,2] <span
class="math inline">\(\rightarrow\)</span>
[<strong>0</strong>,<strong>0</strong>,2]</td>
<td style="text-align: left;">[0,<strong>1</strong>,<strong>1</strong>] <span
class="math inline">\(\rightarrow\)</span>
[0,<strong>2</strong>,<strong>2</strong>]</td>
</tr>
</tbody>
</table>
</div>
<p>A simplistic example is reported. The CNMP model can be trained on both
arms of a robot and enable the movement of an object from a position that is
reachable only from the left arm to a position that is reachable only to the
right arm. This action requires the internal knowledge of the position of the
object to find the right sequence and not to execute actions that are possible
but not meaningful if the object is not in the desired position. The object
position is encoded with a state <span class="math inline">\([1,2,3]\)</span>
depending on its position from left to right. We assume the left position is
only reachable from the left arm, the central position from both arms and the
right position from the right arm. The arms&#x2019; positions are encoded in the same
way, but by definition, the left arm can reach only the first two and the
right arm only the last two. In a real-world scenario, the position of the arm
will not be monodimensional but probably multidimensional, but the example is
still valid. The world representation is finally <span
class="math inline">\([PositionLeftArm, PositionObject,
PositionRightArm]\)</span>. The CNMP model will learn the changes in time of
both the arms and the environment representation. The possible demonstrations
are depicted in <a href="#footab:cnmp_env_represenation"
data-reference-type="ref" data-reference="tab:cnmp_env_represenation">4.1</a>,
where pick and place means close the gripper, move to a position, and open the
gripper.</p>
<p>Given a condition on the initial state where the object is on the left
<span class="math inline">\([0,0,2]\)</span>, the method proposed will build
the whole graph of possible actions and resultant environment states. At this
point the sequence for bringing the object from left to right, final state
<span class="math inline">\([0,2,2]\)</span> is:</p>
<ol>
<li><p><span class="math inline">\([0,0,2] \rightarrow [1,1,2]\)</span> left
arm pick and place from side to center</p></li>
<li><p><span class="math inline">\([1,1,2] \rightarrow [0,1,2]\)</span> left
arm move arm to its side</p></li>
<li><p><span class="math inline">\([0,1,2] \rightarrow [0,1,1]\)</span> right
arm move arm to center</p></li>
<li><p><span class="math inline">\([0,1,1] \rightarrow [0,2,2]\)</span> right
arm pick and place from center to side</p></li>
</ol>
<p>It&#x2019;s worth noting that the method doesn&#x2019;t require additional data
structures, but it&#x2019;s all embedded in the network.</p>
<p>This method extension means adding the dimensions normally to the network&#x2019;s
input and output. In this way, it&#x2019;s still possible to use the previously
presented method to discard the actions that don&#x2019;t bring to a valid state. In
the example, it is impossible to move one arm in the center while the other is
there so the arms won&#x2019;t collide.</p>
<p>Moreover, the action possibilities can still be filtered by analyzing
abrupt changes in the world representation to understand when an action&#x2019;s
output will lead to a state where further actions are not possible. In the
example, the object must be in the center to be picked and placed on the side.
If not, the generated action will show an abrupt initial change in the
environment representation.</p>
<p>While the presented method works out of the box for reaching goals in
space, a possible extension for meaningful executions is presented here.
Nevertheless, the scope of this work is not action planning, which is a
complex topic in robotics beyond the scope of this work.</p>
</section>
</section>
</section>
<section id="foochap:implementation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span>
Implementation</h1>
<p>This chapter gives a more in-depth view of the implementation of the
methods discussed before. This doesn&#x2019;t mean other implementations are not
possible or the method designed can&#x2019;t be built in other programming languages
and frameworks.</p>
<p>The platforms and tools presented in the <a href="#foochap:platforms"
data-reference-type="ref" data-reference="chap:platforms">3</a> are used.
Python was chosen as a programming language for its ability to deal with data
and machine learning but it can as well be used for robotics thanks to the
"rospy" package.</p>
<p>The most important passages in the code developed and only the key points
of the solution implemented will be explained below. They can be used as a
reference for clarification, for another implementation, or for further
improvements.</p>
<p>This code is available in a <a
href="https://github.com/igor-lirussi/CNMP-Robotic-Skill-Synthesis">GitHub
public repository</a>, <span class="citation" data-cites="url:CNMPrepo">(Igor
Lirussi 2023c)</span>.</p>
<p>Following the structure of the <a href="#foochap:design"
data-reference-type="ref" data-reference="chap:design">4</a>, this chapter is
divided into two main subsections.</p>
<p>The first subsection explains the implementation of the partial skill
combination. The parts of this section follow the previous chapter; the first
is relative to the implementation of the CNMP model with the task parameter
only in condition, and the second is relative to the CNMP model with the
task-parameter only in query. The third part is a comparison of all three
models. Lastly, the last two parts will explain how CNMP changing task
executed in time with one conditioning or multiple conditions are
developed.</p>
<p>The second subsection shows the implementation of the previously explained
End-To-End Skill Concatenation method.</p>
<p>Since the CNMP model has been used in both subsections, the structure is
briefly reported in <a href="#foolst:cnmp" data-reference-type="ref"
data-reference="lst:cnmp">[lst:cnmp]</a>. The encoder and the decoder networks
are multilayer perceptions consisting of three fully connected layers with the
non-linear ReLU activation function. The dimensionality is fixed to 128
dimensions except for the first and last layers.</p>
<p>The encoder&#x2019;s first layer has a dimensionality equal to the sum of the
input and output ones, since the observations have both of them. It will
output the latent representations to aggregate together with
position-independent operations, as in equation <a
href="#fooeq:commutative-operation" data-reference-type="ref"
data-reference="eq:commutative-operation">[eq:commutative-operation]</a>.</p>
<p>The first layer of the decoder has a dimensionality equal to 128 for the
latent representation plus the input query desired to concatenate. The decoder
outputs double the output dimensions since for each one is returned the mean
and standard deviation.</p>
<div class="sourceCode" id="foolst:cnmp" data-float="" data-language="Python"
data-firstline="1" data-lastline="20" data-caption="CNMP model code"
label="lst:cnmp"><pre class="sourceCode python"><code class="sourceCode python"><span id="foolst:cnmp-1"><a href="#foolst:cnmp-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="foolst:cnmp-2"><a href="#foolst:cnmp-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNMP(nn.Module):</span>
<span id="foolst:cnmp-3"><a href="#foolst:cnmp-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="foolst:cnmp-4"><a href="#foolst:cnmp-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="foolst:cnmp-5"><a href="#foolst:cnmp-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CNMP, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="foolst:cnmp-6"><a href="#foolst:cnmp-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="foolst:cnmp-7"><a href="#foolst:cnmp-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encoder takes observations which are (X,Y) tuples and produces latent representations for each of them</span></span>
<span id="foolst:cnmp-8"><a href="#foolst:cnmp-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="foolst:cnmp-9"><a href="#foolst:cnmp-9" aria-hidden="true" tabindex="-1"></a>        nn.Linear(d_x<span class="op">+</span>d_y,<span class="dv">128</span>),nn.ReLU(),</span>
<span id="foolst:cnmp-10"><a href="#foolst:cnmp-10" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">128</span>,<span class="dv">128</span>),nn.ReLU(),</span>
<span id="foolst:cnmp-11"><a href="#foolst:cnmp-11" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">128</span>,<span class="dv">128</span>)</span>
<span id="foolst:cnmp-12"><a href="#foolst:cnmp-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="foolst:cnmp-13"><a href="#foolst:cnmp-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="foolst:cnmp-14"><a href="#foolst:cnmp-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">#Decoder takes the (r_mean, target_t) tuple and produces mean and std values for each dimension of the output</span></span>
<span id="foolst:cnmp-15"><a href="#foolst:cnmp-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="foolst:cnmp-16"><a href="#foolst:cnmp-16" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">128</span><span class="op">+</span>d_x,<span class="dv">128</span>),nn.ReLU(),</span>
<span id="foolst:cnmp-17"><a href="#foolst:cnmp-17" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">128</span>,<span class="dv">128</span>),nn.ReLU(),</span>
<span id="foolst:cnmp-18"><a href="#foolst:cnmp-18" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">128</span>,<span class="dv">2</span><span class="op">*</span>d_y)</span>
<span id="foolst:cnmp-19"><a href="#foolst:cnmp-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="foolst:cnmp-20"><a href="#foolst:cnmp-20" aria-hidden="true" tabindex="-1"></a>        </span></code></pre></div>
<p>The forward function built in <a href="#foolst:cnmp-forward-function"
data-reference-type="ref"
data-reference="lst:cnmp-forward-function">[lst:cnmp-forward-function]</a>
passes the observations provided to the encoder, then generates the mean of
all the 128 latent representations created. For all the queries, the
representation is concatenated to them and passed to the decoder. The decoder
produces the mean and the standard deviation for each target queried.</p>
<div class="sourceCode" id="foolst:cnmp-forward-function" data-float=""
data-language="Python" data-firstline="21" data-lastline="27"
data-caption="CNMP model forward function"
label="lst:cnmp-forward-function"><pre class="sourceCode python"><code class="sourceCode python"><span id="foolst:cnmp-forward-function-1"><a href="#foolst:cnmp-forward-function-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>,observations,target_t):</span>
<span id="foolst:cnmp-forward-function-2"><a href="#foolst:cnmp-forward-function-2" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> <span class="va">self</span>.encoder(observations) <span class="co"># Generating observations</span></span>
<span id="foolst:cnmp-forward-function-3"><a href="#foolst:cnmp-forward-function-3" aria-hidden="true" tabindex="-1"></a>        r_mean <span class="op">=</span> torch.mean(r,dim<span class="op">=</span><span class="dv">0</span>) <span class="co"># Taking mean and generating the general representation</span></span>
<span id="foolst:cnmp-forward-function-4"><a href="#foolst:cnmp-forward-function-4" aria-hidden="true" tabindex="-1"></a>        r_mean <span class="op">=</span> r_mean.repeat(target_t.shape[<span class="dv">0</span>],<span class="dv">1</span>) <span class="co"># Duplicating general representation for every target_t</span></span>
<span id="foolst:cnmp-forward-function-5"><a href="#foolst:cnmp-forward-function-5" aria-hidden="true" tabindex="-1"></a>        concat <span class="op">=</span> torch.cat((r_mean,target_t),dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># Concatenating each target_t with general representation</span></span>
<span id="foolst:cnmp-forward-function-6"><a href="#foolst:cnmp-forward-function-6" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.decoder(concat) <span class="co"># Producing mean and std values for each target_t</span></span>
<span id="foolst:cnmp-forward-function-7"><a href="#foolst:cnmp-forward-function-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
<p>Lastly, the loss function implemented is visible in <a
href="#foolst:cnmp-error-function" data-reference-type="ref"
data-reference="lst:cnmp-error-function">[lst:cnmp-error-function]</a> is the
implementation of the log probability loss explained in the <a
href="#foochap:background" data-reference-type="ref"
data-reference="chap:background">2</a>. The standard deviation values <span
class="math inline">\(\sigma\)</span> ("sigma" in the code at <em>line 3</em>)
are passed through the Softplus activation function. Softplus is a smooth
approximation of the ReLU function and ensures that the standard deviation
remains positive.</p>
<p>The following line creates a PyTorch distribution object. It specifies that
the distribution, with independent axes, is Normal with mean mean and standard
deviation sigma.</p>
<p>Finally, the negative log-likelihood of the target values given the
distribution (dist) is computed. This is used for backpropagation in network
training.</p>
<div class="sourceCode" id="foolst:cnmp-error-function" data-float=""
data-language="Python" data-firstline="29"
data-caption="CNMP model loss function for trainig"
label="lst:cnmp-error-function"><pre class="sourceCode python"><code class="sourceCode python"><span id="foolst:cnmp-error-function-1"><a href="#foolst:cnmp-error-function-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_prob_loss(output, target):</span>
<span id="foolst:cnmp-error-function-2"><a href="#foolst:cnmp-error-function-2" aria-hidden="true" tabindex="-1"></a>    mean, sigma <span class="op">=</span> output.chunk(<span class="dv">2</span>, dim <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="foolst:cnmp-error-function-3"><a href="#foolst:cnmp-error-function-3" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> F.softplus(sigma)</span>
<span id="foolst:cnmp-error-function-4"><a href="#foolst:cnmp-error-function-4" aria-hidden="true" tabindex="-1"></a>    dist <span class="op">=</span> D.Independent(D.Normal(loc<span class="op">=</span>mean, scale<span class="op">=</span>sigma), <span class="dv">1</span>)</span>
<span id="foolst:cnmp-error-function-5"><a href="#foolst:cnmp-error-function-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>torch.mean(dist.log_prob(target))</span></code></pre></div>
<section id="foopartial-skill-combination-1" class="level2" data-number="5.1">
<h2 data-number="5.1"><span class="header-section-number">5.1</span> Partial
Skill Combination</h2>
<p>In this section, the concept of task parameters was introduced. At the
implementation level, it doesn&#x2019;t change the previously explained network
structure since the dimension of the input will be increased, for example by
one: <span class="math inline">\(d\_x = 2\)</span>, and the code will adapt
the network structure.</p>
<p>An example of input for a query will be:</p>
<p><span class="math inline">\(tensor([[0.1357, 1.0]],
dtype=torch.float64)\)</span>.</p>
<p>Finally, an observation will be in the form of <span
class="math inline">\([0.6],[1.0],[0.45]\)</span>, where the two dimensions of
the input (note the <span class="math inline">\(1.0\)</span> as TP) are
concatenated with the output desired.</p>
<p>Another crucial passage in the <a href="#foochap:design"
data-reference-type="ref" data-reference="chap:design">4</a> was to vary
independently the two task parameters of the observations and the queries in
order to build the visualization of their influence in <a
href="#foofig:tp-influence" data-reference-type="ref"
data-reference="fig:tp-influence">4.5</a>.</p>
<p>To achieve this, a simple matrix of observations and queries was built,
where each row contains observations and a time query to the network.
Subsequently, the matrix was edited with constant task parameters in the
observations, letting the TP vary in the queries, in order to build the plot.
The opposite was implemented in order to build the plot where the task
parameters vary in the observations and are constant in the queries.</p>
<section id="foocnmp-model-with-task-parameter-only-in-condition-1"
class="level3" data-number="5.1.1">
<h3 data-number="5.1.1"><span class="header-section-number">5.1.1</span> CNMP
model with task-parameter only in condition</h3>
<p>In this part, a new architecture is implemented, as proposed in <a
href="#foochap:design" data-reference-type="ref"
data-reference="chap:design">4</a>, based on the previously explained CNMP
model.</p>
<p>Among the edits in the code, one of the most important ones is the change
of the model itself in the definition seen previously in <a
href="#foolst:cnmp" data-reference-type="ref"
data-reference="lst:cnmp">[lst:cnmp]</a>.</p>
<div class="sourceCode" id="foolst:cnmp-just-condition" data-float=""
data-language="Python" data-firstline="0" data-lastline="7"
data-caption="CNMP model architecture change for TP only in conditions"
label="lst:cnmp-just-condition"><pre class="sourceCode python"><code class="sourceCode python"><span id="foolst:cnmp-just-condition-1"><a href="#foolst:cnmp-just-condition-1" aria-hidden="true" tabindex="-1"></a>        <span class="co">#Decoder takes the (r_mean, target_t) tuple and produces mean and std values for each dimension of the output</span></span>
<span id="foolst:cnmp-just-condition-2"><a href="#foolst:cnmp-just-condition-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="foolst:cnmp-just-condition-3"><a href="#foolst:cnmp-just-condition-3" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">128</span><span class="op">+</span>d_x<span class="op">-</span>d_TP,<span class="dv">128</span>),nn.ReLU(), <span class="co">#edited here to pass only x without tp</span></span>
<span id="foolst:cnmp-just-condition-4"><a href="#foolst:cnmp-just-condition-4" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">128</span>,<span class="dv">128</span>),nn.ReLU(),</span>
<span id="foolst:cnmp-just-condition-5"><a href="#foolst:cnmp-just-condition-5" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">128</span>,<span class="dv">2</span><span class="op">*</span>d_y)</span>
<span id="foolst:cnmp-just-condition-6"><a href="#foolst:cnmp-just-condition-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="foolst:cnmp-just-condition-7"><a href="#foolst:cnmp-just-condition-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="foolst:cnmp-just-condition-8"><a href="#foolst:cnmp-just-condition-8" aria-hidden="true" tabindex="-1"></a>predicted_Y,predicted_std <span class="op">=</span> predict_model(np.array([np.concatenate((v_X[i,<span class="dv">0</span>],v_Y[i,<span class="dv">0</span>]))]), np.delete(v_X[i], <span class="op">-</span>d_TP, axis<span class="op">=</span><span class="dv">1</span>), plot<span class="op">=</span> <span class="va">False</span>) <span class="co">#edited here to pass v_X only with first element and not tp</span></span></code></pre></div>
<p>As is visible in the <a href="#foolst:cnmp-just-condition"
data-reference-type="ref"
data-reference="lst:cnmp-just-condition">[lst:cnmp-just-condition]</a>, while
the encoder stays the same, the decoder has input dimensions reduced. The
subtraction is at <em>line 3</em>, after the concatenation of the latent
representation&#x2019;s 128 dimensions and the <span
class="math inline">\(d\_x\)</span> input dimensions, where <span
class="math inline">\(d\_TP\)</span> is the number of dimensions of the task
parameters.</p>
<div class="sourceCode" id="foolst:cnmp-just-condition-training" data-float=""
data-language="Python" data-firstline="8"
data-caption="CNMP training code change for TP only in conditions"
label="lst:cnmp-just-condition-training"><pre
class="sourceCode python"><code class="sourceCode python"><span id="foolst:cnmp-just-condition-training-1"><a href="#foolst:cnmp-just-condition-training-1" aria-hidden="true" tabindex="-1"></a>predicted_Y,predicted_std <span class="op">=</span> predict_model(np.array([np.concatenate((v_X[i,<span class="dv">0</span>],v_Y[i,<span class="dv">0</span>]))]), np.delete(v_X[i], <span class="op">-</span>d_TP, axis<span class="op">=</span><span class="dv">1</span>), plot<span class="op">=</span> <span class="va">False</span>) <span class="co">#edited here to pass v_X only with first element and not tp</span></span></code></pre></div>
<p>Another fundamental change in the process is the input at the prediction
time. The <a href="#foolst:cnmp-just-condition-training"
data-reference-type="ref"
data-reference="lst:cnmp-just-condition-training">[lst:cnmp-just-condition-training]</a>,
taken from the prediction during training, shows the normal conditions in the
first part of the function. In the second argument, the queries are passed.
These are in the form of a matrix with inputs and task parameters in the
columns, and the rows are the times.</p>
<p>In the new version, it is possible to see how the matrix is deprived by the
last <span class="math inline">\(d\_x\)</span> columns containing the task
parameters. The input passed to the network corresponds indeed to the
dimensionality of the decoder designed before in the model.</p>
<p>This allows to query the network simply with an array of times <span
class="math inline">\(t\)</span> desired.</p>
</section>
<section id="foocnmp-model-with-task-parameter-only-in-query-1" class="level3"
data-number="5.1.2">
<h3 data-number="5.1.2"><span class="header-section-number">5.1.2</span> CNMP
model with task-parameter only in query</h3>
<p>In this part, another new architecture is proposed based on the previously
explained CNMP model, as the opposite of the one presented in the previous
part.</p>
<p>To maintain the comparison between all the code developed, the most
important changes presented here will follow in parallel the previous
part.</p>
<p>One of the most important changes is again the model architecture itself,
seen previously in <a href="#foolst:cnmp" data-reference-type="ref"
data-reference="lst:cnmp">[lst:cnmp]</a>.</p>
<div class="sourceCode" id="foolst:cnmp-just-query" data-float=""
data-language="Python" data-firstline="0" data-lastline="7"
data-caption="CNMP model architecture change for TP only in query"
label="lst:cnmp-just-query"><pre class="sourceCode python"><code class="sourceCode python"><span id="foolst:cnmp-just-query-1"><a href="#foolst:cnmp-just-query-1" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encoder takes observations which are (X,Y) tuples and produces latent representations for each of them</span></span>
<span id="foolst:cnmp-just-query-2"><a href="#foolst:cnmp-just-query-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="foolst:cnmp-just-query-3"><a href="#foolst:cnmp-just-query-3" aria-hidden="true" tabindex="-1"></a>        nn.Linear(d_x<span class="op">-</span>d_TP<span class="op">+</span>d_y,<span class="dv">128</span>),nn.ReLU(), <span class="co">#edited here to pass only x without tp</span></span>
<span id="foolst:cnmp-just-query-4"><a href="#foolst:cnmp-just-query-4" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">128</span>,<span class="dv">128</span>),nn.ReLU(),</span>
<span id="foolst:cnmp-just-query-5"><a href="#foolst:cnmp-just-query-5" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">128</span>,<span class="dv">128</span>)</span>
<span id="foolst:cnmp-just-query-6"><a href="#foolst:cnmp-just-query-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="foolst:cnmp-just-query-7"><a href="#foolst:cnmp-just-query-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="foolst:cnmp-just-query-8"><a href="#foolst:cnmp-just-query-8" aria-hidden="true" tabindex="-1"></a>predicted_Y,predicted_std <span class="op">=</span> predict_model(np.array([np.concatenate((np.delete(v_X[i,<span class="dv">0</span>], <span class="op">-</span>d_TP, axis<span class="op">=</span><span class="dv">0</span>),v_Y[i,<span class="dv">0</span>]))]), v_X[i], plot<span class="op">=</span> <span class="va">False</span>) <span class="co">#edited here to pass only with first element and not tp</span></span></code></pre></div>
<p>As it is possible to see in <a href="#foolst:cnmp-just-query"
data-reference-type="ref"
data-reference="lst:cnmp-just-query">[lst:cnmp-just-query]</a>, the decoder
this time was untouched, but the encoder was modified to accommodate different
dimensions in his input. The dimensionality of the first layer of the network
was reduced by the number of task parameter dimensions. At <em>line 3</em>,
the dimensions of the task parameter <span
class="math inline">\(d\_TP\)</span> are subtracted from the dimensions of the
observations <span class="math inline">\(d\_x+d\_y\)</span>.</p>
<div class="sourceCode" id="foolst:cnmp-just-query-training" data-float=""
data-language="Python" data-firstline="8"
data-caption="CNMP training code change for TP only in query"
label="lst:cnmp-just-query-training"><pre
class="sourceCode python"><code class="sourceCode python"><span id="foolst:cnmp-just-query-training-1"><a href="#foolst:cnmp-just-query-training-1" aria-hidden="true" tabindex="-1"></a>predicted_Y,predicted_std <span class="op">=</span> predict_model(np.array([np.concatenate((np.delete(v_X[i,<span class="dv">0</span>], <span class="op">-</span>d_TP, axis<span class="op">=</span><span class="dv">0</span>),v_Y[i,<span class="dv">0</span>]))]), v_X[i], plot<span class="op">=</span> <span class="va">False</span>) <span class="co">#edited here to pass only with first element and not tp</span></span></code></pre></div>
<p>Similarly to the previous architecture, another fundamental change in the
process is the input at the prediction time. The <a
href="#foolst:cnmp-just-query-training" data-reference-type="ref"
data-reference="lst:cnmp-just-query-training">[lst:cnmp-just-query-training]</a>,
taken from the prediction during training, shows this time the normal queries
<span class="math inline">\(v_X [ i ]\)</span> in the second part of the
function. In the first argument, though, the observations are passed in a
different way. These are also in the form of a vector with inputs, outputs,
and task parameters concatenated in order.</p>
<p>In the model implemented, the observations are deprived of the last <span
class="math inline">\(d\_x\)</span> columns of the input relative to their
task parameters. Now, the observations passed to the network correspond to the
dimensionality of the first layer of the decoder designed.</p>
<p>This allows conditioning the network simply with conditions that are
parameterless.</p>
</section>
<section id="foocomparison-of-the-previous-models-1" class="level3"
data-number="5.1.3">
<h3 data-number="5.1.3"><span class="header-section-number">5.1.3</span>
Comparison of the previous models</h3>
<p>For the comparisons among models developed, a brief extract of code is
reported in <a href="#foolst:cnmp-comparison" data-reference-type="ref"
data-reference="lst:cnmp-comparison">[lst:cnmp-comparison]</a>. Using the
<em>numpy</em> library, the real values are compared to the predicted
ones.</p>
<p>Different metrics have been used. Initially, the Mean Squared Error (MSE)
is calculated. It measures the average squared difference between the actual
(<span class="math inline">\(real\_values\)</span>) and predicted values
(<span class="math inline">\(predicted\_Y\)</span>). It penalizes larger
errors more severely due to the squaring operation. Since the errors produced
were minimal, this is the metric chosen and reported in <a
href="#footab:CNMP3vs" data-reference-type="ref"
data-reference="tab:CNMP3vs">[tab:CNMP3vs]</a>.</p>
<p>Subsequently, also the Mean Absolute Error (MAE) is computed at <em>line
3</em>. It provides the average absolute difference between the actual and
predicted values.</p>
<p>Lastly, the Root Mean Squared Error (RMSE) is calculated on the basis of
this one since it is the square root of the Mean Absolute Error. These metrics
were chosen as they are the most commonly used in regression analysis.</p>
<div class="sourceCode" id="foolst:cnmp-comparison" data-float=""
data-language="Python" data-caption="Extract of code for comparison of models"
label="lst:cnmp-comparison"><pre class="sourceCode python"><code class="sourceCode python"><span id="foolst:cnmp-comparison-1"><a href="#foolst:cnmp-comparison-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="foolst:cnmp-comparison-2"><a href="#foolst:cnmp-comparison-2" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> np.mean((real_values <span class="op">-</span> predicted_Y)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="foolst:cnmp-comparison-3"><a href="#foolst:cnmp-comparison-3" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> np.mean(np.<span class="bu">abs</span>(real_values <span class="op">-</span> predicted_Y))</span>
<span id="foolst:cnmp-comparison-4"><a href="#foolst:cnmp-comparison-4" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mse)</span></code></pre></div>
</section>
<section id="foocnmp-changing-task-in-time-with-one-conditioning-point-1"
class="level3" data-number="5.1.4">
<h3 data-number="5.1.4"><span class="header-section-number">5.1.4</span> CNMP
changing task in time with one conditioning point</h3>
<p>In this part is present a possible implementation of the method proposed to
shift the execution of a skill in time.</p>
<p>For the sake of simplicity, the model used was the proposed CNMP without
task parameters in the query. This does not imply that the method does not
work with the other CNMP architectures analyzed.</p>
<p>The method is a mathematical way to feed the model with the right
conditions, so it is possible to implement it on other networks for custom
needs or for better performance.</p>
<p>Implementations for the other architectures were also developed but, for
simplicity and avoiding repetitions, only one will be explained.</p>
<p>As depicted in <a href="#foofig:tpshift" data-reference-type="ref"
data-reference="fig:tpshift">4.11</a>, the shift implemented is linear, and,
given an observation, goes from one task parameter to the other. In <a
href="#foolst:shift-tp-one-condition" data-reference-type="ref"
data-reference="lst:shift-tp-one-condition">[lst:shift-tp-one-condition]</a>,
a matrix of times linearly changing task parameters and values is created. The
values column is empty and will be filled later.</p>
<p>Subsequently, the model is queried for every timestep, with the TP changing
<em>line 7</em>. Finally, values predicted are inserted in the third column of
the matrix for every time step at <em>line 10</em>.</p>
<p>At the end, the resulting matrix will have all the data required. The
plotting process visible on the right in <a href="#foofig:tpshift"
data-reference-type="ref" data-reference="fig:tpshift">4.11</a> will not be
described here cause it is not necessary, but it was achieved using the
<em>Matplotlib</em> python library.</p>
<div class="sourceCode" id="foolst:shift-tp-one-condition" data-float=""
data-language="Python"
data-caption="Extract of code for task shift with one observation"
label="lst:shift-tp-one-condition"><pre class="sourceCode python"><code class="sourceCode python"><span id="foolst:shift-tp-one-condition-1"><a href="#foolst:shift-tp-one-condition-1" aria-hidden="true" tabindex="-1"></a>mixed_tp<span class="op">=</span>graph[<span class="dv">0</span>].copy() <span class="co">#first column time</span></span>
<span id="foolst:shift-tp-one-condition-2"><a href="#foolst:shift-tp-one-condition-2" aria-hidden="true" tabindex="-1"></a>mixed_tp[:,<span class="dv">1</span>]<span class="op">=</span>np.linspace(<span class="dv">2</span>,<span class="dv">1</span>,mixed_tp.shape[<span class="dv">0</span>]) <span class="co">#second column tp changing</span></span>
<span id="foolst:shift-tp-one-condition-3"><a href="#foolst:shift-tp-one-condition-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="foolst:shift-tp-one-condition-4"><a href="#foolst:shift-tp-one-condition-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index, el <span class="kw">in</span> <span class="bu">enumerate</span>(mixed_tp):</span>
<span id="foolst:shift-tp-one-condition-5"><a href="#foolst:shift-tp-one-condition-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="foolst:shift-tp-one-condition-6"><a href="#foolst:shift-tp-one-condition-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;now time </span><span class="sc">%f</span><span class="st"> and TP </span><span class="sc">%f</span><span class="st">&quot;</span><span class="op">%</span>(el[<span class="dv">0</span>], el[<span class="dv">1</span>]))</span>
<span id="foolst:shift-tp-one-condition-7"><a href="#foolst:shift-tp-one-condition-7" aria-hidden="true" tabindex="-1"></a>        pred_y <span class="op">=</span> model( torch.from_numpy(np.array([np.concatenate(([<span class="fl">0.5</span>],[el[<span class="dv">1</span>]],[<span class="fl">0.6</span>]))])) , </span>
<span id="foolst:shift-tp-one-condition-8"><a href="#foolst:shift-tp-one-condition-8" aria-hidden="true" tabindex="-1"></a>        torch.from_numpy(np.array([[el[<span class="dv">0</span>]]])) ).numpy()</span>
<span id="foolst:shift-tp-one-condition-9"><a href="#foolst:shift-tp-one-condition-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(pred_y[<span class="dv">0</span>][<span class="dv">0</span>]) <span class="co">#  value returned</span></span>
<span id="foolst:shift-tp-one-condition-10"><a href="#foolst:shift-tp-one-condition-10" aria-hidden="true" tabindex="-1"></a>        mixed_tp[index,<span class="dv">2</span>]<span class="op">=</span>pred_y[<span class="dv">0</span>][<span class="dv">0</span>] <span class="co"># fill 3rd column</span></span>
<span id="foolst:shift-tp-one-condition-11"><a href="#foolst:shift-tp-one-condition-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mixed_tp)</span></code></pre></div>
</section>
<section
id="foocnmp-changing-task-in-time-with-multiple-conditioning-points-1"
class="level3" data-number="5.1.5">
<h3 data-number="5.1.5"><span class="header-section-number">5.1.5</span> CNMP
changing task in time with multiple conditioning points</h3>
<p>Below are proposed key parts of the final implementation for changing
multiple times tasks in the same prediction time span.</p>
<p>There are two key functions for achieving this result: the first is
dedicated to building a matrix of data, and the second one has as duty the use
of this matrix to generate the predictions.</p>
<section
id="foobuilding-the-matrix-of-timestep-and-observation-for-the-timesteps."
class="level5" data-number="5.1.5.0.1">
<h5 data-number="5.1.5.0.1"><span
class="header-section-number">5.1.5.0.1</span> Building the matrix of timestep
and observation for the timesteps. </h5>
<p>The first key part is building a matrix that defines how the conditioning
point will move in time and task space. This function is responsible for the
results visible in the <a href="#foofig:tp-multiple-shift"
data-reference-type="ref" data-reference="fig:tp-multiple-shift">4.14</a> and
<a href="#foofig:tp-multiple-shift-condition" data-reference-type="ref"
data-reference="fig:tp-multiple-shift-condition">4.15</a>.</p>
<p>The function receives the observation list, with time as the first
dimension, and returns the matrix with the observation for every timestep.
Initially, the observations are sorted based on their time. The matrix is
defined with queries of time (in the first position) concatenated to only one
observation at that specific time. The matrix has dimensions <span
class="math inline">\(1+d\_X+d\_Y\)</span>, recalling that <span
class="math inline">\(d\_X\)</span> includes the <span
class="math inline">\(d\_TP\)</span> dimensionality of the task
parameters.</p>
<p>Subsequently, the action will be repeated for every timestep. The loop also
keeps track of the current and next observation in time, starting from the
first. Some optimizations were performed not to calculate the same couples of
closest points every time, but won&#x2019;t be reported here for shortness.</p>
<p>If the current time is less than the observation time or this is the last
observation, the function fills the current timestep of the matrix with the
current observation. This takes care of the initial and final periods.</p>
<p>If the timestep is in between two observations, the core part happens. The
code implementation should check the whole predictions in time of the two
observations, finding the two closest points. In the code extract, this part
is visible in <a href="#foolst:shift-tp-more-conditions-build-matrixPT1"
data-reference-type="ref"
data-reference="lst:shift-tp-more-conditions-build-matrixPT1">[lst:shift-tp-more-conditions-build-matrixPT1]</a>.
This operation can be done once per couple of observations.</p>
<div class="sourceCode" id="foolst:shift-tp-more-conditions-build-matrixPT1"
data-float="" data-language="Python" data-firstline="6" data-lastline="9"
data-caption="Extract of code for task shift with multiple observations, couple of observation finding"
label="lst:shift-tp-more-conditions-build-matrixPT1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="foolst:shift-tp-more-conditions-build-matrixPT1-1"><a href="#foolst:shift-tp-more-conditions-build-matrixPT1-1" aria-hidden="true" tabindex="-1"></a>pred_1ob <span class="op">=</span> model( torch.from_numpy(np.array([observations[curr_obs_num,:]])) , torch.from_numpy(times_array.reshape(<span class="dv">200</span>,<span class="dv">1</span>)) ).numpy()</span>
<span id="foolst:shift-tp-more-conditions-build-matrixPT1-2"><a href="#foolst:shift-tp-more-conditions-build-matrixPT1-2" aria-hidden="true" tabindex="-1"></a>pred_2ob <span class="op">=</span> model( torch.from_numpy(np.array([observations[next_obs_num,:]])) , torch.from_numpy(times_array.reshape(<span class="dv">200</span>,<span class="dv">1</span>)) ).numpy()</span>
<span id="foolst:shift-tp-more-conditions-build-matrixPT1-3"><a href="#foolst:shift-tp-more-conditions-build-matrixPT1-3" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> <span class="bu">abs</span>(pred_1ob[:,<span class="dv">0</span>]<span class="op">-</span>pred_2ob[:,<span class="dv">0</span>]) <span class="co">#computes the couples distances on the y predicted</span></span>
<span id="foolst:shift-tp-more-conditions-build-matrixPT1-4"><a href="#foolst:shift-tp-more-conditions-build-matrixPT1-4" aria-hidden="true" tabindex="-1"></a>index_min<span class="op">=</span>np.argmin(diff) <span class="co"># saves the closest couple</span></span></code></pre></div>
<p>Once the closest couple of points is found in the trajectories, the points
among them will not bias the task parameter change. For this reason, the start
and end obtained will be interpolated in the time period of the transition.
The final interpolated point obtained for this timestep is the desired
observation with the non-biasing position and the mixed TP. In the <a
href="#foolst:shift-tp-more-conditions-build-matrixPT2"
data-reference-type="ref"
data-reference="lst:shift-tp-more-conditions-build-matrixPT2">[lst:shift-tp-more-conditions-build-matrixPT2]</a>,
it is possible to see how, in the current timestep, the interpolation is
executed among the two previously defined points. The resulting conditioning
point is inserted in the matrix after the current timestep.</p>
<div class="sourceCode" id="foolst:shift-tp-more-conditions-build-matrixPT2"
data-float="" data-language="Python" data-firstline="14" data-lastline="17"
data-caption="Extract of code for task shift with multiple observations, couple of observation interpolating"
label="lst:shift-tp-more-conditions-build-matrixPT2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="foolst:shift-tp-more-conditions-build-matrixPT2-1"><a href="#foolst:shift-tp-more-conditions-build-matrixPT2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the fraction of interpolation</span></span>
<span id="foolst:shift-tp-more-conditions-build-matrixPT2-2"><a href="#foolst:shift-tp-more-conditions-build-matrixPT2-2" aria-hidden="true" tabindex="-1"></a>fraction <span class="op">=</span> (curr_time <span class="op">-</span> observations[curr_obs_num,<span class="dv">0</span>]) <span class="op">/</span> (observations[next_obs_num,<span class="dv">0</span>] <span class="op">-</span> observations[curr_obs_num,<span class="dv">0</span>])</span>
<span id="foolst:shift-tp-more-conditions-build-matrixPT2-3"><a href="#foolst:shift-tp-more-conditions-build-matrixPT2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#interpolated_param = start_param + fraction * (end_param - start_param)</span></span>
<span id="foolst:shift-tp-more-conditions-build-matrixPT2-4"><a href="#foolst:shift-tp-more-conditions-build-matrixPT2-4" aria-hidden="true" tabindex="-1"></a>observation_timestep_matrix[time_index,<span class="dv">1</span><span class="op">+</span>i]<span class="op">=</span>start_param[i]<span class="op">+</span> fraction <span class="op">*</span> (end_param[i] <span class="op">-</span> start_param[i])</span></code></pre></div>
<p>The plottings of the matrix depicted in <a href="#foofig:tp-multiple-shift"
data-reference-type="ref" data-reference="fig:tp-multiple-shift">4.14</a> and
<a href="#foofig:tp-multiple-shift-condition" data-reference-type="ref"
data-reference="fig:tp-multiple-shift-condition">4.15</a> are achieved again
with the <em>Matplotlib</em> python library. The relative code will not be
reported as well because it&#x2019;s not necessary for the sake of the result but
only to visualize the correctness of the procedure.</p>
</section>
<section
id="foobuilding-the-function-to-query-the-network-with-the-matrix-obtained."
class="level5" data-number="5.1.5.0.2">
<h5 data-number="5.1.5.0.2"><span
class="header-section-number">5.1.5.0.2</span> Building the function to query
the network with the matrix obtained. </h5>
<p>The second key part uses the matrix built before to query the network at
every timestep with the changing observations. It is responsible for the final
results obtained and visualized in <a href="#foofig:tp-multiple-shift-results"
data-reference-type="ref"
data-reference="fig:tp-multiple-shift-results">4.16</a>.</p>
<p>This part, instead of passing to the network the array of multiple
observations, passes the computed array with a single observation of every
time step. In the <a href="#foolst:shift-tp-more-conditions-function"
data-reference-type="ref"
data-reference="lst:shift-tp-more-conditions-function">[lst:shift-tp-more-conditions-function]</a>,
it is possible to see how the model is fed with the observation at <span
class="math inline">\(time\_index\)</span> and the current <span
class="math inline">\(time\)</span>. The matrix previously built is split in
the array <span class="math inline">\(matirx\_time\)</span> (first column of
the matrix), and the matrix of observations for every time step <span
class="math inline">\(matrix\_observations\)</span>.</p>
<div class="sourceCode" id="foolst:shift-tp-more-conditions-function"
data-float="" data-language="Python"
data-caption="Extract of code for task shift with multiple observations, using the matrix obtained"
label="lst:shift-tp-more-conditions-function"><pre
class="sourceCode python"><code class="sourceCode python"><span id="foolst:shift-tp-more-conditions-function-1"><a href="#foolst:shift-tp-more-conditions-function-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> time_index, time <span class="kw">in</span> <span class="bu">enumerate</span>(matrix_times):</span>
<span id="foolst:shift-tp-more-conditions-function-2"><a href="#foolst:shift-tp-more-conditions-function-2" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> model( torch.from_numpy(np.array([ matrix_observations[time_index] ])) , torch.from_numpy(np.array([[time]])) ).numpy()</span>
<span id="foolst:shift-tp-more-conditions-function-3"><a href="#foolst:shift-tp-more-conditions-function-3" aria-hidden="true" tabindex="-1"></a>    predicted_Y[time_index] <span class="op">=</span> prediction[:,:d_y]</span>
<span id="foolst:shift-tp-more-conditions-function-4"><a href="#foolst:shift-tp-more-conditions-function-4" aria-hidden="true" tabindex="-1"></a>    predicted_std[time_index] <span class="op">=</span> np.log(<span class="dv">1</span><span class="op">+</span>np.exp(prediction[:,d_y:]))</span></code></pre></div>
<p>The predicted output value is saved in an array and plotted, as in the
results in <a href="#foofig:tp-multiple-shift-results"
data-reference-type="ref"
data-reference="fig:tp-multiple-shift-results">4.16</a>.</p>
</section>
</section>
</section>
<section id="fooend-to-end-skill-concatenation-1" class="level2"
data-number="5.2">
<h2 data-number="5.2"><span class="header-section-number">5.2</span>
End-To-End Skill Concatenation</h2>
<p>In this section, the key parts of the implementation for concatenating
trajectories are introduced. The training of the network on the actions
performed has been already explained, so it will be skipped.</p>
<p>There are two important parts worth highlighting: the first one is the
construction of the graph of all possible actions, and the second one is the
filtering of the ones that are not executable.</p>
<section id="foobuilding-the-graph-of-concatenated-trajectories"
class="level3" data-number="5.2.1">
<h3 data-number="5.2.1"><span class="header-section-number">5.2.1</span>
Building the graph of concatenated trajectories</h3>
<p>The first key part is building the graph of the whole possible trajectories
obtained from a starting point and combining multiple skills. This code is
responsible for the visualization of the results in the <a
href="#foofig:diff-traj-concat" data-reference-type="ref"
data-reference="fig:diff-traj-concat">4.20</a>.</p>
<p>Initially, as possible to see in the <a href="#foolst:concat-build-graph1"
data-reference-type="ref"
data-reference="lst:concat-build-graph1">[lst:concat-build-graph1]</a>, the
empty structure is defined, row by row will be added later. The final number
of trajectories is calculated as explained in <a href="#foochap:design"
data-reference-type="ref" data-reference="chap:design">4</a> elevating the
number of actions available to the power of the number of time they can be
concatenated. This will also determine the full time lenght of the
execution.</p>
<div class="sourceCode" id="foolst:concat-build-graph1" data-float=""
data-language="Python" data-firstline="0" data-lastline="4"
data-caption="Extract of code for building the graph of skills concatenated"
label="lst:concat-build-graph1"><pre class="sourceCode python"><code class="sourceCode python"><span id="foolst:concat-build-graph1-1"><a href="#foolst:concat-build-graph1-1" aria-hidden="true" tabindex="-1"></a>num_traj<span class="op">=</span><span class="bu">len</span>(params_array)<span class="op">**</span>(max_depth<span class="op">+</span><span class="dv">1</span>)</span>
<span id="foolst:concat-build-graph1-2"><a href="#foolst:concat-build-graph1-2" aria-hidden="true" tabindex="-1"></a>lengh_time<span class="op">=</span>time_steps<span class="op">*</span>(max_depth<span class="op">+</span><span class="dv">1</span>)</span>
<span id="foolst:concat-build-graph1-3"><a href="#foolst:concat-build-graph1-3" aria-hidden="true" tabindex="-1"></a>full_graph <span class="op">=</span> np.zeros((<span class="dv">0</span>,lengh_time)) <span class="co"># initially empty</span></span>
<span id="foolst:concat-build-graph1-4"><a href="#foolst:concat-build-graph1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="foolst:concat-build-graph1-5"><a href="#foolst:concat-build-graph1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># prevision, given a starting point</span></span></code></pre></div>
<p>Next, the recursive function will take care of filling the graph for each
step, it&#x2019;s possible to see the most important lines in the <a
href="#foolst:concat-build-graph2" data-reference-type="ref"
data-reference="lst:concat-build-graph2">[lst:concat-build-graph2]</a>.</p>
<p>The model is queried for every action given a conditioning point based on
the result of the previous one. The starting point and end point of the
actions predicted are visualized, then the predictions are added at the end of
the new line to add to the graph. Finally, for every action still, the
function will call itself again. The parameter new line this time has now all
the actions preditions in the end.</p>
<div class="sourceCode" id="foolst:concat-build-graph2" data-float=""
data-language="Python" data-firstline="5"
data-caption="Extract of code for filling the graph of skills concatenated"
label="lst:concat-build-graph2"><pre class="sourceCode python"><code class="sourceCode python"><span id="foolst:concat-build-graph2-1"><a href="#foolst:concat-build-graph2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prevision, given a starting point</span></span>
<span id="foolst:concat-build-graph2-2"><a href="#foolst:concat-build-graph2-2" aria-hidden="true" tabindex="-1"></a>predicted_Ys,predicted_std<span class="op">=</span>predict_model(np.array([np.concatenate(([<span class="fl">0.0</span>],params_array[i],[starting_pt]))]),time_queries)</span>
<span id="foolst:concat-build-graph2-3"><a href="#foolst:concat-build-graph2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="sc">%.2f</span><span class="st"> to </span><span class="sc">%.2f</span><span class="st">&quot;</span><span class="op">%</span>(predicted_Ys[<span class="dv">0</span>],predicted_Ys[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="foolst:concat-build-graph2-4"><a href="#foolst:concat-build-graph2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;filling form </span><span class="sc">%i</span><span class="st"> to </span><span class="sc">%i</span><span class="st">&quot;</span><span class="op">%</span>(current_depth<span class="op">*</span>time_queries.shape[<span class="dv">0</span>],(current_depth<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>time_queries.shape[<span class="dv">0</span>]))</span>
<span id="foolst:concat-build-graph2-5"><a href="#foolst:concat-build-graph2-5" aria-hidden="true" tabindex="-1"></a><span class="co">#modify line</span></span>
<span id="foolst:concat-build-graph2-6"><a href="#foolst:concat-build-graph2-6" aria-hidden="true" tabindex="-1"></a>new_line[ <span class="dv">0</span>, current_depth<span class="op">*</span>time_queries.shape[<span class="dv">0</span>]:(current_depth<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>time_queries.shape[<span class="dv">0</span>] ] <span class="op">=</span> predicted_Ys.T</span>
<span id="foolst:concat-build-graph2-7"><a href="#foolst:concat-build-graph2-7" aria-hidden="true" tabindex="-1"></a><span class="co">#recursive call to further modify</span></span>
<span id="foolst:concat-build-graph2-8"><a href="#foolst:concat-build-graph2-8" aria-hidden="true" tabindex="-1"></a>full_graph<span class="op">=</span>add_trajectories(predicted_Ys[<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>], current_depth<span class="op">+</span><span class="dv">1</span>, max_depth, full_graph, new_line)</span></code></pre></div>
<p>If the end time is reached the new line containing the new trajectory
generated is added to the graph. No more recursive calls will be performed in
this case.</p>
</section>
<section id="foopruning-the-graph-of-concatenated-trajectories" class="level3"
data-number="5.2.2">
<h3 data-number="5.2.2"><span class="header-section-number">5.2.2</span>
Pruning the graph of concatenated trajectories</h3>
<p>The second important part of the method is to remove the movement
primitives that are unfeasable or unmenaingful. To remove these trajectories
the jumps in the trajectories that exceed a certain threshold will make the
trajectory discarded.</p>
<p>In <a href="#foolst:concat-prune-graph" data-reference-type="ref"
data-reference="lst:concat-prune-graph">[lst:concat-prune-graph]</a> is
possible to see how the graph is passed to the function and a new one empty is
built. For each sequence in the original graph the differences among the
elements are computed as an array.</p>
<p>Next, the array converted to positive values with the absoulte
function.</p>
<p>If none of these differences exceed the threshold desired, the sequence is
added to the graph of valid sequences.</p>
<div class="sourceCode" id="foolst:concat-prune-graph" data-float=""
data-language="Python"
data-caption="Extract of code for filling the graph of skills concatenated"
label="lst:concat-prune-graph"><pre class="sourceCode python"><code class="sourceCode python"><span id="foolst:concat-prune-graph-1"><a href="#foolst:concat-prune-graph-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_sequences_with_large_jumps(graph, threshold):</span>
<span id="foolst:concat-prune-graph-2"><a href="#foolst:concat-prune-graph-2" aria-hidden="true" tabindex="-1"></a>    valid_sequences <span class="op">=</span> []</span>
<span id="foolst:concat-prune-graph-3"><a href="#foolst:concat-prune-graph-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sequence <span class="kw">in</span> graph:</span>
<span id="foolst:concat-prune-graph-4"><a href="#foolst:concat-prune-graph-4" aria-hidden="true" tabindex="-1"></a>        diffs <span class="op">=</span> np.<span class="bu">abs</span>(np.diff(sequence))</span>
<span id="foolst:concat-prune-graph-5"><a href="#foolst:concat-prune-graph-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> np.<span class="bu">any</span>(diffs <span class="op">&gt;</span> threshold):</span>
<span id="foolst:concat-prune-graph-6"><a href="#foolst:concat-prune-graph-6" aria-hidden="true" tabindex="-1"></a>            valid_sequences.append(sequence)</span></code></pre></div>
</section>
</section>
</section>
<section id="foochap:validation" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Validation
and Testing</h1>
<p>This chapter presents the validation and testing of the methods that were
before designed and then implemented in this research. The accuracy of the
results has already been discussed with the error comparison tables in the <a
href="#foochap:design" data-reference-type="ref"
data-reference="chap:design">4</a>. To demonstrate the applicability of this
research, instead, the methods are tested on real-life robots, <a
href="#foochap:platforms" data-reference-type="ref"
data-reference="chap:platforms">3</a>.</p>
<p>To achieve these results, some minor adaptations had to be made. For
example, the number of input or output dimensions of the network changed to
accommodate the joint space of the robots since the monodimensional example
given in the design is not enough.</p>
<p>Furthermore, the time scale for the movement primitives has been changed to
a longer time period. The trajectories generated if in cartesian space, were
passed to an inverse kinematic algorithm to retrieve the joint space.</p>
<p>Overall, these variations don&#x2019;t impact though what has been discussed and
implemented before.</p>
<section id="foopartial-skill-combination-2" class="level2" data-number="6.1">
<h2 data-number="6.1"><span class="header-section-number">6.1</span> Partial
Skill Combination</h2>
<p>To test the capabilities of the method developed, the robot selected is the
UR10. It is worth noting that the research is not designed specifically for
this robot and can be applied to any robot with different specifications.</p>
<p>The UR10 robot in the setup described in the <a href="#foochap:platforms"
data-reference-type="ref" data-reference="chap:platforms">3</a> has two main
parts: the robot arm that executes the movements and the gripper that grasps
the desired object.</p>
<section id="foour10-robot-interface" class="level5" data-number="6.1.0.0.1">
<h5 data-number="6.1.0.0.1"><span
class="header-section-number">6.1.0.0.1</span> UR10 robot interface</h5>
<p>For the sake of completeness, this paragraph briefly describes the Python
interface developed to simplify the use of the robot in general and enable
this experiment. This interface is available in a <a
href="https://github.com/igor-lirussi/UR10_robot_interface">GitHub public
repository</a> <span class="citation" data-cites="url:UR10repo">(Igor Lirussi
2023e)</span>. The interface acts as a bridge between the user commands and
the ROS topics exposed by the robot&#x2019;s computer.</p>
<p>It implements a series of ROS Subscribers to retrieve the desired data
published by the robot and save it into buffer variables. Some functions are
present to get the variables about the current status of the robot, this makes
the read operations fast and not blocking.</p>
<p>Also, some ROS Publisher nodes are implemented to send commands and
information to the robot. These nodes publish data to the robot&#x2019;s defined
topics in order to move the device or set the desired settings. Functions are
also present here to simplify the calls and send the ROS messages through the
nodes implemented.</p>
</section>
<section id="foof-gripper-interface" class="level5" data-number="6.1.0.0.2">
<h5 data-number="6.1.0.0.2"><span
class="header-section-number">6.1.0.0.2</span> 3F Gripper interface</h5>
<p>Another codebase repository used in this experiment has been developed to
easily interface the gripper to the user and ease its operation.</p>
<p>This interface is available in a <a
href="https://github.com/igor-lirussi/Gripper3F_interface">GitHub public
repository</a> <span class="citation" data-cites="url:3FGripperrepo">(Igor
Lirussi 2023d)</span>. The code has been developed separately since the
gripper is a different entity from the robot and has its own ROS services and
IP address. The interface also acts as a connecting link between the Python
commands and the ROS services made available by the gripper integrated
circuit.</p>
<p>In this case, since the gripper doesn&#x2019;t offer topics but services (see <a
href="#foochap:platforms" data-reference-type="ref"
data-reference="chap:platforms">3</a>) some ServiceProxies are implemented to
simplify the operation of this last. The interface offers functions that ease
the call of these services to open, close, set the aperture or force of the
gripper, get the position, and so on.</p>
</section>
<section id="footrajectories-recorder-and-playback" class="level5"
data-number="6.1.0.0.3">
<h5 data-number="6.1.0.0.3"><span
class="header-section-number">6.1.0.0.3</span> Trajectories recorder and
playback</h5>
<p>Finally, these two interfaces were used to build a code that allows
recording the trajectory in time of the robot&#x2019;s joint positions. Furthermore,
the trajectory recorded allows the recording of the cartesian position of the
end effector and the gripper aperture.</p>
<p>Moreover, for our experiment, also the code for playing back these
trajectories has been written. This work allows the testing in real life of
the trajectories generated by the CNMPs architectures developed before.</p>
</section>
<section id="footesting" class="level5" data-number="6.1.0.0.4">
<h5 data-number="6.1.0.0.4"><span
class="header-section-number">6.1.0.0.4</span> Testing</h5>
<figure id="foofig:6Dteach">
<img src="Images/6Dteach.jpg" />
<figcaption> Some moments from learn by demonstration. On the left, action to
overcome obstacles. On the right, action to pass under a tunnel. </figcaption>
</figure>
<p>The robot was initially used to record some trajectories. This is part of
the initial teaching of the "learn by demonstration" process. The trajectories
were recorded with the previous code repository developed.</p>
<p>In <a href="#foofig:6Dteach" data-reference-type="ref"
data-reference="fig:6Dteach">6.1</a>, it is possible to see two moments of
this process. On the left image, the capture of an instant from the first
action demonstration on how to overcome obstacles. In the right image, another
action is taught on how to pass under a tunnel.</p>
<figure id="foofig:6Dtrain">
<img src="Images/6Dv.png" />
<figcaption> The 6 dimensions of a recorded trajectory on a real robot and the
resulting mixed trajectory in grey generated by the network. </figcaption>
</figure>
<p>The UR10 robot has 6 Degrees of Freedom (DoF), so the network input is
expanded to 6 dimensions. The choice of joint space is motivated by not
dealing later with the inverse kinematics to reproduce trajectories from
cartesian space. The six dimensions, one for every joint, of the two
trajectories recorded are visible in <a href="#foofig:6Dtrain"
data-reference-type="ref" data-reference="fig:6Dtrain">6.2</a>.</p>
<p>In the figure, the first action of passing over the obstacle is represented
by the blue line in the graphs, it indicates the movement of every joint in
time. Similarly, the action of passing through a tunnel is visible as the
orange line for the same joints in time.</p>
<p>After training the network with the two trajectories and obtaining a
satisfactory result, the method proposed is applied. The output trajectory
generated is visible in the grey line of every graph.</p>
<p>It is worth noting that it is not a simple linear transition, but the
network captures the non-linearity dependencies across the trajectories and
interpolates them accordingly. This results in a mixture of skills that is
coherent and maintains the properties of the actions.</p>
<p>In the graph where the blue joint executes two curves, it is possible to
notice this behavior. The trajectory generated does not pass directly from one
conditioning point to the other. The final trajectory keeps the properties of
the first blue one, descending for a while and then mixing with the orange
one.</p>
<figure id="foofig:6Dresult">
<img src="Images/6Dresult.jpg" />
<figcaption> Some instants from the playback of the trajectory generated where
the robot successfully completes the combination of the two actions.
</figcaption>
</figure>
<p>Finally, the trajectory generated is reproduced with the playback code
developed. The robot successfully combines the actions and brings the object
to the final destination without colliding with the environment. Some moments
of the executions are visible in the <a href="#foofig:6Dresult"
data-reference-type="ref" data-reference="fig:6Dresult">6.3</a>. The robot
initially overcomes the obstacles with the first part of the action taught,
then proceeds to successfully pass through the tunnel, changing its position
and orientation.</p>
</section>
</section>
<section id="fooend-to-end-skill-concatenation-2" class="level2"
data-number="6.2">
<h2 data-number="6.2"><span class="header-section-number">6.2</span>
End-To-End Skill Concatenation</h2>
<p>For the End-To-End Skill Concatenation, the robot used for real-life
testing is Baxter Robot, <a href="#foochap:platforms"
data-reference-type="ref" data-reference="chap:platforms">3</a>. It is crucial
to highlight that the research is not exclusive to this specific robot though.
The platform has been used only to demonstrate the real applicability and
efficacy of the work presented.</p>
<p>Moreover, part of the choice to use this platform instead of UR10 is indeed
to prove that the models and methods developed are platform-independent and
can be implemented on any robot.</p>
<p>Baxter robot has a complete set of interaction and Learning from
Demonstration capacities. Moreover, the manipulation capabilities, although
limited by the two-finger parallel grippers, are easier to integrate with the
expert demonstrations and trajectory recordings. The robot has two buttons on
the hands that are easy to press and link with the gripper position. The end
effectors are light and simple to move.</p>
<p>This part of the research implies the concatenation of primitives, so many
different demonstrations. Furthermore, the skills are related to the
environment and manipulation of objects, so it&#x2019;s crucial to have a platform
that is easy to interact with. For this reason, the Baxter robot has been
selected for this research demonstration.</p>
<section id="foobaxter-robot-interface" class="level5"
data-number="6.2.0.0.1">
<h5 data-number="6.2.0.0.1"><span
class="header-section-number">6.2.0.0.1</span> Baxter robot interface</h5>
<p>Another repository implemented along with the previous ones for the
experiments in real life is the interface for the Baxter robot in Python 3.
This work was driven by the need for Python 3 compatible functions since
Baxter has an interface, but it is only Python 2, and it&#x2019;s not compatible with
the majority of ML frameworks nowadays. This interface is available in a <a
href="https://github.com/igor-lirussi/baxter-python3">GitHub public
repository</a> <span class="citation" data-cites="url:Baxterrepo">(Igor
Lirussi 2023b)</span>.</p>
<p>Baxter exposes a ROS system as well, with ROS topics and services. For this
reason, the interface uses nodes and ServiceProxies to send messages or call
actions in the robot. To speed up the reading process, there are buffer
variables for the data that the robot publishes. This has been especially
useful in the recording stage since it increased the resolution of the
recordings from 10Hz to 100Hz.</p>
<p>The robot exposes plenty of options, being designed for collaborative
tasks. It has the possibility to set the joint position, to set the cartesian
position of the end effector, and to retrieve these last ones. Moreover, it&#x2019;s
possible to set and get the gripper position or the display image, or read the
data from the infrared sensors and cameras in the hands.</p>
<p>All these options are available in the Python 3 interface developed.
Moreover, some examples are present of how to move the robot or control from
the keyboard, how to get the data from the sensors and cameras, and how to use
the inverse kinematic services and the grippers.</p>
</section>
<section id="footrajectories-recorder-and-playback-1" class="level5"
data-number="6.2.0.0.2">
<h5 data-number="6.2.0.0.2"><span
class="header-section-number">6.2.0.0.2</span> Trajectories recorder and
playback</h5>
<p>Lastly, in the interface developed previously, it is possible to find also
the code to record the trajectories of the arms of the robot and play them
back.</p>
<p>The recorder waits for the pressing of a button and starts to save current
time and the position of the joints at the frequency desired. Moreover, the
recorder also saves the end effector cartesian position and orientation for
later comparison.</p>
<p>The gripper position and force read are also saved in the data for every
timestep. This is useful for manipulation tasks that require handling
objects.</p>
<p>Furthermore, a trajectory visualizer for cartesian and joint space has been
developed as well. The code enables the user to see the movement primitive in
3D and check the correctness of the data recorded or about to be played. The
3D visualization of the trajectory can be optionally surrounded by five graphs
for each individual dimension of the cartesian or joint space. The color of
the points in 3D corresponds to the gripper aperture.</p>
<p>Lastly, the complementary code for the playback of trajectories enables the
robot to move precisely from the data provided. The trajectories can be
previously recorded or generated with a network. For this reason, this code
was especially useful in testing this research.</p>
</section>
<section id="foobaxter-detecting-and-reaching-objects" class="level5"
data-number="6.2.0.0.3">
<h5 data-number="6.2.0.0.3"><span
class="header-section-number">6.2.0.0.3</span> Baxter detecting and reaching
objects</h5>
<p>Another code repository was developed to complement the research done with
a method that can bring the robot actuator to the initial position. In the
discussion presented, the robot arm has to be in an initial state, from which
possible and meaningful actions are generated. Here we briefly propose a
method that can reach the initial state. This will make the demonstration more
complete from the side of a spectator and independent from an expert who has
to guide the robot in the initial state.</p>
<p>The code and the neural network model weights are available in a <a
href="https://github.com/igor-lirussi/Baxter-Robot-ObjDet">GitHub public
repository</a> <span class="citation" data-cites="url:BaxterrepoObjDet">(Igor
Lirussi 2023a)</span>.</p>
<p>The method developed consists of putting the robot arm in a pose from which
the cameras in the hands are leveraged to acquire the RGB image of the table
below. Subsequently, the objects present will be detected with the use of a
neural network for object detection, namely YOLO. The robot will use the given
positions of the objects to move the arm slightly toward the one desired. The
inverse kinematics service of the robot takes care of computing the pose for
the new cartesian position. The process repeats till the infrared (IR) sensor
of the hand detects the distance of the object as "graspable". At this point,
the gripper closes and the object is reached for further desired
manipulation.</p>
<p>This code has been used in the research as a viable way to get to the
initial pose and the object. Many other more complicated approaches are
possible. Reached the initial state, then demonstrated the method presented
are demonstrated.</p>
</section>
<section id="footesting-1" class="level5" data-number="6.2.0.0.4">
<h5 data-number="6.2.0.0.4"><span
class="header-section-number">6.2.0.0.4</span> Testing</h5>
</section>
</section>
</section>
<section id="foochap:conclusions" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span>
Conclusions</h1>
<p>Humans are capable of executing a wide variety of complex actions, based on
prior experience. In this thesis, we provided a possible approach to novel
high-level skill generation by combining movement primitives learned by CNMP
models.</p>
<section id="foofuture-work" class="level2" data-number="7.1">
<h2 data-number="7.1"><span class="header-section-number">7.1</span> Future
work</h2>
<p>Possible future works are ...</p>
<div id="foorefs" class="references csl-bib-body hanging-indent" role="list">
<div id="fooref-Ahmetoglu_2022" class="csl-entry" role="listitem">
Ahmetoglu, Alper, M. Yunus Seker, Justus Piater, Erhan Oztop, e Emre Ugur.
2022. <span>&#xAB;DeepSym: Deep Symbol Generation and Rule Learning for Planning
from Unsupervised Robot Interaction&#xBB;</span>. <em>Journal of Artificial
Intelligence Research</em> 75 (novembre): 709&#x2013;45. <a
href="https://doi.org/10.1613/jair.1.13754">https://doi.org/10.1613/jair.1.13754</a>.
</div>
<div id="fooref-url:anaconda" class="csl-entry" role="listitem">
Anaconda. 2023. <span>&#xAB;Anaconda&#xBB;</span>. <a href="https://www.anaconda.com"
class="uri">https://www.anaconda.com</a>.
</div>
<div id="fooref-ARGALL2009469" class="csl-entry" role="listitem">
Argall, Brenna D., Sonia Chernova, Manuela Veloso, e Brett Browning. 2009.
<span>&#xAB;A survey of robot learning from demonstration&#xBB;</span>. <em>Robotics and
Autonomous Systems</em> 57 (5): 469&#x2013;83. https://doi.org/<a
href="https://doi.org/10.1016/j.robot.2008.10.024">https://doi.org/10.1016/j.robot.2008.10.024</a>.
</div>
<div id="fooref-biagiotti2008trajectory" class="csl-entry" role="listitem">
Biagiotti, Luigi, e Claudio Melchiorri. 2008. <em>Trajectory planning for
automatic machines and robots</em>. Springer Science &amp; Business Media.
</div>
<div id="fooref-calinon2016tutorial" class="csl-entry" role="listitem">
Calinon, Sylvain. 2016. <span>&#xAB;A tutorial on task-parameterized movement
learning and retrieval&#xBB;</span>. <em>Intelligent service robotics</em> 9: 1&#x2013;29.
</div>
<div id="fooref-chu2013using" class="csl-entry" role="listitem">
Chu, Vivian, Ian McMahon, Lorenzo Riano, Craig G McDonald, Qin He, Jorge
Martinez Perez-Tejada, Michael Arrigo, et al. 2013. <span>&#xAB;Using robotic
exploratory procedures to learn the meaning of haptic adjectives&#xBB;</span>. In
<em>2013 IEEE International Conference on Robotics and Automation</em>,
3048&#x2013;55. IEEE.
</div>
<div id="fooref-damianou2013deep" class="csl-entry" role="listitem">
Damianou, Andreas, e Neil D Lawrence. 2013. <span>&#xAB;Deep gaussian
processes&#xBB;</span>. In <em>Artificial intelligence and statistics</em>, 207&#x2013;15.
PMLR.
</div>
<div id="fooref-deak2003development" class="csl-entry" role="listitem">
Deak, Gedeon O. 2003. <span>&#xAB;The development of cognitive flexibility and
language abilities&#xBB;</span>. <em>Advances in child development and
behavior</em> 31: 273&#x2013;328.
</div>
<div id="fooref-dechant2021toward" class="csl-entry" role="listitem">
DeChant, Chad, e Daniel Bauer. 2021. <span>&#xAB;Toward robots that learn to
summarize their actions in natural language: a set of tasks&#xBB;</span>. In
<em>5th Annual Conference on Robot Learning, Blue Sky Submission Track</em>.
<a
href="https://openreview.net/forum?id=n3AW_ISWCXf">https://openreview.net/forum?id=n3AW_ISWCXf</a>.
</div>
<div id="fooref-egidi2006decomposition" class="csl-entry" role="listitem">
Egidi, Massimo. 2006. <span>&#xAB;Decomposition patterns in problem
solving&#xBB;</span>. <em>Contributions to Economic Analysis</em> 280: 15&#x2013;46.
</div>
<div id="fooref-DBLP:journals/corr/abs-1807-01613" class="csl-entry"
role="listitem">
Garnelo, Marta, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton,
Murray Shanahan, Yee Whye Teh, Danilo J. Rezende, e S. M. Ali Eslami. 2018.
<span>&#xAB;Conditional Neural Processes&#xBB;</span>. <em>CoRR</em> abs/1807.01613. <a
href="http://arxiv.org/abs/1807.01613">http://arxiv.org/abs/1807.01613</a>.
</div>
<div id="fooref-garnelo2018neural" class="csl-entry" role="listitem">
Garnelo, Marta, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J
Rezende, SM Eslami, e Yee Whye Teh. 2018. <span>&#xAB;Neural processes&#xBB;</span>.
<em>arXiv preprint arXiv:1807.01622</em>.
</div>
<div id="fooref-gasparetto2007new" class="csl-entry" role="listitem">
Gasparetto, Alessandro, e V Zanotto. 2007. <span>&#xAB;A new method for smooth
trajectory planning of robot manipulators&#xBB;</span>. <em>Mechanism and machine
theory</em> 42 (4): 455&#x2013;71.
</div>
<div id="fooref-gupta2019relay" class="csl-entry" role="listitem">
Gupta, Abhishek, Vikash Kumar, Corey Lynch, Sergey Levine, e Karol Hausman.
2019. <span>&#xAB;Relay Policy Learning: Solving Long-Horizon Tasks via Imitation
and Reinforcement Learning&#xBB;</span>. <a
href="https://arxiv.org/abs/1910.11956">https://arxiv.org/abs/1910.11956</a>.
</div>
<div id="fooref-horton2012affordances" class="csl-entry" role="listitem">
Horton, Thomas E, Arpan Chakraborty, e Robert St Amant. 2012.
<span>&#xAB;Affordances for robots: a brief survey&#xBB;</span>. <em>AVANT. Pismo
Awangardy Filozoficzno-Naukowej</em> 2: 70&#x2013;84.
</div>
<div id="fooref-hu2019hierarchical" class="csl-entry" role="listitem">
Hu, Hengyuan, Denis Yarats, Qucheng Gong, Yuandong Tian, e Mike Lewis. 2019.
<span>&#xAB;Hierarchical decision making by generating and following natural
language instructions&#xBB;</span>. <em>Advances in neural information processing
systems</em> 32.
</div>
<div id="fooref-url:BaxterrepoObjDet" class="csl-entry" role="listitem">
Igor Lirussi. 2023a. <span>&#xAB;Baxter Object Detection and Grasping&#xBB;</span>. <a
href="https://github.com/igor-lirussi/Baxter-Robot-ObjDet"
class="uri">https://github.com/igor-lirussi/Baxter-Robot-ObjDet</a>.
</div>
<div id="fooref-url:Baxterrepo" class="csl-entry" role="listitem">
&#x2014;&#x2014;&#x2014;. 2023b. <span>&#xAB;Baxter Python interface&#xBB;</span>. <a
href="https://github.com/igor-lirussi/baxter-python3"
class="uri">https://github.com/igor-lirussi/baxter-python3</a>.
</div>
<div id="fooref-url:CNMPrepo" class="csl-entry" role="listitem">
&#x2014;&#x2014;&#x2014;. 2023c. <span>&#xAB;CNMP-Robotic-Skill-Synthesis&#xBB;</span>. <a
href="https://github.com/igor-lirussi/CNMP-Robotic-Skill-Synthesis"
class="uri">https://github.com/igor-lirussi/CNMP-Robotic-Skill-Synthesis</a>.
</div>
<div id="fooref-url:3FGripperrepo" class="csl-entry" role="listitem">
&#x2014;&#x2014;&#x2014;. 2023d. <span>&#xAB;Three Finger Gripper Python interface&#xBB;</span>. <a
href="https://github.com/igor-lirussi/Gripper3F_interface"
class="uri">https://github.com/igor-lirussi/Gripper3F_interface</a>.
</div>
<div id="fooref-url:UR10repo" class="csl-entry" role="listitem">
&#x2014;&#x2014;&#x2014;. 2023e. <span>&#xAB;UR10 Python interface&#xBB;</span>. <a
href="https://github.com/igor-lirussi/UR10_robot_interface"
class="uri">https://github.com/igor-lirussi/UR10_robot_interface</a>.
</div>
<div id="fooref-ijspeert2002learning" class="csl-entry" role="listitem">
Ijspeert, Auke, Jun Nakanishi, e Stefan Schaal. 2002. <span>&#xAB;Learning
attractor landscapes for learning motor primitives&#xBB;</span>. <em>Advances in
neural information processing systems</em> 15.
</div>
<div id="fooref-Goertler2018VisualExplorationGaussian" class="csl-entry"
role="listitem">
J. G&#xF6;rtler, O. Deussen, R. Kehlbeck. 2018. <span>&#xAB;A Visual Exploration of
Gaussian Processes&#xBB;</span>. In <em>Proceedings of the Workshop on
Visualization for AI Explainability (VISxAI)</em>. <a
href="https://www.jgoertler.com/visual-exploration-gaussian-processes/">https://www.jgoertler.com/visual-exploration-gaussian-processes/</a>.
</div>
<div id="fooref-7523298" class="csl-entry" role="listitem">
Jamone, Lorenzo, Emre Ugur, Angelo Cangelosi, Luciano Fadiga, Alexandre
Bernardino, Justus Piater, e Jos&#xE9; Santos-Victor. 2018. <span>&#xAB;Affordances in
Psychology, Neuroscience, and Robotics: A Survey&#xBB;</span>. <em>IEEE
Transactions on Cognitive and Developmental Systems</em> 10 (1): 4&#x2013;25. <a
href="https://doi.org/10.1109/TCDS.2016.2594134">https://doi.org/10.1109/TCDS.2016.2594134</a>.
</div>
<div id="fooref-url:jupyter" class="csl-entry" role="listitem">
Jupyter Notebook. 2023. <span>&#xAB;Jupyter Notebook&#xBB;</span>. <a
href="https://jupyter.org" class="uri">https://jupyter.org</a>.
</div>
<div id="fooref-kapoor2010gaussian" class="csl-entry" role="listitem">
Kapoor, Ashish, Kristen Grauman, Raquel Urtasun, e Trevor Darrell. 2010.
<span>&#xAB;Gaussian processes for object categorization&#xBB;</span>. <em>International
journal of computer vision</em> 88: 169&#x2013;88.
</div>
<div id="fooref-karmiloff1994beyond" class="csl-entry" role="listitem">
Karmiloff-Smith, By A. 1994. <span>&#xAB;Beyond modularity: A developmental
perspective on cognitive science&#xBB;</span>. <em>European journal of disorders of
communication</em> 29 (1): 95&#x2013;105.
</div>
<div id="fooref-khazatsky2021can" class="csl-entry" role="listitem">
Khazatsky, Alexander, Ashvin Nair, Daniel Jing, e Sergey Levine. 2021.
<span>&#xAB;What can i do here? learning new skills by imagining visual
affordances&#xBB;</span>. In <em>2021 IEEE International Conference on Robotics and
Automation (ICRA)</em>, 14291&#x2013;97. IEEE.
</div>
<div id="fooref-kim2007clustering" class="csl-entry" role="listitem">
Kim, Hyun-Chul, e Jaewook Lee. 2007. <span>&#xAB;Clustering based on gaussian
processes&#xBB;</span>. <em>Neural computation</em> 19 (11): 3088&#x2013;3107.
</div>
<div id="fooref-kober2013reinforcement" class="csl-entry" role="listitem">
Kober, Jens, J Andrew Bagnell, e Jan Peters. 2013. <span>&#xAB;Reinforcement
learning in robotics: A survey&#xBB;</span>. <em>The International Journal of
Robotics Research</em> 32 (11): 1238&#x2013;74.
</div>
<div id="fooref-kober2008policy" class="csl-entry" role="listitem">
Kober, Jens, e Jan Peters. 2008. <span>&#xAB;Policy search for motor primitives in
robotics&#xBB;</span>. <em>Advances in neural information processing systems</em>
21.
</div>
<div id="fooref-konidaris2019necessity" class="csl-entry" role="listitem">
Konidaris, George. 2019. <span>&#xAB;On the necessity of abstraction&#xBB;</span>.
<em>Current opinion in behavioral sciences</em> 29: 1&#x2013;7.
</div>
<div id="fooref-kroemer2021review" class="csl-entry" role="listitem">
Kroemer, Oliver, Scott Niekum, e George Konidaris. 2021. <span>&#xAB;A review of
robot learning for manipulation: Challenges, representations, and
algorithms&#xBB;</span>. <em>The Journal of Machine Learning Research</em> 22 (1):
1395&#x2013;1476.
</div>
<div id="fooref-lee2011incremental" class="csl-entry" role="listitem">
Lee, Dongheui, e Christian Ott. 2011. <span>&#xAB;Incremental kinesthetic teaching
of motion primitives using the motion refinement tube&#xBB;</span>. <em>Autonomous
Robots</em> 31: 115&#x2013;31.
</div>
<div id="fooref-maranesi2014cortical" class="csl-entry" role="listitem">
Maranesi, Monica, Luca Bonini, e Leonardo Fogassi. 2014. <span>&#xAB;Cortical
processing of object affordances for self and others&#x2019; action&#xBB;</span>.
<em>Frontiers in psychology</em> 5: 538.
</div>
<div id="fooref-mulling2013learning" class="csl-entry" role="listitem">
M&#xFC;lling, Katharina, Jens Kober, Oliver Kroemer, e Jan Peters. 2013.
<span>&#xAB;Learning to select and generalize striking movements in robot table
tennis&#xBB;</span>. <em>The International Journal of Robotics Research</em> 32
(3): 263&#x2013;79.
</div>
<div id="fooref-nguyen2009model" class="csl-entry" role="listitem">
Nguyen-Tuong, Duy, Matthias Seeger, e Jan Peters. 2009. <span>&#xAB;Model learning
with local gaussian process regression&#xBB;</span>. <em>Advanced Robotics</em> 23
(15): 2015&#x2013;34.
</div>
<div id="fooref-osiurak2017affordance" class="csl-entry" role="listitem">
Osiurak, Fran&#xE7;ois, Yves Rossetti, e Arnaud Badets. 2017. <span>&#xAB;What is an
affordance? 40 years later&#xBB;</span>. <em>Neuroscience &amp; Biobehavioral
Reviews</em> 77: 403&#x2013;17.
</div>
<div id="fooref-paraschos2013probabilistic" class="csl-entry" role="listitem">
Paraschos, Alexandros, Christian Daniel, Jan R Peters, e Gerhard Neumann.
2013. <span>&#xAB;Probabilistic movement primitives&#xBB;</span>. <em>Advances in neural
information processing systems</em> 26.
</div>
<div id="fooref-pastor2009learning" class="csl-entry" role="listitem">
Pastor, Peter, Heiko Hoffmann, Tamim Asfour, e Stefan Schaal. 2009.
<span>&#xAB;Learning and generalization of motor skills by learning from
demonstration&#xBB;</span>. In <em>2009 IEEE International Conference on Robotics
and Automation</em>, 763&#x2013;68. IEEE.
</div>
<div id="fooref-paszke2019pytorch" class="csl-entry" role="listitem">
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, et al. 2019. <span>&#xAB;PyTorch: An Imperative Style,
High-Performance Deep Learning Library&#xBB;</span>. <a
href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</a>.
</div>
<div id="fooref-url:pytorch" class="csl-entry" role="listitem">
Pytorch. 2023. <span>&#xAB;Tensors and Dynamic neural networks in Python with
strong GPU acceleration&#xBB;</span>. <a href="https://pytorch.org"
class="uri">https://pytorch.org</a>.
</div>
<div id="fooref-ravichandar2020recent" class="csl-entry" role="listitem">
Ravichandar, Harish, Athanasios S Polydoros, Sonia Chernova, e Aude Billard.
2020. <span>&#xAB;Recent advances in robot learning from demonstration&#xBB;</span>.
<em>Annual review of control, robotics, and autonomous systems</em> 3:
297&#x2013;330.
</div>
<div id="fooref-url:3fgripper" class="csl-entry" role="listitem">
Robotiq. 2023a. <span>&#xAB;3-Finger Adaptive Robot Gripper&#xBB;</span>. <a
href="https://robotiq.com/products/3-finger-adaptive-robot-gripper"
class="uri">https://robotiq.com/products/3-finger-adaptive-robot-gripper</a>.
</div>
<div id="fooref-url:ftsensor" class="csl-entry" role="listitem">
&#x2014;&#x2014;&#x2014;. 2023b. <span>&#xAB;FT 300-S Force Torque Sensor&#xBB;</span>. <a
href="https://robotiq.com/products/ft-300-force-torque-sensor"
class="uri">https://robotiq.com/products/ft-300-force-torque-sensor</a>.
</div>
<div id="fooref-url:ros" class="csl-entry" role="listitem">
ROS. 2023. <span>&#xAB;Robot Operating System&#xBB;</span>. <a
href="https://www.ros.org" class="uri">https://www.ros.org</a>.
</div>
<div id="fooref-rosen2022role" class="csl-entry" role="listitem">
Rosen, Eric, Ben M Abbatematteo, Skye Thompson, Tuluhan Akbulut, e George
Konidaris. 2022. <span>&#xAB;On the Role of Structure in Manipulation Skill
Learning&#xBB;</span>. In <em>CoRL 2022 Workshop on Learning, Perception, and
Abstraction for Long-Horizon Planning</em>.
</div>
<div id="fooref-salimbeni2017doubly" class="csl-entry" role="listitem">
Salimbeni, Hugh, e Marc Deisenroth. 2017. <span>&#xAB;Doubly stochastic variational
inference for deep Gaussian processes&#xBB;</span>. <em>Advances in neural
information processing systems</em> 30.
</div>
<div id="fooref-saveriano2021dynamic" class="csl-entry" role="listitem">
Saveriano, Matteo, Fares J Abu-Dakka, Alja&#x17E; Kramberger, e Luka Peternel. 2021.
<span>&#xAB;Dynamic movement primitives in robotics: A tutorial survey&#xBB;</span>.
<em>The International Journal of Robotics Research</em>, 02783649231201196.
</div>
<div id="fooref-schaal1999imitation" class="csl-entry" role="listitem">
Schaal, Stefan. 1999. <span>&#xAB;Is imitation learning the route to humanoid
robots?&#xBB;</span> <em>Trends in cognitive sciences</em> 3 (6): 233&#x2013;42.
</div>
<div id="fooref-schaal2006dynamic" class="csl-entry" role="listitem">
&#x2014;&#x2014;&#x2014;. 2006. <span>&#xAB;Dynamic movement primitives-a framework for motor control in
humans and humanoid robotics&#xBB;</span>. In <em>Adaptive motion of animals and
machines</em>, 261&#x2013;80. Springer.
</div>
<div id="fooref-seeger2004gaussian" class="csl-entry" role="listitem">
Seeger, Matthias. 2004. <span>&#xAB;Gaussian processes for machine
learning&#xBB;</span>. <em>International journal of neural systems</em> 14 (02):
69&#x2013;106.
</div>
<div id="fooref-simeonov2021long" class="csl-entry" role="listitem">
Simeonov, Anthony, Yilun Du, Beomjoon Kim, Francois Hogan, Joshua Tenenbaum,
Pulkit Agrawal, e Alberto Rodriguez. 2021. <span>&#xAB;A long horizon planning
framework for manipulating rigid pointcloud objects&#xBB;</span>. In <em>Conference
on Robot Learning</em>, 1582&#x2013;1601. PMLR.
</div>
<div id="fooref-snelson2006sparse" class="csl-entry" role="listitem">
Snelson, Edward, e Zoubin Ghahramani. 2006. <span>&#xAB;Sparse Gaussian processes
using pseudo-inputs&#xBB;</span>. <em>Advances in Neural Information Processing
Systems</em> 18: 1259&#x2013;66.
</div>
<div id="fooref-ugur2020compliant" class="csl-entry" role="listitem">
Ugur, Emre, e Hakan Girgin. 2020. <span>&#xAB;Compliant parametric dynamic movement
primitives&#xBB;</span>. <em>Robotica</em> 38 (3): 457&#x2013;74.
</div>
<div id="fooref-Ugur-RSS-19" class="csl-entry" role="listitem">
Ugur, Muhammet Yunus Seker AND Mert Imre AND Justus Piater AND Emre. 2019.
<span>&#xAB;Conditional Neural Movement Primitives&#xBB;</span>. In <em>Proceedings of
Robotics: Science and Systems</em>. FreiburgimBreisgau, Germany. <a
href="https://doi.org/10.15607/RSS.2019.XV.071">https://doi.org/10.15607/RSS.2019.XV.071</a>.
</div>
<div id="fooref-url:ur10" class="csl-entry" role="listitem">
Universal Robots. 2023. <span>&#xAB;UR10 (robot) &#x2014; medium duty industrial
collaborative robot&#xBB;</span>. <a
href="https://www.universal-robots.com/products/ur10-robot/"
class="uri">https://www.universal-robots.com/products/ur10-robot/</a>.
</div>
<div id="fooref-wiki:baxter" class="csl-entry" role="listitem">
Wikipedia contributors. 2023. <span>&#xAB;Baxter (robot) &#x2014;
<span>Wikipedia</span><span>,</span> The Free Encyclopedia&#xBB;</span>. <a
href="https://en.wikipedia.org/w/index.php?title=Baxter_(robot)&amp;oldid=1183931177"
class="uri">https://en.wikipedia.org/w/index.php?title=Baxter_(robot)&amp;oldid=1183931177</a>.
</div>
<div id="fooref-wilson2016deep" class="csl-entry" role="listitem">
Wilson, Andrew Gordon, Zhiting Hu, Ruslan Salakhutdinov, e Eric P Xing. 2016.
<span>&#xAB;Deep kernel learning&#xBB;</span>. In <em>Artificial intelligence and
statistics</em>, 370&#x2013;78. PMLR.
</div>
</div>
</section>
</section>                    
                </div>
            </div>

</script>
            <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>
        </div>
    </body>
</html>
